{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
        "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1o0X6SXgwjkZ12aGPra7_KYMfPKfZhHCD?usp=sharing)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c2nWQ-SS8-LD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Master Generative AI in 8 Weeks\n",
        "**What You'll Learn:**\n",
        "- Master cutting-edge AI tools & frameworks\n",
        "- 6 weeks of hands-on, project-based learning\n",
        "- Weekly live mentorship sessions\n",
        "- Join Innovation Community\n",
        "\n",
        "Learn by building. Get expert mentorship and work on real AI projects.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)"
      ],
      "metadata": {
        "id": "87ki5l9_8_8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RAG (Retrieval-Augmented Generation) System using Llama 3 405B Model\n",
        "\n",
        "\n",
        "- Enhancing language model outputs with relevant external knowledge\n",
        "\n",
        "- Combining the power of large language models with dynamic information retrieval"
      ],
      "metadata": {
        "id": "ruXVpm7Op6bL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install required dependencies\n",
        "\n",
        "### Note: In a Jupyter notebook, you would use !pip install or %pip install\n"
      ],
      "metadata": {
        "id": "2GUsOHtCpjn8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htke3C6BJ60t"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-openai faiss-cpu langchain openai requests numpy tiktoken langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import necessary libraries\n"
      ],
      "metadata": {
        "id": "aykr9KRWpB3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from google.colab import userdata\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "import requests\n",
        "import numpy as np\n",
        "import faiss\n",
        "import os\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wHLBlaREe_Ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize the language model (LLM)\n",
        "### We're using the Llama 3 405B model hosted on Fireworks.ai"
      ],
      "metadata": {
        "id": "uklMfmdsrO_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    model=\"accounts/fireworks/models/llama-v3p1-405b-instruct\",\n",
        "    openai_api_key=userdata.get(\"FIREWORKS_API_KEY\"),\n",
        "    openai_api_base=\"https://api.fireworks.ai/inference/v1\"\n",
        ")"
      ],
      "metadata": {
        "id": "6yik2PCFpAiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create embeddings for the documents\n",
        "### Note: We're using OpenAI's embeddings here. You might want to use a different embedding model.\n"
      ],
      "metadata": {
        "id": "GUwTZNwAoYms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings(openai_api_key=userdata.get(\"OPENAI_API_KEY\"))\n"
      ],
      "metadata": {
        "id": "G1AZxiLrk4r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare your document collection using WebBaseLoader\n",
        "### We're using WebBaseLoader to load content from specified URLs\n"
      ],
      "metadata": {
        "id": "bTVNISEwo2SU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "    \"https://www.theverge.com/2024/7/23/24204055/meta-ai-llama-3-1-open-source-assistant-openai-chatgpt\",\n",
        "    # Add more URLs as needed\n",
        "]\n",
        "loader = WebBaseLoader(urls)\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "QLskT7IelTh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the documents into chunks\n",
        "### This makes it easier to process and retrieve relevant information"
      ],
      "metadata": {
        "id": "L_WdzAJVoh5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "nIE3VaLEmAC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create embeddings for the documents and store them in a FAISS vector database\n"
      ],
      "metadata": {
        "id": "teZYIFsFolN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = FAISS.from_documents(texts, embeddings)"
      ],
      "metadata": {
        "id": "G1aO4JIgmCiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine retrieval and generation\n",
        "#### This sets up the RAG system, combining the FAISS database for retrieval\n",
        "#### with the Llama 3 405B model for generation"
      ],
      "metadata": {
        "id": "ILi3pnQdoomW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 3}),\n",
        "    return_source_documents=True,\n",
        ")"
      ],
      "metadata": {
        "id": "x6P-pryWnYGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the RAG system with example queries\n",
        "#### You can modify these queries or add more as needed"
      ],
      "metadata": {
        "id": "Qb1iTqGVos_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Query 1"
      ],
      "metadata": {
        "id": "0LbFZRMtsEZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Meta's new Llama 3.1 model outperforms which models?\"\n",
        "result = qa_chain.invoke({\"query\": query})\n",
        "print(result['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VajRyJTbncg-",
        "outputId": "574548e1-30d6-44ef-ac71-9ddb6f71e5f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "According to the text, Meta's Llama 3.1 model outperforms OpenAI's GPT-4o and Anthropic's Claude 3.5 Sonnet on certain benchmarks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Query 2"
      ],
      "metadata": {
        "id": "PSJ6RCeNsGnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"give me top features of Llama 3.1 model\"\n",
        "result = qa_chain.invoke({\"query\": query})\n",
        "print(result['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsKyUEh5r_te",
        "outputId": "b9b226ad-803e-4bab-9640-c71c2f857f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the text, here are some of the top features of the Llama 3.1 model:\n",
            "\n",
            "1. Complex problem-solving: Llama 3.1 is capable of integrating with a search engine API to \"retrieve information from the internet based on a complex query and call multiple tools in succession in order to complete your tasks\".\n",
            "\n",
            "2. Advanced natural language processing: It can understand and respond to complex queries and is capable of generating human-like text.\n",
            "\n",
            "3. Image generation: Meta AI, powered by Llama 3.1, includes a new feature called \"Imagine Me\" that can scan a face through a phone's camera and then let users insert their likeness into images it generates.\n",
            "\n",
            "4. Multilingual support: Llama 3.1 will be updated to support new languages, including French, German, Hindi, Italian, and Spanish.\n",
            "\n",
            "5. Availability across multiple platforms: Llama 3.1 will be first accessible through WhatsApp and the Meta AI website in the US, followed by Instagram and Facebook in the coming weeks, and also on the Quest headset in the coming weeks.\n",
            "\n",
            "6. Free usage: Meta is releasing Llama 3.1 as an open-source model, free for developers to use and tune to their liking. \n",
            "\n",
            "7. Performance: Meta claims that Llama 3.1 outperforms GPT-4o and Anthropic’s Claude 3.5 Sonnet on several benchmarks.\n"
          ]
        }
      ]
    }
  ]
}