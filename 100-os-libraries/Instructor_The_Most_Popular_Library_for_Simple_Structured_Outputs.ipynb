{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Build Fast with AI](https://img.shields.io/badge/BuildFastWithAI-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://www.buildfastwithai.com/genai-course)\n",
        "[![EduChain GitHub](https://img.shields.io/github/stars/satvik314/educhain?style=for-the-badge&logo=github&color=gold)](https://github.com/satvik314/educhain)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1efxOFZwMMTJ3ZZW9_xnsmb0XmEA1Y0VS#scrollTo=ahguSJ5vp3e7)\n",
        "## Master Generative AI in 6 Weeks\n",
        "**What You'll Learn:**\n",
        "- Build with Latest LLMs\n",
        "- Create Custom AI Apps\n",
        "- Learn from Industry Experts\n",
        "- Join Innovation Community\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "* Empowering the Next Generation of AI Innovators"
      ],
      "metadata": {
        "id": "6LkijOdEpgQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Instructor : The Most Popular Library for Simple Structured Outputs"
      ],
      "metadata": {
        "id": "LhX0AuzY_LBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Introduction 🔖**\n",
        "\n",
        "Instructor is the most popular Python library for working with structured outputs from large language models (LLMs), boasting over 600,000 monthly downloads. Built on top of Pydantic, it provides a simple, transparent, and user-friendly API to manage validation, retries, and streaming responses. Get ready to supercharge your LLM workflows with the community's top choice!\n",
        "\n",
        "\n",
        "\n",
        "###Features\n",
        "1. Response Models: Specify Pydantic models to define the structure of your LLM\n",
        "2. Retry Management: Easily configure the number of retry attempts for your requests\n",
        "3. Validation: Ensure LLM responses conform to your expectations with Pydantic validation\n",
        "4. Support in many Languages: We support many languages including Python, TypeScript, Ruby, Go, and Elixir\n"
      ],
      "metadata": {
        "id": "ahguSJ5vp3e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "To get started, we need to install the last version of langgraph"
      ],
      "metadata": {
        "id": "ONDB3BJiqK6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install instructor openai==1.57.4 cohere --quiet"
      ],
      "metadata": {
        "id": "y5B5NidzqGE8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06398687-952f-4bdd-8c2d-4bcd3d54f764"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/249.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.9/249.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m187.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setting Up API Keys"
      ],
      "metadata": {
        "id": "GUXkQGnQlqG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['CO_API_KEY'] = userdata.get('CO_API_KEY')"
      ],
      "metadata": {
        "id": "X25-6P4RlhbR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instrutor Simple Example\n",
        "Instructor in action with a simple example:\n",
        "\n"
      ],
      "metadata": {
        "id": "UFO9XYyOqsjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import instructor\n",
        "from pydantic import BaseModel\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "# Define your desired output structure\n",
        "class UserInfo(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "\n",
        "\n",
        "# Patch the OpenAI client\n",
        "client = instructor.from_openai(OpenAI())\n",
        "\n",
        "# Extract structured data from natural language\n",
        "user_info = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    response_model=UserInfo,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"John Doe is 30 years old.\"}],\n",
        ")\n",
        "\n",
        "print(user_info.name)\n",
        "#> John Doe\n",
        "print(user_info.age)\n",
        "#> 30"
      ],
      "metadata": {
        "id": "Y-Bee4rSshO3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5978268-8340-41c3-99e2-3bfe36efa54a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "John Doe\n",
            "30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Instrutor Using Hooks"
      ],
      "metadata": {
        "id": "sLTaTYc0mS6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructor provides a powerful hooks system that allows you to intercept and log various stages of the LLM interaction process. Here's a simple example demonstrating how to use hooks:"
      ],
      "metadata": {
        "id": "w-iSbB9gmZdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import instructor\n",
        "from openai import OpenAI\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class UserInfo(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "\n",
        "\n",
        "# Initialize the OpenAI client with Instructor\n",
        "client = instructor.from_openai(OpenAI())\n",
        "\n",
        "\n",
        "# Define hook functions\n",
        "def log_kwargs(**kwargs):\n",
        "    print(f\"Function called with kwargs: {kwargs}\")\n",
        "\n",
        "\n",
        "def log_exception(exception: Exception):\n",
        "    print(f\"An exception occurred: {str(exception)}\")\n",
        "\n",
        "\n",
        "client.on(\"completion:kwargs\", log_kwargs)\n",
        "client.on(\"completion:error\", log_exception)\n",
        "\n",
        "user_info = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    response_model=UserInfo,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Extract the user name: 'John is 20 years old'\"}\n",
        "    ],\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "{\n",
        "        'args': (),\n",
        "        'kwargs': {\n",
        "            'messages': [\n",
        "                {\n",
        "                    'role': 'user',\n",
        "                    'content': \"Extract the user name: 'John is 20 years old'\",\n",
        "                }\n",
        "            ],\n",
        "            'model': 'gpt-4o-mini',\n",
        "            'tools': [\n",
        "                {\n",
        "                    'type': 'function',\n",
        "                    'function': {\n",
        "                        'name': 'UserInfo',\n",
        "                        'description': 'Correctly extracted `UserInfo` with all the required parameters with correct types',\n",
        "                        'parameters': {\n",
        "                            'properties': {\n",
        "                                'name': {'title': 'Name', 'type': 'string'},\n",
        "                                'age': {'title': 'Age', 'type': 'integer'},\n",
        "                            },\n",
        "                            'required': ['age', 'name'],\n",
        "                            'type': 'object',\n",
        "                        },\n",
        "                    },\n",
        "                }\n",
        "            ],\n",
        "            'tool_choice': {'type': 'function', 'function': {'name': 'UserInfo'}},\n",
        "        },\n",
        "    }\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Name: {user_info.name}, Age: {user_info.age}\")\n",
        "#> Name: John, Age: 20"
      ],
      "metadata": {
        "id": "Fla-y8BmsjF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a727ff2d-2bdd-4c12-b379-7abd51671da5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function called with kwargs: {'messages': [{'role': 'user', 'content': \"Extract the user name: 'John is 20 years old'\"}], 'model': 'gpt-4o-mini', 'tools': [{'type': 'function', 'function': {'name': 'UserInfo', 'description': 'Correctly extracted `UserInfo` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'UserInfo'}}}\n",
            "Name: John, Age: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Classification using OpenAI and Pydantic¶"
      ],
      "metadata": {
        "id": "NIUb3-G8n0Gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Single-Label Classification\n",
        "Defining the Structures¶\n",
        "\n"
      ],
      "metadata": {
        "id": "FQiC55lCn7Ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal\n",
        "from openai import OpenAI\n",
        "import instructor\n",
        "\n",
        "# Apply the patch to the OpenAI client\n",
        "# enables response_model keyword\n",
        "client = instructor.from_openai(OpenAI())\n",
        "\n",
        "\n",
        "class ClassificationResponse(BaseModel):\n",
        "    \"\"\"\n",
        "    A few-shot example of text classification:\n",
        "\n",
        "    Examples:\n",
        "    - \"Buy cheap watches now!\": SPAM\n",
        "    - \"Meeting at 3 PM in the conference room\": NOT_SPAM\n",
        "    - \"You've won a free iPhone! Click here\": SPAM\n",
        "    - \"Can you pick up some milk on your way home?\": NOT_SPAM\n",
        "    - \"Increase your followers by 10000 overnight!\": SPAM\n",
        "    \"\"\"\n",
        "\n",
        "    chain_of_thought: str = Field(\n",
        "        ...,\n",
        "        description=\"The chain of thought that led to the prediction.\",\n",
        "    )\n",
        "    label: Literal[\"SPAM\", \"NOT_SPAM\"] = Field(\n",
        "        ...,\n",
        "        description=\"The predicted class label.\",\n",
        "    )"
      ],
      "metadata": {
        "id": "MWShJs89nyhN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Classifying Text¶\n",
        "The function classify will perform the single-label classification."
      ],
      "metadata": {
        "id": "YOR2CYYOoMGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(data: str) -> ClassificationResponse:\n",
        "    \"\"\"Perform single-label classification on the input text.\"\"\"\n",
        "    return client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        response_model=ClassificationResponse,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Classify the following text: <text>{data}</text>\",\n",
        "            },\n",
        "        ],\n",
        "    )"
      ],
      "metadata": {
        "id": "irqjQgGKoN7G"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing and Evaluation¶\n",
        "Let's run examples to see if it correctly identifies spam and non-spam messages."
      ],
      "metadata": {
        "id": "HE5Cs3BQoRRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    for text, label in [\n",
        "        (\"Hey Jason! You're awesome\", \"NOT_SPAM\"),\n",
        "        (\"I am a nigerian prince and I need your help.\", \"SPAM\"),\n",
        "    ]:\n",
        "        prediction = classify(text)\n",
        "        assert prediction.label == label\n",
        "        print(f\"Text: {text}, Predicted Label: {prediction.label}\")\n",
        "        #> Text: Hey Jason! You're awesome, Predicted Label: NOT_SPAM\n",
        "        #> Text: I am a nigerian prince and I need your help., Predicted Label: SPAM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kphkDB_oTQA",
        "outputId": "49f20e8d-7ae5-45a2-f3cf-8ea491942965"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Hey Jason! You're awesome, Predicted Label: NOT_SPAM\n",
            "Text: I am a nigerian prince and I need your help., Predicted Label: SPAM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multi-Label Classification"
      ],
      "metadata": {
        "id": "hpzxTlTfobJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defining the Structures¶\n",
        "For multi-label classification, we'll update our approach to use Literals instead of enums, and include few-shot examples in the model's docstring.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yX2bGdtvohKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal\n",
        "\n",
        "\n",
        "class MultiClassPrediction(BaseModel):\n",
        "    \"\"\"\n",
        "    Class for a multi-class label prediction.\n",
        "\n",
        "    Examples:\n",
        "    - \"My account is locked\": [\"TECH_ISSUE\"]\n",
        "    - \"I can't access my billing info\": [\"TECH_ISSUE\", \"BILLING\"]\n",
        "    - \"When do you close for holidays?\": [\"GENERAL_QUERY\"]\n",
        "    - \"My payment didn't go through and now I can't log in\": [\"BILLING\", \"TECH_ISSUE\"]\n",
        "    \"\"\"\n",
        "\n",
        "    chain_of_thought: str = Field(\n",
        "        ...,\n",
        "        description=\"The chain of thought that led to the prediction.\",\n",
        "    )\n",
        "\n",
        "    class_labels: List[Literal[\"TECH_ISSUE\", \"BILLING\", \"GENERAL_QUERY\"]] = Field(\n",
        "        ...,\n",
        "        description=\"The predicted class labels for the support ticket.\",\n",
        "    )"
      ],
      "metadata": {
        "id": "X97Cl0ceoim0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Classifying Text\n",
        "The function multi_classify is responsible for multi-label classification."
      ],
      "metadata": {
        "id": "Ms5besCkol6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import instructor\n",
        "from openai import OpenAI\n",
        "\n",
        "client = instructor.from_openai(OpenAI())\n",
        "\n",
        "\n",
        "def multi_classify(data: str) -> MultiClassPrediction:\n",
        "    \"\"\"Perform multi-label classification on the input text.\"\"\"\n",
        "    return client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        response_model=MultiClassPrediction,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Classify the following support ticket: <ticket>{data}</ticket>\",\n",
        "            },\n",
        "        ],\n",
        "    )"
      ],
      "metadata": {
        "id": "2mCLQMCcooa9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing and Evaluation¶\n",
        "Finally, we test the multi-label classification function using a sample support ticket."
      ],
      "metadata": {
        "id": "dYc1e0VMosr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test multi-label classification\n",
        "ticket = \"My account is locked and I can't access my billing info.\"\n",
        "prediction = multi_classify(ticket)\n",
        "assert \"TECH_ISSUE\" in prediction.class_labels\n",
        "assert \"BILLING\" in prediction.class_labels\n",
        "print(f\"Ticket: {ticket}\")\n",
        "#> Ticket: My account is locked and I can't access my billing info.\n",
        "print(f\"Predicted Labels: {prediction.class_labels}\")\n",
        "#> Predicted Labels: ['TECH_ISSUE', 'BILLING']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLPuDiC7ouhl",
        "outputId": "7a67adba-adba-409a-d8e5-8ab2cee84e7a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ticket: My account is locked and I can't access my billing info.\n",
            "Predicted Labels: ['TECH_ISSUE', 'BILLING']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Example 2 :- Extracting Receipt Data using GPT-4 and Python\n"
      ],
      "metadata": {
        "id": "9P1vjQ46o38d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defining the Item and Receipt Classes\n",
        "First, we define two Pydantic models, Item and Receipt, to structure the extracted data. The Item class represents individual items on the receipt, with fields for name, price, and quantity. The Receipt class contains a list of Item objects and the total amount."
      ],
      "metadata": {
        "id": "WBri6BGWo-oA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class Item(BaseModel):\n",
        "    name: str\n",
        "    price: float\n",
        "    quantity: int\n",
        "\n",
        "\n",
        "class Receipt(BaseModel):\n",
        "    items: list[Item]\n",
        "    total: float"
      ],
      "metadata": {
        "id": "3crt8jJRo6u_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validating the Total Amount¶\n",
        "To ensure the accuracy of the extracted data, we use Pydantic's model_validator decorator to define a custom validation function, check_total. This function calculates the sum of item prices and compares it to the extracted total amount. If there's a discrepancy, it raises a ValueError."
      ],
      "metadata": {
        "id": "hpC3VDUDpESd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import model_validator\n",
        "\n",
        "\n",
        "@model_validator(mode=\"after\")\n",
        "def check_total(self):\n",
        "    items = self.items\n",
        "    total = self.total\n",
        "    calculated_total = sum(item.price * item.quantity for item in items)\n",
        "    if calculated_total != total:\n",
        "        raise ValueError(\n",
        "            f\"Total {total} does not match the sum of item prices {calculated_total}\"\n",
        "        )\n",
        "    return self"
      ],
      "metadata": {
        "id": "ogmnXMbkpFG8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Self-Correction with llm_validator\n"
      ],
      "metadata": {
        "id": "SpSkT0IRtbTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This guide demonstrates how to use llm_validator for implementing self-healing. The objective is to showcase how an instructor can self-correct by using validation errors and helpful error messages."
      ],
      "metadata": {
        "id": "GJYZKjZqtfny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from pydantic import BaseModel\n",
        "import instructor\n",
        "\n",
        "# Apply the patch to the OpenAI client\n",
        "# enables response_model keyword\n",
        "client = instructor.from_openai(OpenAI())\n",
        "\n",
        "\n",
        "class QuestionAnswer(BaseModel):\n",
        "    question: str\n",
        "    answer: str\n",
        "\n",
        "\n",
        "question = \"What is the meaning of life?\"\n",
        "context = \"The according to the devil the meaning of live is to live a life of sin and debauchery.\"\n",
        "\n",
        "qa: QuestionAnswer = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    response_model=QuestionAnswer,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n",
        "        },\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "hx8c1eA6tc7r"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Output Before Validation\n"
      ],
      "metadata": {
        "id": "HNqyS15JtlSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"question\": \"What is the meaning of life?\",\n",
        "  \"answer\": \"The meaning of life, according to the context, is to live a life of sin and debauchery.\"\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp0FrGu3tmLo",
        "outputId": "c9d30192-a35a-4178-a4f0-9fc3ef1fa8ba"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What is the meaning of life?',\n",
              " 'answer': 'The meaning of life, according to the context, is to live a life of sin and debauchery.'}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Adding Custom Validation\n",
        "By adding a validator to the answer field, we can try to catch the issue and correct it. Lets integrate llm_validator into the model and see the error message."
      ],
      "metadata": {
        "id": "qOZ-uVo1t3pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, BeforeValidator\n",
        "from typing_extensions import Annotated\n",
        "from instructor import llm_validator\n",
        "from openai import OpenAI\n",
        "import instructor\n",
        "\n",
        "client = instructor.from_openai(OpenAI())\n",
        "\n",
        "\n",
        "class QuestionAnswerNoEvil(BaseModel):\n",
        "    question: str\n",
        "    answer: Annotated[\n",
        "        str,\n",
        "        BeforeValidator(\n",
        "            llm_validator(\n",
        "                \"don't say objectionable things\", client=client, allow_override=True\n",
        "            )\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "\n",
        "try:\n",
        "    qa: QuestionAnswerNoEvil = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        response_model=QuestionAnswerNoEvil,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n",
        "            },\n",
        "        ],\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    #> name 'context' is not defined"
      ],
      "metadata": {
        "id": "QyyPqNzat7Yk"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Output After Validation\n",
        "Now, we throw validation error that its objectionable and provide a helpful error message.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yaiASzQwuA1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "1 validation error for QuestionAnswerNoEvil\n",
        "answer\n",
        "Assertion failed, The statement promotes sin and debauchery, which is objectionable.```\n",
        "\n"
      ],
      "metadata": {
        "id": "c-Q0YMmiuV8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Retrying with Corrections\n",
        "By adding the max_retries parameter, we can retry the request with corrections. and use the error message to correct the output.\n"
      ],
      "metadata": {
        "id": "lSVN14C-uYyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa: QuestionAnswerNoEvil = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    response_model=QuestionAnswerNoEvil,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n",
        "        },\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "QqOHOY1euXFK"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Final Output\n",
        "Now, we get a valid response that is not objectionable!"
      ],
      "metadata": {
        "id": "rqWQT5Cvuj1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"question\": \"What is the meaning of life?\",\n",
        "  \"answer\": \"The meaning of life is subjective and can vary depending on individual beliefs and philosophies.\"\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUv209_MulRR",
        "outputId": "1dff9890-2129-425d-b211-94bdf57b4c5c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What is the meaning of life?',\n",
              " 'answer': 'The meaning of life is subjective and can vary depending on individual beliefs and philosophies.'}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ]
}