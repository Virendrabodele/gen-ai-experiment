{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Build Fast with AI](https://img.shields.io/badge/BuildFastWithAI-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://www.buildfastwithai.com/genai-course)\n",
        "[![EduChain GitHub](https://img.shields.io/github/stars/satvik314/educhain?style=for-the-badge&logo=github&color=gold)](https://github.com/satvik314/educhain)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1dzOlT0qpTejLormcV6nif1SPRUe_JDwU#scrollTo=2-ctKceib2aa)\n",
        "## Master Generative AI in 6 Weeks\n",
        "**What You'll Learn:**\n",
        "- Build with Latest LLMs\n",
        "- Create Custom AI Apps\n",
        "- Learn from Industry Experts\n",
        "- Join Innovation Community\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "*Empowering the Next Generation of AI Innovators"
      ],
      "metadata": {
        "id": "w2E0cXt5RvNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🚀 **Milvus: Unlocking the Power of Vector Databases**  \n",
        "Milvus is an open-source vector database optimized for managing and searching large-scale, high-dimensional data.  \n",
        "It supports various machine learning and AI applications by enabling efficient similarity search and analytics.  \n",
        "Milvus offers features like distributed architecture, high availability, and scalability, making it suitable for industries such as finance, healthcare, and e-commerce.  \n"
      ],
      "metadata": {
        "id": "LcsWSSNrVyGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Integrating Sentence Transformers with Milvus for Embedding Generation**"
      ],
      "metadata": {
        "id": "2-ctKceib2aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Setup and Installation**\n"
      ],
      "metadata": {
        "id": "3gSrlWKCV4Gk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhYCNX5sCc9z"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pymilvus\n",
        "!pip install \"pymilvus[model]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade --quiet pymilvus milvus-haystack markdown-it-py mdit_plain"
      ],
      "metadata": {
        "id": "Rcn2uaR4ZJFP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f44b3ca-abcf-46dd-fcc0-05f6bd97b205"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/391.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m286.7/391.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m391.4/391.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import model\n",
        "\n",
        "sentence_transformer_ef = model.dense.SentenceTransformerEmbeddingFunction(\n",
        "    model_name='all-MiniLM-L6-v2', # Specify the model name\n",
        "    device='cpu' # Specify the device to use, e.g., 'cpu' or 'cuda:0'\n",
        ")"
      ],
      "metadata": {
        "id": "1FogcCRoRxNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Creating Embeddings for Text Data**"
      ],
      "metadata": {
        "id": "xyijKukPa0kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        "    \"Artificial intelligence was founded as an academic discipline in 1956.\",\n",
        "    \"Alan Turing was the first person to conduct substantial research in AI.\",\n",
        "    \"Born in Maida Vale, London, Turing was raised in southern England.\",\n",
        "]\n",
        "\n",
        "docs_embeddings = sentence_transformer_ef.encode_documents(docs)\n",
        "\n",
        "print(\"Embeddings:\", docs_embeddings)\n",
        "print(\"Dim:\", sentence_transformer_ef.dim, docs_embeddings[0].shape)\n"
      ],
      "metadata": {
        "id": "Ltl6FLPQRmlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Creating Embedding for Queries**"
      ],
      "metadata": {
        "id": "BtuRkOYRbSec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries = [\"When was artificial intelligence founded\",\n",
        "           \"Where was Alan Turing born?\"]\n",
        "\n",
        "query_embeddings = sentence_transformer_ef.encode_queries(queries)\n",
        "\n",
        "print(\"Embeddings:\", query_embeddings)\n",
        "print(\"Dim:\", sentence_transformer_ef.dim, query_embeddings[0].shape)\n"
      ],
      "metadata": {
        "id": "RIW1Fd8PWF7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Retrieval-Augmented Generation (RAG) with Milvus and Haystack**"
      ],
      "metadata": {
        "id": "Dl_cO03FWoBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setup API Keys"
      ],
      "metadata": {
        "id": "sBkjWf9O-jlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "vDT45IubWKue"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Prepare the data**"
      ],
      "metadata": {
        "id": "77fO9EkKZkLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "url = \"https://www.gutenberg.org/cache/epub/7785/pg7785.txt\"\n",
        "file_path = \"./davinci.txt\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    urllib.request.urlretrieve(url, file_path)\n"
      ],
      "metadata": {
        "id": "Osryi3nDZZNP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Create the indexing Pipeline**"
      ],
      "metadata": {
        "id": "Rw7m9QCWZqZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack import Pipeline\n",
        "from haystack.components.converters import MarkdownToDocument\n",
        "from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\n",
        "from haystack.components.preprocessors import DocumentSplitter\n",
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack.utils import Secret\n",
        "\n",
        "from milvus_haystack import MilvusDocumentStore\n",
        "from milvus_haystack.milvus_embedding_retriever import MilvusEmbeddingRetriever\n",
        "\n",
        "\n",
        "document_store = MilvusDocumentStore(\n",
        "    connection_args={\"uri\": \"./milvus.db\"},\n",
        "    drop_old=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "gBO-E4wvZoJs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexing_pipeline = Pipeline()\n",
        "indexing_pipeline.add_component(\"converter\", MarkdownToDocument())\n",
        "indexing_pipeline.add_component(\n",
        "    \"splitter\", DocumentSplitter(split_by=\"sentence\", split_length=2)\n",
        ")\n",
        "indexing_pipeline.add_component(\"embedder\", OpenAIDocumentEmbedder())\n",
        "indexing_pipeline.add_component(\"writer\", DocumentWriter(document_store))\n",
        "indexing_pipeline.connect(\"converter\", \"splitter\")\n",
        "indexing_pipeline.connect(\"splitter\", \"embedder\")\n",
        "indexing_pipeline.connect(\"embedder\", \"writer\")\n",
        "indexing_pipeline.run({\"converter\": {\"sources\": [file_path]}})\n",
        "\n",
        "print(\"Number of documents:\", document_store.count_documents())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVxTjrdMZtc_",
        "outputId": "cb8ec715-46fc-4176-c81f-a2e980300a49"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting markdown files to Documents: 100%|██████████| 1/1 [00:00<00:00,  8.45it/s]\n",
            "Calculating embeddings: 9it [00:06,  1.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Create the retrieval pipeline**\n"
      ],
      "metadata": {
        "id": "FNwc711gaUu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'Where is the painting \"Warrior\" currently stored?'\n",
        "\n",
        "retrieval_pipeline = Pipeline()\n",
        "retrieval_pipeline.add_component(\"embedder\", OpenAITextEmbedder())\n",
        "retrieval_pipeline.add_component(\n",
        "    \"retriever\", MilvusEmbeddingRetriever(document_store=document_store, top_k=3)\n",
        ")\n",
        "retrieval_pipeline.connect(\"embedder\", \"retriever\")\n",
        "\n",
        "retrieval_results = retrieval_pipeline.run({\"embedder\": {\"text\": question}})\n",
        "\n",
        "for doc in retrieval_results[\"retriever\"][\"documents\"]:\n",
        "    print(doc.content)\n",
        "    print(\"-\" * 10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBZ9YcqPZzse",
        "outputId": "8828557e-678b-4346-a411-2b2b0a5d988e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "). The\n",
            "composition of this oil-painting seems to have been built up on the\n",
            "second cartoon, which he had made some eight years earlier, and which\n",
            "was apparently taken to France in 1516 and ultimately lost.\n",
            "----------\n",
            "\n",
            "This \"Baptism of Christ,\" which is now in the Accademia in Florence\n",
            "and is in a bad state of preservation, appears to have been a\n",
            "comparatively early work by Verrocchio, and to have been painted\n",
            "in 1480-1482, when Leonardo would be about thirty years of age.\n",
            "\n",
            "To about this period belongs the superb drawing of the \"Warrior,\" now\n",
            "in the Malcolm Collection in the British Museum.\n",
            "----------\n",
            "\" Although he\n",
            "completed the cartoon, the only part of the composition which he\n",
            "eventually executed in colour was an incident in the foreground\n",
            "which dealt with the \"Battle of the Standard.\" One of the many\n",
            "supposed copies of a study of this mural painting now hangs on the\n",
            "south-east staircase in the Victoria and Albert Museum.\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Create the RAG pipeline**"
      ],
      "metadata": {
        "id": "9s8kNNbyaIWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.utils import Secret\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from haystack.components.generators import OpenAIGenerator\n",
        "\n",
        "prompt_template = \"\"\"Answer the following query based on the provided context. If the context does\n",
        "                     not include an answer, reply with 'I don't know'.\\n\n",
        "                     Query: {{query}}\n",
        "                     Documents:\n",
        "                     {% for doc in documents %}\n",
        "                        {{ doc.content }}\n",
        "                     {% endfor %}\n",
        "                     Answer:\n",
        "                  \"\"\"\n",
        "\n",
        "rag_pipeline = Pipeline()\n",
        "rag_pipeline.add_component(\"text_embedder\", OpenAITextEmbedder())\n",
        "rag_pipeline.add_component(\n",
        "    \"retriever\", MilvusEmbeddingRetriever(document_store=document_store, top_k=3)\n",
        ")\n",
        "rag_pipeline.add_component(\"prompt_builder\", PromptBuilder(template=prompt_template))\n",
        "rag_pipeline.add_component(\n",
        "    \"generator\",\n",
        "    OpenAIGenerator(\n",
        "        api_key=Secret.from_token(os.getenv(\"OPENAI_API_KEY\")),\n",
        "        generation_kwargs={\"temperature\": 0},\n",
        "    ),\n",
        ")\n",
        "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
        "rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
        "rag_pipeline.connect(\"prompt_builder\", \"generator\")\n",
        "\n",
        "results = rag_pipeline.run(\n",
        "    {\n",
        "        \"text_embedder\": {\"text\": question},\n",
        "        \"prompt_builder\": {\"query\": question},\n",
        "    }\n",
        ")\n",
        "print(\"RAG answer:\", results[\"generator\"][\"replies\"][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_GoZD3qZ5Nv",
        "outputId": "353dd893-ab2d-4241-917e-60ab9eab3588"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG answer: The painting \"Warrior\" is currently stored in the Malcolm Collection in the British Museum.\n"
          ]
        }
      ]
    }
  ]
}