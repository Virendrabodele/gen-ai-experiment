{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Build Fast with AI](https://img.shields.io/badge/BuildFastWithAI-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://www.buildfastwithai.com/genai-course)\n",
        "[![EduChain GitHub](https://img.shields.io/github/stars/satvik314/educhain?style=for-the-badge&logo=github&color=gold)](https://github.com/satvik314/educhain)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1SQciZw0CMjRbCCJ815bQiTbn_tGly_yN?usp=sharing)\n",
        "## Master Generative AI in 6 Weeks\n",
        "**What You'll Learn:**\n",
        "- Build with Latest LLMs\n",
        "- Create Custom AI Apps\n",
        "- Learn from Industry Experts\n",
        "- Join Innovation Community\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "*Empowering the Next Generation of AI Innovators"
      ],
      "metadata": {
        "id": "2t0P-r-8JfI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ö° Semantic Kernel: Versatile AI Orchestration Framework\n",
        "\n",
        "Semantic Kernel is an open-source framework for **integrating large language models (LLMs)** into traditional applications, enabling seamless AI orchestration and automation. üöÄ\n",
        "\n",
        "### üîë **Key Features**:\n",
        "- üß© Modular kernel architecture for streamlined service and plugin management.\n",
        "- üåê Built-in connectors for OpenAI, Azure OpenAI, and Hugging Face.\n",
        "- ‚úèÔ∏è Supports prompt engineering, dynamic task planning, and function orchestration.\n",
        "- üìà Scalable and customizable for AI-driven applications.\n"
      ],
      "metadata": {
        "id": "kEeSp6EjJeqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Install Semantic Kernel**"
      ],
      "metadata": {
        "id": "43sTArgJvmEA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZvEyvYQmKDI"
      },
      "outputs": [],
      "source": [
        "!pip install semantic-kernel\n",
        "from semantic_kernel import __version__\n",
        "print(f\"Semantic Kernel version: {__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Initial Configuration for the Notebook**\n"
      ],
      "metadata": {
        "id": "g7kJ_qX_vt4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "notebook_dir = os.path.abspath(\"\")\n",
        "parent_dir = os.path.dirname(notebook_dir)\n",
        "grandparent_dir = os.path.dirname(parent_dir)\n",
        "\n",
        "sys.path.append(grandparent_dir)"
      ],
      "metadata": {
        "id": "LDUrtmshvtCW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Configuring the Kernel**\n"
      ],
      "metadata": {
        "id": "sKWmV3-Mv-1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from semantic_kernel import Kernel\n",
        "\n",
        "kernel = Kernel()"
      ],
      "metadata": {
        "id": "JkOOWbpPv648"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Using OpenAI Configuration**\n"
      ],
      "metadata": {
        "id": "9Vyz5IzZwFVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "OPENAI_API_KEY= userdata.get('OPENAI_API_KEY')\n",
        "OPENAI_CHAT_MODEL_ID='gpt-4o'\n",
        "\n",
        "os.environ['GLOBAL_LLM_SERVICE'] = 'OpenAI'\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_CHAT_MODEL_ID'] = 'gpt-4o'\n",
        "os.environ['OPENAI_TEXT_MODEL_ID'] = 'gpt-4'"
      ],
      "metadata": {
        "id": "Tz-IRifcwBUL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Creating an OpenAI Assistant with Semantic Kernel**\n"
      ],
      "metadata": {
        "id": "24aNOw8kYeek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Import Libraries**"
      ],
      "metadata": {
        "id": "KJn0yiZHSjj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import os\n",
        "import re\n",
        "\n",
        "from openai import OpenAI\n",
        "from semantic_kernel.agents import AgentGroupChat\n",
        "from semantic_kernel.agents.open_ai import OpenAIAssistantAgent\n",
        "from semantic_kernel.contents.chat_message_content import ChatMessageContent\n",
        "from semantic_kernel.contents.utils.author_role import AuthorRole"
      ],
      "metadata": {
        "id": "vRfH5WM_SUgO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Display Introductory Message**\n"
      ],
      "metadata": {
        "id": "SgAmWq2ASlho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_intro_message():\n",
        "    print(\n",
        "        \"\"\"\n",
        "    Chat with an AI assistant backed by a Semantic Kernel OpenAIAssistantAgent.\n",
        "\n",
        "    To start: you can upload files to the assistant using the command (brackets included):\n",
        "\n",
        "    [upload code_interpreter | file_search file_path]\n",
        "\n",
        "    where `code_interpreter` or `file_search` is the purpose of the file and\n",
        "    `file_path` is the path to the file. For example:\n",
        "\n",
        "    [upload code_interpreter file.txt]\n",
        "\n",
        "    This will upload file.txt to the assistant for use with the code interpreter tool.\n",
        "\n",
        "    Type \"exit\" to exit the chat.\n",
        "    \"\"\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "7dr_C5RPSk0i"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Parse Upload Command**\n"
      ],
      "metadata": {
        "id": "mWjMz4HFSsvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_upload_command(user_input: str):\n",
        "    \"\"\"Parse the user input for an upload command.\"\"\"\n",
        "    match = re.search(r\"\\[upload\\s+(code_interpreter|file_search)\\s+(.+)\\]\", user_input)\n",
        "    if match:\n",
        "        return match.group(1), match.group(2)\n",
        "    return None, None"
      ],
      "metadata": {
        "id": "-fncTz5YSvac"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Handle File Upload**\n"
      ],
      "metadata": {
        "id": "TAADYU5SSyzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def handle_file_upload(assistant_agent: OpenAIAssistantAgent, purpose: str, file_path: str):\n",
        "    \"\"\"Handle the file upload command.\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "\n",
        "    file_id = await assistant_agent.add_file(file_path, purpose=\"assistants\")\n",
        "    print(f\"File uploaded: {file_id}\")\n",
        "\n",
        "    if purpose == \"code_interpreter\":\n",
        "        await enable_code_interpreter(assistant_agent, file_id)\n",
        "    elif purpose == \"file_search\":\n",
        "        await enable_file_search(assistant_agent, file_id)"
      ],
      "metadata": {
        "id": "0eo92lgnS1Et"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Enable Code Interpreter**\n"
      ],
      "metadata": {
        "id": "zOvLlanqS4Bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def enable_code_interpreter(assistant_agent: OpenAIAssistantAgent, file_id: str):\n",
        "    \"\"\"Enable the file for code interpreter.\"\"\"\n",
        "    assistant_agent.code_interpreter_file_ids.append(file_id)\n",
        "    tools = [{\"type\": \"file_search\"}, {\"type\": \"code_interpreter\"}]\n",
        "    tool_resources = {\"code_interpreter\": {\"file_ids\": assistant_agent.code_interpreter_file_ids}}\n",
        "    await assistant_agent.modify_assistant(\n",
        "        assistant_id=assistant_agent.assistant.id, tools=tools, tool_resources=tool_resources\n",
        "    )\n",
        "    print(\"File enabled for code interpreter.\")"
      ],
      "metadata": {
        "id": "-o52VHFTS6WX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Enable File Search**\n"
      ],
      "metadata": {
        "id": "BmpmQutvS9Hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def enable_file_search(assistant_agent: OpenAIAssistantAgent, file_id: str):\n",
        "    \"\"\"Enable the file for file search.\"\"\"\n",
        "    if assistant_agent.vector_store_id is not None:\n",
        "        await assistant_agent.client.beta.vector_stores.files.create(\n",
        "            vector_store_id=assistant_agent.vector_store_id, file_id=file_id\n",
        "        )\n",
        "        assistant_agent.file_search_file_ids.append(file_id)\n",
        "    else:\n",
        "        vector_store = await assistant_agent.create_vector_store(file_ids=file_id)\n",
        "        assistant_agent.file_search_file_ids.append(file_id)\n",
        "        assistant_agent.vector_store_id = vector_store.id\n",
        "        tools = [{\"type\": \"file_search\"}, {\"type\": \"code_interpreter\"}]\n",
        "        tool_resources = {\"file_search\": {\"vector_store_ids\": [vector_store.id]}}\n",
        "        await assistant_agent.modify_assistant(\n",
        "            assistant_id=assistant_agent.assistant.id, tools=tools, tool_resources=tool_resources\n",
        "        )\n",
        "    print(\"File enabled for file search.\")"
      ],
      "metadata": {
        "id": "fTJ5weJUS_Zj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cleanup Resources**\n"
      ],
      "metadata": {
        "id": "LyxZMxYuTCFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def cleanup_resources(assistant_agent: OpenAIAssistantAgent):\n",
        "    \"\"\"Cleanup the resources used by the assistant.\"\"\"\n",
        "    if assistant_agent.vector_store_id:\n",
        "        await assistant_agent.delete_vector_store(assistant_agent.vector_store_id)\n",
        "    for file_id in assistant_agent.code_interpreter_file_ids:\n",
        "        await assistant_agent.delete_file(file_id)\n",
        "    for file_id in assistant_agent.file_search_file_ids:\n",
        "        await assistant_agent.delete_file(file_id)\n",
        "    await assistant_agent.delete()"
      ],
      "metadata": {
        "id": "H9NJEggwTEQe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code = '''def factorial(n):\n",
        "    if n == 0:\n",
        "        return 1\n",
        "    return n * factorial(n-1)\n",
        "\n",
        "num = int(input(\"Enter a number: \"))\n",
        "print(f\"The factorial of {num} is {factorial(num)}\")'''\n",
        "\n",
        "with open('code.txt', 'w') as file:\n",
        "    file.write(code)\n",
        "\n",
        "print(\"Code saved to code.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yULMBHAsZL5-",
        "outputId": "716621a2-35d2-4b02-93ff-dae01af42cef"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Code saved to code.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Main Chat Assistant Function**\n"
      ],
      "metadata": {
        "id": "CMNdMDGeTG_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "    assistant_agent = None\n",
        "    try:\n",
        "        display_intro_message()\n",
        "\n",
        "        # Create the OpenAI Assistant Agent\n",
        "        assistant_agent = await OpenAIAssistantAgent.create(\n",
        "            service_id=\"AIAssistant\",\n",
        "            description=\"An AI assistant that helps with everyday tasks.\",\n",
        "            instructions=\"Help the user with their task.\",\n",
        "            enable_code_interpreter=True,\n",
        "            enable_file_search=True,\n",
        "            api_key=OPENAI_API_KEY,\n",
        "            model_id=OPENAI_CHAT_MODEL_ID,\n",
        "        )\n",
        "\n",
        "        # Define an agent group chat, which drives the conversation\n",
        "        # We add messages to the chat and then invoke the agent to respond.\n",
        "        chat = AgentGroupChat()\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                user_input = input(\"User:> \")\n",
        "            except (KeyboardInterrupt, EOFError):\n",
        "                print(\"\\n\\nExiting chat...\")\n",
        "                break\n",
        "\n",
        "            if user_input.strip().lower() == \"exit\":\n",
        "                print(\"\\n\\nExiting chat...\")\n",
        "                break\n",
        "\n",
        "            purpose, file_path = parse_upload_command(user_input)\n",
        "            if purpose and file_path:\n",
        "                await handle_file_upload(assistant_agent, purpose, file_path)\n",
        "                continue\n",
        "\n",
        "            await chat.add_chat_message(message=ChatMessageContent(role=AuthorRole.USER, content=user_input))\n",
        "            async for content in chat.invoke(agent=assistant_agent):\n",
        "                print(f\"Assistant:> # {content.role} - {content.name or '*'}: '{content.content}'\")\n",
        "    finally:\n",
        "        if assistant_agent:\n",
        "            await cleanup_resources(assistant_agent)\n",
        "\n",
        "await main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de77b6c-5c9a-4117-93bf-44357f6dd430",
        "id": "8auzyNd1a3gB"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Chat with an AI assistant backed by a Semantic Kernel OpenAIAssistantAgent.\n",
            "\n",
            "    To start: you can upload files to the assistant using the command (brackets included):\n",
            "\n",
            "    [upload code_interpreter | file_search file_path]\n",
            "\n",
            "    where `code_interpreter` or `file_search` is the purpose of the file and\n",
            "    `file_path` is the path to the file. For example:\n",
            "\n",
            "    [upload code_interpreter file.txt]\n",
            "\n",
            "    This will upload file.txt to the assistant for use with the code interpreter tool.\n",
            "\n",
            "    Type \"exit\" to exit the chat.\n",
            "    \n",
            "User:> hi\n",
            "Assistant:> # AuthorRole.ASSISTANT - agent_ICoLFKUrNORUePvN: 'Hello! How can I assist you today?'\n",
            "User:> exit\n",
            "\n",
            "\n",
            "Exiting chat...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import openai\n",
        "from typing import Annotated\n",
        "from semantic_kernel.agents.open_ai import OpenAIAssistantAgent\n",
        "from semantic_kernel.contents.chat_message_content import ChatMessageContent\n",
        "from semantic_kernel.contents.utils.author_role import AuthorRole\n",
        "from semantic_kernel.functions.kernel_function_decorator import kernel_function\n",
        "from semantic_kernel.kernel import Kernel"
      ],
      "metadata": {
        "id": "2Cjr5A-_YLXw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Define Menu Plugin**\n",
        "\n"
      ],
      "metadata": {
        "id": "YU_DmH6pYtEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MenuPlugin:\n",
        "    \"\"\"A sample Menu Plugin used for the concept sample.\"\"\"\n",
        "\n",
        "    @kernel_function(description=\"Provides a list of specials from the menu.\")\n",
        "    def get_specials(self) -> Annotated[str, \"Returns the specials from the menu.\"]:\n",
        "        return \"\"\"\n",
        "        Special Soup: Clam Chowder\n",
        "        Special Salad: Cobb Salad\n",
        "        Special Drink: Chai Tea\n",
        "        \"\"\"\n",
        "\n",
        "    @kernel_function(description=\"Provides the price of the requested menu item.\")\n",
        "    def get_item_price(\n",
        "        self, menu_item: Annotated[str, \"The name of the menu item.\"]\n",
        "    ) -> Annotated[str, \"Returns the price of the menu item.\"]:\n",
        "        return \"$9.99\""
      ],
      "metadata": {
        "id": "APorrFZMYsQV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Helper Method to Invoke Agent**\n"
      ],
      "metadata": {
        "id": "SXdqEUNvY2ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "async def invoke_agent(agent: OpenAIAssistantAgent, thread_id: str, input: str) -> None:\n",
        "    \"\"\"Invoke the agent with the user input.\"\"\"\n",
        "    await agent.add_chat_message(thread_id=thread_id, message=ChatMessageContent(role=AuthorRole.USER, content=input))\n",
        "\n",
        "    print(f\"# {AuthorRole.USER}: '{input}'\")\n",
        "\n",
        "    async for content in agent.invoke(thread_id=thread_id):\n",
        "        if content.role != AuthorRole.TOOL:\n",
        "            print(f\"# {content.role}: {content.content}\")\n"
      ],
      "metadata": {
        "id": "pcZJkYY4Y034"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Main Function to Create and Invoke Agent**\n"
      ],
      "metadata": {
        "id": "3aIGuMunY-Zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "    # Create the instance of the Kernel\n",
        "    kernel = Kernel()\n",
        "\n",
        "    # Add the sample plugin to the kernel\n",
        "    kernel.add_plugin(plugin=MenuPlugin(), plugin_name=\"menu\")\n",
        "\n",
        "    # Create the OpenAI Assistant Agent\n",
        "    service_id = \"agent\"\n",
        "    if use_azure_openai:\n",
        "        agent = await AzureAssistantAgent.create(\n",
        "            kernel=kernel, service_id=service_id, name=HOST_NAME, instructions=HOST_INSTRUCTIONS\n",
        "        )\n",
        "    else:\n",
        "        agent = await OpenAIAssistantAgent.create(\n",
        "            kernel=kernel, service_id=service_id, name=HOST_NAME, instructions=HOST_INSTRUCTIONS\n",
        "        )\n",
        "\n",
        "    thread_id = await agent.create_thread()\n",
        "\n",
        "    try:\n",
        "        await invoke_agent(agent, thread_id=thread_id, input=\"Hello\")\n",
        "        await invoke_agent(agent, thread_id=thread_id, input=\"What is the special soup?\")\n",
        "        await invoke_agent(agent, thread_id=thread_id, input=\"What is the special drink?\")\n",
        "        await invoke_agent(agent, thread_id=thread_id, input=\"Thank you\")\n",
        "    finally:\n",
        "        await agent.delete_thread(thread_id)\n",
        "        await agent.delete()\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "id": "14Es_RnKUIGt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}