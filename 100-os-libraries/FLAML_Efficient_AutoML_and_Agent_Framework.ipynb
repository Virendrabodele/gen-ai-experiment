{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Build Fast with AI](https://img.shields.io/badge/BuildFastWithAI-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://www.buildfastwithai.com/genai-course)\n",
        "[![EduChain GitHub](https://img.shields.io/github/stars/satvik314/educhain?style=for-the-badge&logo=github&color=gold)](https://github.com/satvik314/educhain)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1sNL9qqf4vTb6wQnxvkctTkHnC44tzU6_?usp=sharing)\n",
        "## Master Generative AI in 6 Weeks\n",
        "**What You'll Learn:**\n",
        "- Build with Latest LLMs\n",
        "- Create Custom AI Apps\n",
        "- Learn from Industry Experts\n",
        "- Join Innovation Community\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "*Empowering the Next Generation of AI Innovators"
      ],
      "metadata": {
        "id": "JVh17WmRh7_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FLAML: A Lightweight Library for ML & AI\n",
        "\n",
        "FLAML is a lightweight Python library designed for efficient automation of machine learning and AI operations. It streamlines workflows involving large language models and machine learning algorithms, optimizing their performance with minimal computational resources. FLAML supports rapid and economical automatic tuning, effectively managing large search spaces with complex constraints and early stopping. Additionally, it enables the development of next-generation GPT-X applications through a generic multi-agent conversation framework, seamlessly integrating LLMs, tools, and human input."
      ],
      "metadata": {
        "id": "-jNIGqPch71e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Setup and Installation**"
      ],
      "metadata": {
        "id": "hccAlSCqh7pb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxoBtjlaZfSQ"
      },
      "outputs": [],
      "source": [
        "%pip install flaml[autogen]~=2.0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Set your API Endpoint**"
      ],
      "metadata": {
        "id": "VqWer0Sui3Yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "config_list = [\n",
        "    {\n",
        "        'model': 'gpt-4',\n",
        "        'api_key':OPENAI_API_KEY,\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "QbypAvA6bpqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**AgentChat two users**"
      ],
      "metadata": {
        "id": "91jGmlhHi_o8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_config_list = [config for config in config_list if config['model'] in [\"gpt-4\", \"gpt-4-32k\"]]\n",
        "\n",
        "# Pass the filtered config list to autogen\n",
        "llm_config = {\"config_list\": filtered_config_list, \"seed\": 42}"
      ],
      "metadata": {
        "id": "QW9UAPhFex9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Construct Agents**\n"
      ],
      "metadata": {
        "id": "IF6Ay7QGjJ1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_expert(message):\n",
        "    assistant_for_expert = autogen.AssistantAgent(\n",
        "        name=\"assistant_for_expert\",\n",
        "        llm_config={\n",
        "            \"temperature\": 0,\n",
        "            \"config_list\": config_list,\n",
        "        },\n",
        "    )\n",
        "    expert = autogen.UserProxyAgent(\n",
        "        name=\"expert\",\n",
        "        human_input_mode=\"ALWAYS\",\n",
        "        code_execution_config={\"work_dir\": \"expert\"},\n",
        "    )\n",
        "\n",
        "    expert.initiate_chat(assistant_for_expert, message=message)\n",
        "    expert.stop_reply_at_receive(assistant_for_expert)\n",
        "    # expert.human_input_mode, expert.max_consecutive_auto_reply = \"NEVER\", 0\n",
        "    # final message sent from the expert\n",
        "    expert.send(\"summarize the solution and explain the answer in an easy-to-understand way\", assistant_for_expert)\n",
        "    # return the last message the expert received\n",
        "    return expert.last_message()[\"content\"]"
      ],
      "metadata": {
        "id": "0mn3LTC6cIQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assistant_for_student = autogen.AssistantAgent(\n",
        "    name=\"assistant_for_student\",\n",
        "    system_message=\"You are a helpful assistant. Reply TERMINATE when the task is done.\",\n",
        "    llm_config={\n",
        "        \"request_timeout\": 600,\n",
        "        \"seed\": 42,\n",
        "        # Excluding azure openai endpoints from the config list.\n",
        "        # Change to `exclude=\"openai\"` to exclude openai endpoints, or remove the `exclude` argument to include both.\n",
        "        \"config_list\": autogen.config_list_openai_aoai(exclude=\"aoai\"),\n",
        "        \"model\": \"gpt-4-0613\",  # make sure the endpoint you use supports the model\n",
        "        \"temperature\": 0,\n",
        "        \"functions\": [\n",
        "            {\n",
        "                \"name\": \"ask_expert\",\n",
        "                \"description\": \"ask expert when you can't solve the problem satisfactorily.\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"message\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"question to ask expert. Make sure the question include enough context, such as the code and the execution result. The expert does not know the conversation between you and the user, unless you share the conversation with the expert.\",\n",
        "                        },\n",
        "                    },\n",
        "                    \"required\": [\"message\"],\n",
        "                },\n",
        "            }\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "\n",
        "student = autogen.UserProxyAgent(\n",
        "    name=\"student\",\n",
        "    human_input_mode=\"TERMINATE\",\n",
        "    max_consecutive_auto_reply=10,\n",
        "    code_execution_config={\"work_dir\": \"student\"},\n",
        "    function_map={\"ask_expert\": ask_expert},\n",
        ")"
      ],
      "metadata": {
        "id": "u7MahgRVcP_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Perform a task**"
      ],
      "metadata": {
        "id": "tkcl5-3ajS8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student.initiate_chat(\n",
        "    assistant_for_student,\n",
        "    message=\"\"\"Find $a + b + c$, given that $x+y \\\\neq -1$ and\n",
        "\\\\begin{align}\n",
        "\tax + by + c & = x + 7,\\\\\n",
        "\ta + bx + cy & = 2x + 6y,\\\\\n",
        "\tay + b + cx & = 4x + y.\n",
        "\\\\end{align}.\n",
        "\"\"\",\n",
        ")"
      ],
      "metadata": {
        "id": "4vKWj1L1fHQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Agentchat groupchat**"
      ],
      "metadata": {
        "id": "Ap4RINunjdoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_proxy = autogen.UserProxyAgent(\n",
        "   name=\"User_proxy\",\n",
        "   system_message=\"A human admin.\",\n",
        "   code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n",
        "   human_input_mode=\"TERMINATE\"\n",
        ")\n",
        "coder = autogen.AssistantAgent(\n",
        "    name=\"Coder\",\n",
        "    llm_config=llm_config,\n",
        ")\n",
        "pm = autogen.AssistantAgent(\n",
        "    name=\"Product_manager\",\n",
        "    system_message=\"Creative in software product ideas.\",\n",
        "    llm_config=llm_config,\n",
        ")\n",
        "groupchat = autogen.GroupChat(agents=[user_proxy, coder, pm], messages=[], max_round=12)\n",
        "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
      ],
      "metadata": {
        "id": "ZWneD_yQfxnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Start Chat**"
      ],
      "metadata": {
        "id": "RUULQiK-jnJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_proxy.initiate_chat(manager, message=\"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\")\n"
      ],
      "metadata": {
        "id": "4moysi89g6oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Agentchat planning**"
      ],
      "metadata": {
        "id": "uQUaeJedjzJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "planner = autogen.AssistantAgent(\n",
        "    name=\"planner\",\n",
        "    llm_config={\"config_list\": config_list},\n",
        "    # the default system message of the AssistantAgent is overwritten here\n",
        "    system_message=\"You are a helpful AI assistant. You suggest coding and reasoning steps for another AI assistant to accomplish a task. Do not suggest concrete code. For any action beyond writing code or reasoning, convert it to a step which can be implemented by writing code. For example, the action of browsing the web can be implemented by writing code which reads and prints the content of a web page. Finally, inspect the execution result. If the plan is not good, suggest a better plan. If the execution is wrong, analyze the error and suggest a fix.\"\n",
        ")\n",
        "planner_user = autogen.UserProxyAgent(\n",
        "    name=\"planner_user\",\n",
        "    max_consecutive_auto_reply=0,  # terminate without auto-reply\n",
        "    human_input_mode=\"NEVER\",\n",
        ")\n",
        "\n",
        "def ask_planner(message):\n",
        "    planner_user.initiate_chat(planner, message=message)\n",
        "    # return the last message received from the planner\n",
        "    return planner_user.last_message()[\"content\"]"
      ],
      "metadata": {
        "id": "9n3JOTh2hqet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Construct Agents**"
      ],
      "metadata": {
        "id": "NjC5vO5Jj9uR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an AssistantAgent instance named \"assistant\"\n",
        "assistant = autogen.AssistantAgent(\n",
        "    name=\"assistant\",\n",
        "    llm_config={\n",
        "        \"temperature\": 0,\n",
        "        \"request_timeout\": 600,\n",
        "        \"seed\": 42,\n",
        "        \"model\": \"gpt-4-0613\",\n",
        "        \"config_list\": autogen.config_list_openai_aoai(exclude=\"aoai\"),\n",
        "        \"functions\": [\n",
        "            {\n",
        "                \"name\": \"ask_planner\",\n",
        "                \"description\": \"ask planner to: 1. get a plan for finishing a task, 2. verify the execution result of the plan and potentially suggest new plan.\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"message\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"question to ask planner. Make sure the question include enough context, such as the code and the execution result. The planner does not know the conversation between you and the user, unless you share the conversation with the planner.\",\n",
        "                        },\n",
        "                    },\n",
        "                    \"required\": [\"message\"],\n",
        "                },\n",
        "            },\n",
        "        ],\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "id": "vBiogLkohqvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_proxy = autogen.UserProxyAgent(\n",
        "    name=\"user_proxy\",\n",
        "    human_input_mode=\"TERMINATE\",\n",
        "    max_consecutive_auto_reply=10,\n",
        "    # is_termination_msg=lambda x: \"content\" in x and x[\"content\"] is not None and x[\"content\"].rstrip().endswith(\"TERMINATE\"),\n",
        "    code_execution_config={\"work_dir\": \"planning\"},\n",
        "    function_map={\"ask_planner\": ask_planner},\n",
        ")"
      ],
      "metadata": {
        "id": "y0WAeguDhtih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Start Chat**"
      ],
      "metadata": {
        "id": "FsmenAMvjvhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_proxy.initiate_chat(\n",
        "    assistant,\n",
        "    message=\"\"\"Suggest a fix to an open good first issue of flaml\"\"\",\n",
        ")"
      ],
      "metadata": {
        "id": "5ueLpzWhhv2Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}