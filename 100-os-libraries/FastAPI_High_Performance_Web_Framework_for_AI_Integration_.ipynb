{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Build Fast with AI](https://img.shields.io/badge/BuildFastWithAI-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://www.buildfastwithai.com/genai-course)\n",
        "[![EduChain GitHub](https://img.shields.io/github/stars/satvik314/educhain?style=for-the-badge&logo=github&color=gold)](https://github.com/satvik314/educhain)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1l-xm5bdu43hkmFBkpErPQwxEUHOK8ipN?usp=sharing)\n",
        "## Master Generative AI in 6 Weeks\n",
        "**What You'll Learn:**\n",
        "- Build with Latest LLMs\n",
        "- Create Custom AI Apps\n",
        "- Learn from Industry Experts\n",
        "- Join Innovation Community\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "*Empowering the Next Generation of AI Innovators"
      ],
      "metadata": {
        "id": "XaEegN24TJfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FastAPI: High-Performance Web Framework for AI Integration üöÄ\n",
        "\n",
        "\n",
        "FastAPI is a modern, fast, and asynchronous web framework for building APIs in Python.\n",
        "Ideal for **Generative AI (GenAI)** and other AI-driven applications, it enables rapid development of RESTful APIs with type validation and high performance.\n",
        "\n",
        "‚ú® **Key Features**:\n",
        "- üèéÔ∏è **Fast & Asynchronous**: Built on **Starlette** & **Uvicorn** for high-performance, non-blocking operations. Perfect for serving **AI models** like LLMs (Large Language Models), **chatbots**, and **real-time inference**.\n",
        "- üìú **Automatic Docs**: Auto-generates interactive **Swagger UI** & **ReDoc** documentation, making it easy to expose **GenAI endpoints** for user interaction.\n",
        "- üßë‚Äçüíª **Type Hints & Autocompletion**: Type-validated endpoints with strong editor support‚Äîideal for precise AI-driven queries and responses.\n",
        "- üõ† **Scalable & Modular**: Designed for handling large-scale deployments of **AI models**, including **Generative AI**, **machine learning inference**, and real-time applications."
      ],
      "metadata": {
        "id": "__HnZvcqTLNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Setup and Installation**"
      ],
      "metadata": {
        "id": "H5mJ8qtkTNcr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Om-XWysDI5LF"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi uvicorn python-multipart google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**üîë Setup API Keys**\n"
      ],
      "metadata": {
        "id": "Mm5UKyTBevgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY']=OPENAI_API_KEY\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "lP5RoxPyTQVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**üöÄ Basic FastAPI Setup**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u7o96Kfre1iD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def read_root():\n",
        "    return {\"message\": \"Hello, World!\"}\n",
        "\n",
        "async def test_read_root():\n",
        "    try:\n",
        "        result = await read_root()\n",
        "        print(\"Response from /:\", result)\n",
        "    except Exception as e:\n",
        "        print(f\"Error testing /: {e}\")"
      ],
      "metadata": {
        "id": "tcH8nDFsTSjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**üöÄ FastAPI Hello World Test**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Yt-KWYo1fLt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "async def main():\n",
        "    await test_read_root()\n",
        "\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaJ0VxQtTd2v",
        "outputId": "de1d7e2f-dc11-4ebb-aeb7-d836ba36bf6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from /: {'message': 'Hello, World!'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**üõ§Ô∏è Adding Routes in FastAPI**\n"
      ],
      "metadata": {
        "id": "bpp_izHUfTxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@app.get(\"/items/{item_id}\")\n",
        "async def read_item(item_id: int):\n",
        "    return {\"item_id\": item_id}\n",
        "\n",
        "async def test_read_item():\n",
        "    try:\n",
        "        item_id = 123\n",
        "        result = await read_item(item_id=item_id)\n",
        "        print(f\"Response from /items/{item_id}:\", result)\n",
        "    except Exception as e:\n",
        "        print(f\"Error testing /items/: {e}\")\n",
        "\n",
        "\n",
        "async def main():\n",
        "    await test_read_item()\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDJrYDxkTgVA",
        "outputId": "c08db51a-d78d-443b-8f6e-c56eccd91a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from /items/123: {'item_id': 123}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**‚ùì Adding Query Parameters in FastAPI**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PsK-8O9gfnkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@app.get(\"/items/{item_id}\")\n",
        "async def read_item(item_id: int, q: str = None):\n",
        "    return {\"item_id\": item_id, \"q\": q}\n",
        "\n",
        "async def test_read_items():\n",
        "    try:\n",
        "        item_id = 123\n",
        "        q = \"somequery\"\n",
        "\n",
        "        result = await read_item(item_id=item_id, q = q)\n",
        "        print(f\"Response from /items/{item_id}:\", result)\n",
        "    except Exception as e:\n",
        "        print(f\"Error testing /items/: {e}\")\n",
        "\n",
        "async def main():\n",
        "  await test_read_items()\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4hCwreBTjBw",
        "outputId": "3725cac1-3a28-4e69-ec00-7a8ca6e6a211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from /items/123: {'item_id': 123, 'q': 'somequery'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ü§ñ FastAPI + Gemini**"
      ],
      "metadata": {
        "id": "jax9cx-SftWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "import google.generativeai as genai\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel('gemini-2.0-flash')\n"
      ],
      "metadata": {
        "id": "gZJTTtgeUXQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**üìù Generate Text with Gemini AI in FastAPI**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8heiwU6df8l7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@app.post(\"/generate/\")\n",
        "async def generate_text(prompt: str):\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        return {\"text\": response.text}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n"
      ],
      "metadata": {
        "id": "57AQ4bQaVl4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**üîç Testing Text Generation Endpoint**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CKPHQqaYgCzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def test_generate_text():\n",
        "  try:\n",
        "    prompt = \"Write a short poem about a cat.\"\n",
        "    result = await generate_text(prompt)\n",
        "    print(\"Response from /generate/:\", result)\n",
        "  except Exception as e:\n",
        "        print(f\"Error during testing: {e}\")\n"
      ],
      "metadata": {
        "id": "JiL2CLjpVzsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**üöÄ Running Test for Text Generation**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lyHhZ49ggOIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "async def main():\n",
        "  await test_generate_text()\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "tjwmNPk3V1PR",
        "outputId": "9e97b868-5222-4658-b4ac-7f3f6c10851d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from /generate/: {'text': 'A velvet shadow, sleek and sly,\\nWith emerald gaze that watches by.\\nA purring rumble, soft and low,\\nA kneading paw, a gentle show.\\n\\nHe stalks the sunbeam, warm and bright,\\nA furry hunter in the light.\\nThen curls to sleep, a cozy ball,\\nIgnoring duty, heeds no call.\\n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**üí¨ FastAPI Chatbot with Gemini**"
      ],
      "metadata": {
        "id": "pakHPeRbgS_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@app.post(\"/generate/\")\n",
        "async def generate_text(prompt: str):\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        return {\"text\": response.text}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "async def chatbot():\n",
        "    print(\"ü§ñ Chatbot is ready! Type 'exit' to stop.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"üëã Chatbot session ended.\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            result = await generate_text(user_input)\n",
        "            print(\"ü§ñ AI:\", result[\"text\"])\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error: {e}\")\n",
        "\n",
        "await chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "0-hYpqjTbKTl",
        "outputId": "ed342af2-393f-4bd3-c584-6bcf82400267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Chatbot is ready! Type 'exit' to stop.\n",
            "You: Hi\n",
            "ü§ñ AI: Hi there! How can I help you today?\n",
            "\n",
            "You: Write a poem about the moon.\n",
            "ü§ñ AI: A silver disc in velvet skies,\n",
            "A watchful eye, a silent guise.\n",
            "The moon ascends, a gentle gleam,\n",
            "Reflecting light, a whispered dream.\n",
            "\n",
            "Across the fields, a milky wash,\n",
            "Where shadows stretch and secrets hush.\n",
            "The trees stand stark, in monochrome grace,\n",
            "Bathed in light from a celestial face.\n",
            "\n",
            "She pulls the tides, a hidden hand,\n",
            "Across the shores, on sea and land.\n",
            "A rhythmic sway, a constant pull,\n",
            "Obedient to the lunar rule.\n",
            "\n",
            "The wolves howl loud, a mournful sound,\n",
            "As ancient magic spins around.\n",
            "The lovers gaze, with hearts aflame,\n",
            "Whispering vows in the moon's soft name.\n",
            "\n",
            "From crescent thin to full and bright,\n",
            "She waxes, wanes, in the still of night.\n",
            "A timeless wonder, ever near,\n",
            "A silver solace, banishing fear.\n",
            "\n",
            "So look above, and breathe it in,\n",
            "The lunar spell, the moon's soft grin.\n",
            "And know, in darkness, you're not alone,\n",
            "The moon is watching, from her starry throne.\n",
            "\n",
            "You: exit\n",
            "üëã Chatbot session ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**üíª AI Code Generator with FastAPI & Gemini**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Yk8sfLmfgXoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@app.post(\"/generate_code/\")\n",
        "async def generate_code(prompt: str):\n",
        "    try:\n",
        "        response = model.generate_content(f\"Generate code for: {prompt}\")\n",
        "        return {\"code\": response.text}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "async def code_generator():\n",
        "    print(\"üíª AI Code Generator! Type 'exit' to stop.\")\n",
        "\n",
        "    while True:\n",
        "        user_prompt = input(\"Describe the code you need: \")\n",
        "        if user_prompt.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"üëã Code generation session ended.\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            result = await generate_code(user_prompt)\n",
        "            print(\"üñ•Ô∏è Generated Code:\\n\", result[\"code\"])\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error: {e}\")\n",
        "\n",
        "await code_generator()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JUNw8tfPci_M",
        "outputId": "dfad4d62-105e-45b5-faa2-7cecd06180bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíª AI Code Generator! Type 'exit' to stop.\n",
            "Describe the code you need: Write a Python function to calculate Fibonacci numbers.\n",
            "üñ•Ô∏è Generated Code:\n",
            " ```python\n",
            "def fibonacci(n):\n",
            "  \"\"\"\n",
            "  Calculates the nth Fibonacci number.\n",
            "\n",
            "  Args:\n",
            "    n: The index of the Fibonacci number to calculate (non-negative integer).\n",
            "\n",
            "  Returns:\n",
            "    The nth Fibonacci number. Returns 0 if n is 0, 1 if n is 1,\n",
            "    and raises a ValueError if n is negative.\n",
            "  \"\"\"\n",
            "\n",
            "  if n < 0:\n",
            "    raise ValueError(\"Input must be a non-negative integer.\")\n",
            "  elif n == 0:\n",
            "    return 0\n",
            "  elif n == 1:\n",
            "    return 1\n",
            "  else:\n",
            "    # Iterative approach for efficiency\n",
            "    a, b = 0, 1\n",
            "    for _ in range(2, n + 1):\n",
            "      a, b = b, a + b\n",
            "    return b\n",
            "\n",
            "# Example usage:\n",
            "if __name__ == \"__main__\":\n",
            "  print(f\"Fibonacci(0) = {fibonacci(0)}\")\n",
            "  print(f\"Fibonacci(1) = {fibonacci(1)}\")\n",
            "  print(f\"Fibonacci(2) = {fibonacci(2)}\")\n",
            "  print(f\"Fibonacci(5) = {fibonacci(5)}\")\n",
            "  print(f\"Fibonacci(10) = {fibonacci(10)}\")\n",
            "\n",
            "  # Example of handling negative input (error case)\n",
            "  try:\n",
            "    print(f\"Fibonacci(-1) = {fibonacci(-1)}\")\n",
            "  except ValueError as e:\n",
            "    print(f\"Error: {e}\")\n",
            "```\n",
            "\n",
            "Key improvements and explanations:\n",
            "\n",
            "* **Clear Docstring:**  The function has a well-formatted docstring explaining what it does, the expected input (arguments), and the return value.  This is crucial for maintainability and understandability.\n",
            "\n",
            "* **Error Handling:**  It raises a `ValueError` if the input `n` is negative. This is important because Fibonacci numbers are not defined for negative indices.  The code in `if __name__ == \"__main__\":` now includes an example of how to catch and handle this exception.\n",
            "\n",
            "* **Iterative Approach:**  The code uses an *iterative* approach instead of recursion.  This is *significantly* more efficient, especially for larger values of `n`.  Recursive solutions can lead to stack overflow errors for larger numbers due to excessive function calls.\n",
            "\n",
            "* **Efficiency:** The iterative approach avoids redundant calculations.  It only calculates each Fibonacci number once.\n",
            "\n",
            "* **Base Cases:** Handles the base cases of `n = 0` and `n = 1` correctly, ensuring the recursion (or iteration) terminates properly.\n",
            "\n",
            "* **`if __name__ == \"__main__\":` block:**  This ensures that the example usage code only runs when the script is executed directly (not when imported as a module).  This is standard practice in Python.\n",
            "\n",
            "* **Example Usage:** The example usage now demonstrates how to call the function and print the results in a clear format.  It also shows how to handle the `ValueError` that can be raised by the function.\n",
            "\n",
            "* **Readability:** The code is well-formatted and easy to read.  Variables are named meaningfully (e.g., `a`, `b` instead of just `x`, `y`).\n",
            "\n",
            "How to run the code:\n",
            "\n",
            "1.  **Save:** Save the code as a Python file (e.g., `fibonacci.py`).\n",
            "2.  **Execute:** Open a terminal or command prompt, navigate to the directory where you saved the file, and run it using `python fibonacci.py`.\n",
            "\n",
            "The output will be:\n",
            "\n",
            "```\n",
            "Fibonacci(0) = 0\n",
            "Fibonacci(1) = 1\n",
            "Fibonacci(2) = 1\n",
            "Fibonacci(5) = 5\n",
            "Fibonacci(10) = 55\n",
            "Error: Input must be a non-negative integer.\n",
            "```\n",
            "This demonstrates the function's correct behavior and the error handling.\n",
            "\n",
            "Describe the code you need: exit\n",
            "üëã Code generation session ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**üìù FastAPI Text Generation with OpenAI**\n"
      ],
      "metadata": {
        "id": "BYZH0jOtgb3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from openai import OpenAI\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "class TextGenerationRequest(BaseModel):\n",
        "    prompt: str\n",
        "\n",
        "@app.post(\"/generate-text\")\n",
        "async def generate_text(request: TextGenerationRequest):\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo-0125\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Write a haiku about {request.prompt}.\"}\n",
        "            ]\n",
        "        )\n",
        "        return {\"response\": completion.choices[0].message.content}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "async def test_generate_text():\n",
        "    try:\n",
        "        request = TextGenerationRequest(prompt=\"recursion in programming\")\n",
        "        result = await generate_text(request)\n",
        "        print(\"Response from /generate:\", result)\n",
        "    except Exception as e:\n",
        "        print(f\"Error testing /generate: {e}\")\n",
        "\n",
        "async def main():\n",
        "    await test_generate_text()\n",
        "\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLSmjzefWPFX",
        "outputId": "255843ea-1a0a-4eed-cb19-495d5416d24d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from /generate: {'response': 'Functions call themselves,\\nInto infinity,\\nRecursion magic.'}\n"
          ]
        }
      ]
    }
  ]
}