{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Build Fast with AI](https://img.shields.io/badge/BuildFastWithAI-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://www.buildfastwithai.com/genai-course)\n",
        "[![EduChain GitHub](https://img.shields.io/github/stars/satvik314/educhain?style=for-the-badge&logo=github&color=gold)](https://github.com/satvik314/educhain)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/13wy9bZPr1assJCxZdbgSu1vf5RT924tF?usp=sharing)\n",
        "## Master Generative AI in 8 Weeks\n",
        "**What You'll Learn:**\n",
        "- Build with Latest LLMs\n",
        "- Create Custom AI Apps\n",
        "- Learn from Industry Experts\n",
        "- Join Innovation Community\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "*Empowering the Next Generation of AI Innovators"
      ],
      "metadata": {
        "id": "oklXX1rHimcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangSmith: Building and Optimizing LLM Applications üìöü§ñ\n",
        "\n",
        "**LangSmith** is an open-source library by LangChain. It‚Äôs designed to streamline the development, debugging, and optimization of applications powered by **large language models (LLMs)**. With tools for tracing, evaluation, and prompt management, LangSmith makes it easier to create robust and efficient LLM-based systems.\n",
        "\n",
        "## Key Features üöÄ\n",
        "\n",
        "- **Tracing and Debugging** üîç: Monitor every step of your LLM‚Äôs execution to identify and fix issues quickly with detailed logs.\n",
        "- **Performance Evaluation** üìä: Test and score your model‚Äôs outputs against datasets to measure accuracy and effectiveness.\n",
        "- **Prompt Management** ‚úçÔ∏è: Store, version, and reuse prompts across projects, enabling consistent and efficient workflows.\n",
        "- **Fine-Tuning Support** üîß: Export data and evaluate improvements to tailor models for specific tasks or datasets.\n",
        "- **Seamless Integration** üåê: Works with LangChain and models like OpenAI‚Äôs GPT, making it versatile for various LLM applications."
      ],
      "metadata": {
        "id": "atP2gXFcimYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Installing LangSmith Dependencies** üì¶"
      ],
      "metadata": {
        "id": "tHCEfCoOimSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langsmith langchain_openai langchain_core"
      ],
      "metadata": {
        "id": "J7Sa71Pp4gei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Setting Up Environment Variables** üåç"
      ],
      "metadata": {
        "id": "SqGG8q7dtlE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY']=userdata.get('OPENAI_API_KEY')\n",
        "os.environ['LANGSMITH_API_KEY']=userdata.get('LANGSMITH_API_KEY')"
      ],
      "metadata": {
        "id": "DzAE3JUAin8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Basic LLM Interaction with LangSmith Client** ü§ñ"
      ],
      "metadata": {
        "id": "qrkf-BWhtxF_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpz1ZaAR20qA"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "client = Client()\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
        "\n",
        "response = llm.invoke(\"Hello, how can I use LangSmith?\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Simple RAG with LangSmith-Wrapped OpenAI Client** üîç"
      ],
      "metadata": {
        "id": "FyGBLfBBt7Th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from langsmith.wrappers import wrap_openai\n",
        "\n",
        "openai_client = wrap_openai(OpenAI())\n",
        "\n",
        "\n",
        "def retriever(query: str):\n",
        "    results = [\"Harrison worked at Kensho\"]\n",
        "    return results\n",
        "\n",
        "def rag(question):\n",
        "    docs = retriever(question)\n",
        "    system_message = \"\"\"Answer the users question using only the provided information below:\n",
        "\n",
        "    {docs}\"\"\".format(docs=\"\\n\".join(docs))\n",
        "\n",
        "    return openai_client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": question},\n",
        "        ],\n",
        "        model=\"gpt-4o-mini\",\n",
        "    )"
      ],
      "metadata": {
        "id": "_fHQfxsLjtB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag(\"where did harrison work\")"
      ],
      "metadata": {
        "id": "I3bDEymd42Fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "979c42d8-6e84-4473-d7ce-99445a5aa2ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-B89ZRRO9uBCXGdbVekZ1eru8OndGc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Harrison worked at Kensho.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1741283113, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_06737a9306', usage=CompletionUsage(completion_tokens=8, prompt_tokens=34, total_tokens=42, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Traceable RAG with LangSmith-Wrapped OpenAI Client** üïµÔ∏è"
      ],
      "metadata": {
        "id": "joZJvhrMuAKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from langsmith import traceable\n",
        "from langsmith.wrappers import wrap_openai\n",
        "\n",
        "openai_client = wrap_openai(OpenAI())\n",
        "\n",
        "def retriever(query: str):\n",
        "    results = [\"Harrison worked at Kensho\"]\n",
        "    return results\n",
        "\n",
        "@traceable\n",
        "def rag(question):\n",
        "    docs = retriever(question)\n",
        "    system_message = \"\"\"Answer the users question using only the provided information below:\n",
        "\n",
        "    {docs}\"\"\".format(docs=\"\\n\".join(docs))\n",
        "\n",
        "    return openai_client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": question},\n",
        "        ],\n",
        "        model=\"gpt-4o-mini\",\n",
        "    )"
      ],
      "metadata": {
        "id": "lMTYtOMx0OpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag(\"where did harrison work\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjVwpy_2kl0r",
        "outputId": "ba3e188d-ce7e-4f54-f6b4-33b9336131d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-B89cVnSPyap5HAdEaz4PkSgAt2oz5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Harrison worked at Kensho.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1741283303, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_06737a9306', usage=CompletionUsage(completion_tokens=8, prompt_tokens=34, total_tokens=42, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Storing a Prompt in LangSmith** ‚úçÔ∏è"
      ],
      "metadata": {
        "id": "zUbo9W_8uGhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "client = Client()\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate([\n",
        "(\"system\", \"You are a helpful chatbot.\"),\n",
        "(\"user\", \"{question}\"),\n",
        "])\n",
        "\n",
        "\n",
        "client.push_prompt(\"my-prompt\", object=prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JklkeNvVkoPw",
        "outputId": "14c6e6a7-a047-4772-ba45-584f99750258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://smith.langchain.com/prompts/my-prompt/65a3f7db?organizationId=4ed856cd-56b8-4a3f-9980-4ce959ff08e0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Using a Stored Prompt with OpenAI**"
      ],
      "metadata": {
        "id": "uT-IbW2luMuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "from openai import OpenAI\n",
        "from langchain_core.messages import convert_to_openai_messages\n",
        "\n",
        "\n",
        "client = Client()\n",
        "oai_client = OpenAI()\n",
        "\n",
        "\n",
        "prompt = client.pull_prompt(\"my-prompt\")\n",
        "\n",
        "formatted_prompt = prompt.invoke({\"question\": \"What is the color of the sky?\"})\n",
        "\n",
        "\n",
        "response = oai_client.chat.completions.create(\n",
        "model=\"gpt-4o\",\n",
        "messages=convert_to_openai_messages(formatted_prompt.messages),\n",
        ")"
      ],
      "metadata": {
        "id": "NplbNfGxowP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "mscgZ82zo966"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Updating a Prompt with Spanish Tag** üá™üá∏"
      ],
      "metadata": {
        "id": "NKzAgom_uai3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_prompt = ChatPromptTemplate([\n",
        "(\"system\", \"You are a helpful chatbot. Respond in Spanish.\"),\n",
        "(\"user\", \"{question}\"),\n",
        "])\n",
        "\n",
        "client.push_prompt(\"my-prompt\", object=new_prompt, tags=[\"Spanish\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "15pOJhhwpLH3",
        "outputId": "9f3fcac4-a95a-4f9f-91aa-761f70741a00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://smith.langchain.com/prompts/my-prompt/0ddd23fa?organizationId=4ed856cd-56b8-4a3f-9980-4ce959ff08e0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Generating a Spanish Response with Stored Prompt** üí¨"
      ],
      "metadata": {
        "id": "WKNkkwwruhk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "updated_prompt = client.pull_prompt(\"my-prompt\")\n",
        "\n",
        "formatted_spanish_prompt = updated_prompt.invoke({\"question\": \"¬øCu√°l es el capital de Espa√±a?\"})\n",
        "\n",
        "spanish_response = oai_client.chat.completions.create(model=\"gpt-4o\", messages=convert_to_openai_messages(formatted_spanish_prompt.messages))\n",
        "\n",
        "print(spanish_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxV51PZgpd_x",
        "outputId": "358b6cda-513b-4888-8e9b-b2198b6d9ec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El capital de Espa√±a es Madrid.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Defining a Two-Step Prompt Chain** üåü"
      ],
      "metadata": {
        "id": "P6NnIFSGu7AD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "LRb_Ixitp1Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Initializing LangSmith Client and LLM** ‚öôÔ∏è"
      ],
      "metadata": {
        "id": "txWfIE8ru_Hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = Client()\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "O4aIxaSWrByk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Using Stored Prompts for Summarization** ‚úÇÔ∏è"
      ],
      "metadata": {
        "id": "mlCb2LAtvIaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step1 = ChatPromptTemplate.from_messages([(\"system\", \"Summarize this: {text}\"), (\"user\", \"{text}\")])\n",
        "step2 = ChatPromptTemplate.from_messages([(\"system\", \"Make it concise: {summary}\"), (\"user\", \"{summary}\")])"
      ],
      "metadata": {
        "id": "6cj8hMBJrD-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pushing Prompts to LangSmith** üì§"
      ],
      "metadata": {
        "id": "BOlSwomtvUIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.push_prompt(\"summarize-step1\", object=step1)\n",
        "client.push_prompt(\"summarize-step2\", object=step2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bbbpO6RNrHhb",
        "outputId": "9a261c3c-5467-4afa-ce37-08f493d4d32d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://smith.langchain.com/prompts/summarize-step2/a8b0d632?organizationId=4ed856cd-56b8-4a3f-9980-4ce959ff08e0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Printing Summarization Results** üñ®Ô∏è"
      ],
      "metadata": {
        "id": "xw9-VubVvY9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The quick brown fox jumps over the lazy dog near the riverbank.\"\n",
        "summary_prompt = client.pull_prompt(\"summarize-step1\").invoke({\"text\": text})\n",
        "summary = llm.invoke(summary_prompt.messages).content\n",
        "concise_prompt = client.pull_prompt(\"summarize-step2\").invoke({\"summary\": summary})\n",
        "concise_summary = llm.invoke(concise_prompt.messages).content\n",
        "\n",
        "print(f\"Original: {text}\\nSummary: {summary}\\nConcise: {concise_summary}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJzLtdlXrKLI",
        "outputId": "7998916a-e248-4a15-ef10-d7333aad9606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: The quick brown fox jumps over the lazy dog near the riverbank.\n",
            "Summary: A quick brown fox jumps over a lazy dog by the river.\n",
            "Concise: Brown fox jumps over lazy dog by river.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creating a Math Question Dataset** üìö"
      ],
      "metadata": {
        "id": "kd6ODWEfvgaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset = client.create_dataset(\"math-question\", description=\"Basic math queries\")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\": \"What‚Äôs 5 + 3?\"}, {\"question\": \"What‚Äôs 10 - 4?\"}],\n",
        "    outputs=[{\"answer\": \"8\"}, {\"answer\": \"6\"}],\n",
        "    dataset_id=dataset.id\n",
        ")\n",
        "\n",
        "print(\"Dataset created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pESZjxm9rtIp",
        "outputId": "4222db1e-7ee8-45b7-a7a1-3bcc994a20c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Searching Runs by Name in a Project** üîç"
      ],
      "metadata": {
        "id": "R1qQtqFxvlDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "runs = client.list_runs(project_name=\"default\", name_contains=\"capital\")  # Replace \"default\" with your project name if known\n",
        "\n",
        "for run in runs:\n",
        "    print(f\"Run: {run.name}, Answer: {run.outputs.get('answer', 'N/A')}\")\n",
        "\n",
        "print(\"Search complete!\")"
      ],
      "metadata": {
        "id": "rEsI5nqfrwfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Logging a Run with Astronomy Tag** üåå"
      ],
      "metadata": {
        "id": "ca0EpPFhvogu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "client = Client()\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "response = llm.invoke(\"What‚Äôs the largest planet?\").content\n",
        "run = client.create_run(\n",
        "    name=\"planet-query\",\n",
        "    run_type=\"llm\",\n",
        "    inputs={\"question\": \"What‚Äôs the largest planet?\"},\n",
        "    outputs={\"answer\": response},\n",
        "    tags=[\"astronomy\"]\n",
        ")\n",
        "\n",
        "print(f\"Logged answer: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCKmEtPcsD3j",
        "outputId": "b06786f1-48d3-4153-fe00-639575a484f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logged answer: Jupiter is the largest planet in our solar system.\n"
          ]
        }
      ]
    }
  ]
}