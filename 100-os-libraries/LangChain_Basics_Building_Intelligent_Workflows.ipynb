{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Build Fast with AI](https://img.shields.io/badge/BuildFastWithAI-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://www.buildfastwithai.com/genai-course)\n",
        "[![EduChain GitHub](https://img.shields.io/github/stars/satvik314/educhain?style=for-the-badge&logo=github&color=gold)](https://github.com/satvik314/educhain)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1kukhgpPWKHRrtHGk3XQ2-gUKnX784i4t#scrollTo=gR6qjC_mfMQy)\n",
        "## Master Generative AI in 6 Weeks\n",
        "**What You'll Learn:**\n",
        "- Build with Latest LLMs\n",
        "- Create Custom AI Apps\n",
        "- Learn from Industry Experts\n",
        "- Join Innovation Community\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "*Empowering the Next Generation of AI Innovators"
      ],
      "metadata": {
        "id": "8z9e3cJic3Mt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  🦜️🔗 LangChain\n",
        "LangChain is a framework for developing applications powered by large language models (LLMs)."
      ],
      "metadata": {
        "id": "RNqyuYwKc6X-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LangChain simplifies every stage of the LLM application lifecycle:\n",
        "\n",
        "* Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\n",
        "* Productionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\n",
        "* Deployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform."
      ],
      "metadata": {
        "id": "k47vA72NbbwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "rNQkdK1FdBC7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PVvlozpc2dR",
        "outputId": "906389c9-cee8-4697-d85c-994a06da050b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-community langchain_openai faiss-gpu duckduckgo-search wikipedia --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setup API Keys\n"
      ],
      "metadata": {
        "id": "uKFyf2S8aWUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "# Set API key\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "epqJRf0YdISS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Basic: Simple LLM Chain\n",
        "####This example shows how to create a basic chain that uses an LLM to generate responses."
      ],
      "metadata": {
        "id": "UYqU7m22dNRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'])\n",
        "\n",
        "# Create a prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Create a simple chain\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run the chain\n",
        "response = chain.invoke({\"input\": \"What is LangChain?\"})\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kL1qW5idLq2",
        "outputId": "942507e3-7199-4958-891e-0fcdbedf6ce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain is a blockchain platform that aims to revolutionize the language industry by providing a decentralized marketplace for language services. It enables language professionals such as translators, interpreters, and language teachers to connect directly with clients without the need for intermediaries. The platform utilizes blockchain technology to ensure secure and transparent transactions, as well as to provide a reliable reputation system for users. Overall, LangChain seeks to improve efficiency, transparency, and trust in the language services industry.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Intermediate: Question Answering with RAG\n",
        "This example demonstrates how to implement Retrieval-Augmented Generation (RAG) to answer questions based on your documents."
      ],
      "metadata": {
        "id": "PKud8OSOdmsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###FAISS (Facebook AI Similarity Search) 📚\n",
        "\n",
        "FAISS is a library developed by Facebook AI Research for efficient similarity search and clustering of dense vectors. It's particularly useful in machine learning applications, especially for nearest neighbor search in large datasets."
      ],
      "metadata": {
        "id": "RIL086xGeO1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Load and process the document\n",
        "loader = TextLoader(\"/content/info.txt\")\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(documents)\n",
        "\n",
        "# Create vector store\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "\n",
        "# Create retrieval chain\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Create prompt template\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Create the chain\n",
        "model = ChatOpenAI()\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Use the chain\n",
        "response = chain.invoke(\"What does the document say about X?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "s5MggiDWdTVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Advanced: Creating a Chatbot with Memory\n",
        "\n",
        "This example shows how to create a chatbot that remembers conversation history."
      ],
      "metadata": {
        "id": "VWOGfXgaeqnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# Initialize the chat model\n",
        "chat = ChatOpenAI()\n",
        "\n",
        "# Create a prompt template with memory\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Initialize memory\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "# Create the chain\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=lambda x: memory.load_memory_variables({})[\"history\"]\n",
        "    )\n",
        "    | prompt\n",
        "    | chat\n",
        ")\n",
        "\n",
        "# Example conversation\n",
        "messages = []\n",
        "for question in [\n",
        "    \"What is your name?\",\n",
        "    \"What did I just ask you?\",\n",
        "]:\n",
        "    response = chain.invoke({\"input\": question})\n",
        "    memory.save_context(\n",
        "        {\"input\": question},\n",
        "        {\"output\": response.content}\n",
        "    )\n",
        "    messages.append(response)"
      ],
      "metadata": {
        "id": "tl9e4LJWd6mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "id": "yNulIFGmelOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.Expert: Building an Agent with Tools\n",
        "\n",
        "This example demonstrates how to create an agent that can use tools to accomplish tasks."
      ],
      "metadata": {
        "id": "vc2to07KfHQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔄 LangChain Search Agent Flow:\n",
        "```\n",
        "[User Query] ➡️ [Agent] ➡️ [Tool Selection] ➡️ [Processing] ➡️ [Response]\n",
        "                 │\n",
        "                 ├──➡️ Search Tool\n",
        "                 │\n",
        "                 └──➡️ Wikipedia Tool\n"
      ],
      "metadata": {
        "id": "yX9IRMwEhX63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "from typing import Any, Dict, List\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import Tool, AgentExecutor, create_react_agent\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "class SearchAgent:\n",
        "    def __init__(self, openai_api_key: str = None, temperature: float = 0):\n",
        "        \"\"\"\n",
        "        Initialize the Search Agent with necessary components.\n",
        "\n",
        "        Args:\n",
        "            openai_api_key (str, optional): OpenAI API key. Defaults to None.\n",
        "            temperature (float, optional): LLM temperature parameter. Defaults to 0.\n",
        "        \"\"\"\n",
        "        # Initialize OpenAI LLM\n",
        "        self.llm = ChatOpenAI(\n",
        "            temperature=temperature,\n",
        "            openai_api_key=openai_api_key or os.getenv('OPENAI_API_KEY')\n",
        "        )\n",
        "\n",
        "        # Initialize tools with delay wrappers\n",
        "        self.tools = self._initialize_tools()\n",
        "\n",
        "        # Initialize prompt template\n",
        "        self.prompt = self._create_prompt_template()\n",
        "\n",
        "        # Initialize agent and executor\n",
        "        self.agent = create_react_agent(self.llm, self.tools, self.prompt)\n",
        "        self.agent_executor = AgentExecutor(\n",
        "            agent=self.agent,\n",
        "            tools=self.tools,\n",
        "            verbose=True,\n",
        "            handle_parsing_errors=True\n",
        "        )\n",
        "\n",
        "    def _search_with_delay(self, query: str) -> str:\n",
        "        \"\"\"\n",
        "        Perform a search with a built-in delay to avoid rate limiting.\n",
        "\n",
        "        Args:\n",
        "            query (str): Search query\n",
        "\n",
        "        Returns:\n",
        "            str: Search results\n",
        "        \"\"\"\n",
        "        time.sleep(2)  # 2 second delay\n",
        "        try:\n",
        "            return DuckDuckGoSearchRun().run(query)\n",
        "        except Exception as e:\n",
        "            print(f\"Search error: {str(e)}\")\n",
        "            return f\"Error performing search: {str(e)}\"\n",
        "\n",
        "    def _wiki_with_delay(self, query: str) -> str:\n",
        "        \"\"\"\n",
        "        Perform a Wikipedia search with a built-in delay.\n",
        "\n",
        "        Args:\n",
        "            query (str): Wikipedia search query\n",
        "\n",
        "        Returns:\n",
        "            str: Wikipedia results\n",
        "        \"\"\"\n",
        "        time.sleep(2)  # 2 second delay\n",
        "        try:\n",
        "            return WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper()).run(query)\n",
        "        except Exception as e:\n",
        "            print(f\"Wikipedia error: {str(e)}\")\n",
        "            return f\"Error querying Wikipedia: {str(e)}\"\n",
        "\n",
        "    def _initialize_tools(self) -> List[Tool]:\n",
        "        \"\"\"\n",
        "        Initialize the search and Wikipedia tools.\n",
        "\n",
        "        Returns:\n",
        "            List[Tool]: List of initialized tools\n",
        "        \"\"\"\n",
        "        return [\n",
        "            Tool(\n",
        "                name=\"Search\",\n",
        "                func=self._search_with_delay,\n",
        "                description=\"useful for searching current events and recent information\"\n",
        "            ),\n",
        "            Tool(\n",
        "                name=\"Wikipedia\",\n",
        "                func=self._wiki_with_delay,\n",
        "                description=\"useful for getting detailed background information from Wikipedia\"\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    def _create_prompt_template(self) -> ChatPromptTemplate:\n",
        "        \"\"\"\n",
        "        Create the prompt template for the agent.\n",
        "\n",
        "        Returns:\n",
        "            ChatPromptTemplate: Configured prompt template\n",
        "        \"\"\"\n",
        "        template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Question: {input}\n",
        "\n",
        "{agent_scratchpad}\"\"\"\n",
        "\n",
        "        return ChatPromptTemplate.from_messages([\n",
        "            (\"system\", template)\n",
        "        ])\n",
        "\n",
        "    def invoke_with_retry(self, query: str, max_retries: int = 3) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Invoke the agent with retry logic.\n",
        "\n",
        "        Args:\n",
        "            query (str): The query to process\n",
        "            max_retries (int, optional): Maximum number of retry attempts. Defaults to 3.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, Any]: Agent response\n",
        "\n",
        "        Raises:\n",
        "            Exception: If all retry attempts fail\n",
        "        \"\"\"\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                time.sleep(2)  # Wait before starting\n",
        "                response = self.agent_executor.invoke({\n",
        "                    \"input\": query,\n",
        "                    \"agent_scratchpad\": []\n",
        "                })\n",
        "                return response\n",
        "            except Exception as e:\n",
        "                print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    wait_time = (attempt + 1) * 5  # Exponential backoff\n",
        "                    print(f\"Waiting {wait_time} seconds before retrying...\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    raise Exception(f\"All {max_retries} attempts failed: {str(e)}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to demonstrate the SearchAgent usage.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize the agent\n",
        "        agent = SearchAgent()\n",
        "\n",
        "        # Example queries\n",
        "        queries = [\n",
        "            \"What's the latest news about SpaceX and compare it with their Wikipedia entry?\",\n",
        "            \"What are the current developments in AI and their historical context?\",\n",
        "            \"Compare the current weather in New York with its typical seasonal patterns.\"\n",
        "        ]\n",
        "\n",
        "        # Process each query\n",
        "        for query in queries:\n",
        "            print(f\"\\nProcessing query: {query}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            try:\n",
        "                response = agent.invoke_with_retry(query)\n",
        "                print(\"\\nResponse:\")\n",
        "                print(response[\"output\"])\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing query: {str(e)}\")\n",
        "\n",
        "            print(\"-\" * 50)\n",
        "            time.sleep(5)  # Wait between queries\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "gR6qjC_mfMQy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}