{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzMeUEbfo2P3"
      },
      "source": [
        "<img src=\"https://github.com/Shubhwithai/gen-ai-experiments/blob/main/Logo.png?raw=true\" alt=\"Company Logo\" width=\"200\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1UZu48rBKKA3b11bX2Y2ix5jMXBjc2zr6?usp=chrome_ntp#scrollTo=bHBBMb2IQbOe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-eUzCfCOxmI"
      },
      "source": [
        "**Build Fast with AI** empowers innovators to bring their AI ideas to life through hands-on workshops and expert mentorship. Embark on your zero-to-one journey in Generative AI with our 6-week bootcamp, designed to help you master LLMs and create custom AI applications.\n",
        "\n",
        "Learn more about Our Bootcamp https://www.buildfastwithai.com/genai-course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eJ5qoOkwhyp"
      },
      "source": [
        "## AI Suite : Simple, unified interface to multiple Generative AI providers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSqzIeGAwv8y"
      },
      "source": [
        "* aisuite makes it easy for developers to use multiple LLM through a standardized interface.\n",
        "\n",
        "* Using an interface similar to OpenAI's, aisuite makes it easy to interact with the most popular LLMs and compare the results.\n",
        "\n",
        "* It is a thin wrapper around python client libraries, and allows creators to seamlessly swap out and test responses from different LLM providers without changing their code. Today, the library is primarily focussed on chat completions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SViS0Nluwfj9"
      },
      "source": [
        "AI Suite is a light wrapper to provide a unified interface between LLM providers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFtf3OR_xTgb"
      },
      "source": [
        "###Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC7QZbEGwGf1",
        "outputId": "6663db8f-003b-4a8d-d233-76462b1bb509"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: aisuite[all] in /usr/local/lib/python3.10/dist-packages (0.1.6)\n",
            "Collecting anthropic<0.31.0,>=0.30.1 (from aisuite[all])\n",
            "  Downloading anthropic-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting groq<0.10.0,>=0.9.0 (from aisuite[all])\n",
            "  Downloading groq-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.35.8 in /usr/local/lib/python3.10/dist-packages (from aisuite[all]) (1.54.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.20.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (4.12.2)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.35.8->aisuite[all]) (4.66.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.23.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.26.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.2.3)\n",
            "Downloading anthropic-0.30.1-py3-none-any.whl (863 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m863.9/863.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.9.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, anthropic\n",
            "Successfully installed anthropic-0.30.1 groq-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# Install AI Suite\n",
        "!pip install aisuite[all]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TK-LvHQxqq_"
      },
      "source": [
        "###Setting Up API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzWMdVBExreb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['ANTHROPIC_API_KEY'] = userdata.get('ANTHROPIC_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KtvquRyy7r7"
      },
      "source": [
        "###Custom Pretty Printing Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0JzbGvOy8cX"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint as pp\n",
        "# Set a custom width for pretty-printing\n",
        "def pprint(data, width=80):\n",
        "    \"\"\"Pretty print data with a specified width.\"\"\"\n",
        "    pp(data, width=width)# List of model identifiers to query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvrpvRqlx53p"
      },
      "source": [
        "###Creating a Simple Chat Interaction with an AI Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkyFzEEHx641",
        "outputId": "6829df57-4471-4169-aea9-55a161f90a6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "import aisuite as ai\n",
        "\n",
        "# Initialize the AI client for accessing the language model\n",
        "client = ai.Client()\n",
        "\n",
        "# Define a conversation with a system message and a user message\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful agent, who answers with brevity.\"},\n",
        "    {\"role\": \"user\", \"content\": 'Hi'},\n",
        "]\n",
        "\n",
        "# Request a response from the model\n",
        "response = client.chat.completions.create(model=\"groq:llama-3.2-3b-preview\", messages=messages)\n",
        "\n",
        "# Print the model's response\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGLGMUyGyG5P"
      },
      "source": [
        "###Defining a Function to Interact with the Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYdfqz4RyH52"
      },
      "outputs": [],
      "source": [
        "def ask(message, sys_message=\"You are a helpful agent.\",\n",
        "         model=\"groq:llama-3.2-3b-preview\"):\n",
        "    # Initialize the AI client for accessing the language model\n",
        "    client = ai.Client()\n",
        "\n",
        "    # Construct the messages list for the chat\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": sys_message},\n",
        "        {\"role\": \"user\", \"content\": message}\n",
        "    ]\n",
        "\n",
        "    # Send the messages to the model and get the response\n",
        "    response = client.chat.completions.create(model=model, messages=messages)\n",
        "\n",
        "    # Return the content of the model's response\n",
        "    return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KAcwSbDTyNJv",
        "outputId": "c3c5d982-06f6-42c6-9bd2-88aebad76f27"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The capital of India is New Delhi.'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ask(\"Hi. what is capital of india?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZuI8VAoyhtI"
      },
      "source": [
        "###Confirm each model is using a different provider"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBKtYRDIylO9",
        "outputId": "77c6ed3c-25d2-4873-dbf4-66beb27032c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I was created by Meta AI, a company that specializes in natural language processing and artificial intelligence. My knowledge was built from a massive dataset of text and is continuously updated to improve my responses.\n",
            "I was created by Anthropic.\n",
            "I was created by OpenAI, an artificial intelligence research organization.\n"
          ]
        }
      ],
      "source": [
        "print(ask(\"Who is your creator?\"))\n",
        "print(ask('Who is your creator?', model='anthropic:claude-3-5-sonnet-20240620'))\n",
        "print(ask('Who is your creator?', model='openai:gpt-4o'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cVCg2shyvOe"
      },
      "source": [
        "###Querying Multiple AI Models for a Common Question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGIcEaK9ywUv",
        "outputId": "326ee32b-f71c-4f66-dbb7-5c58584de1bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('llama-3.1-8b-instant: \\n'\n",
            " ' The origins of Artificial Intelligence (AI) can be traced back to 1950 when '\n",
            " 'British mathematician Alan Turing proposed the Turing Test to measure a '\n",
            " \"machine's ability to exhibit intelligent behavior equivalent to, or \"\n",
            " 'indistinguishable from, that of a human. ')\n",
            "('llama-3.2-1b-preview: \\n'\n",
            " ' The origins of artificial intelligence (AI) date back to the 1950s and '\n",
            " '1960s when mathematician and computer scientist John McCarthy coined the '\n",
            " 'term \"artificial intelligence\" at a conference at Dartmouth College, marking '\n",
            " \"the beginning of the field's growth. \")\n",
            "('llama-3.2-3b-preview: \\n'\n",
            " ' The origins of Artificial Intelligence (AI) date back to the 1950s, when '\n",
            " 'computer scientists such as Alan Turing, Marvin Minsky, and John McCarthy '\n",
            " 'began exploring the theoretical foundations and practical applications of '\n",
            " 'machine intelligence, through pioneering work in algorithms, machine '\n",
            " 'learning, and rule-based systems. ')\n",
            "('llama3-70b-8192: \\n'\n",
            " ' The origins of Artificial Intelligence (AI) can be traced back to the 1950s '\n",
            " 'when computer scientist John McCarthy coined the term \"Artificial '\n",
            " 'Intelligence\" and began exploring ways to create machines that could '\n",
            " 'simulate human intelligence, building upon the work of pioneers like Alan '\n",
            " 'Turing, Marvin Minsky, and Nathaniel Rochester. ')\n",
            "('llama3-8b-8192: \\n'\n",
            " ' The origins of Artificial Intelligence (AI) can be traced back to the '\n",
            " '1950s, when computer scientists like Alan Turing, Marvin Minsky, and John '\n",
            " 'McCarthy began exploring ways to create machines that could simulate '\n",
            " 'intelligent behavior, laying the foundation for the development of modern '\n",
            " 'AI. ')\n"
          ]
        }
      ],
      "source": [
        "models = [\n",
        "    'llama-3.1-8b-instant',\n",
        "    'llama-3.2-1b-preview',\n",
        "    'llama-3.2-3b-preview',\n",
        "    'llama3-70b-8192',\n",
        "    'llama3-8b-8192'\n",
        "]\n",
        "\n",
        "# Initialize a list to hold the responses from each model\n",
        "ret = []\n",
        "\n",
        "# Loop through each model and get a response for the specified question\n",
        "for x in models:\n",
        "    ret.append(ask('Write a short one sentence explanation of the origins of AI?', model=f'groq:{x}'))\n",
        "\n",
        "# Print the model's name and its corresponding response\n",
        "for idx, x in enumerate(ret):\n",
        "    pprint(models[idx] + ': \\n ' + x + ' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXSSPLQ7zGp_"
      },
      "source": [
        "###Querying Different AI Providers for a Common Question\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06mVNt-dzHqA",
        "outputId": "462f5714-1125-45cd-80ed-122d7198915d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('groq:llama3-70b-8192: \\n'\n",
            " 'The origins of Artificial Intelligence (AI) can be traced back to the 1950s '\n",
            " 'when computer scientist Alan Turing proposed the Turing Test, a measure of a '\n",
            " \"machine's ability to exhibit intelligent behavior equivalent to, or \"\n",
            " 'indistinguishable from, that of a human. \\n'\n",
            " '\\n')\n",
            "('openai:gpt-4o: \\n'\n",
            " 'Artificial Intelligence originated from the mid-20th century when '\n",
            " 'researchers sought to create machines capable of performing tasks that '\n",
            " 'typically required human intelligence, combining advances in computer '\n",
            " 'science, mathematics, and cognitive psychology. \\n'\n",
            " '\\n')\n",
            "('anthropic:claude-3-5-sonnet-20240620: \\n'\n",
            " 'The origins of AI can be traced back to the 1950s when researchers began '\n",
            " 'exploring the possibility of creating machines that could mimic human '\n",
            " 'intelligence and problem-solving capabilities. \\n'\n",
            " '\\n')\n"
          ]
        }
      ],
      "source": [
        "# List of AI model providers to query\n",
        "providers = [\n",
        "    'groq:llama3-70b-8192',\n",
        "    'openai:gpt-4o',\n",
        "    'anthropic:claude-3-5-sonnet-20240620'\n",
        "]\n",
        "\n",
        "# Initialize a list to hold the responses from each provider\n",
        "ret = []\n",
        "\n",
        "# Loop through each provider and get a response for the specified question\n",
        "for x in providers:\n",
        "    ret.append(ask('Write a short one sentence explanation of the origins of AI?', model=x))\n",
        "\n",
        "# Print the provider's name and its corresponding response\n",
        "for idx, x in enumerate(ret):\n",
        "    pprint(providers[idx] + ': \\n' + x + ' \\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRjtUNYnzSVo"
      },
      "source": [
        "###Generating and Evaluating Questions with AI Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "ur64oxPmzTk_",
        "outputId": "2a0cbad7-32dd-4160-fa9d-ac5aca3a7976"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('Original text:\\n'\n",
            " \"  Here's a short question suitable for asking an LLM:\\n\"\n",
            " '\\n'\n",
            " 'What are the main differences between renewable and non-renewable energy '\n",
            " 'sources?\\n'\n",
            " '\\n')\n",
            "('Option 1 text:\\n'\n",
            " '  Renewable energy sources are naturally replenished and sustainable over '\n",
            " 'time, such as solar, wind, and hydroelectric power. Non-renewable energy '\n",
            " 'sources, like coal, oil, and natural gas, are finite and deplete with use, '\n",
            " 'leading to environmental concerns.\\n'\n",
            " '\\n')\n",
            "('Option 2 text:\\n'\n",
            " '  The main differences between renewable and non-renewable energy sources '\n",
            " 'are:\\n'\n",
            " '\\n'\n",
            " 'Renewable Energy Sources:\\n'\n",
            " '\\n'\n",
            " '* Sustainably available (can be replenished naturally)\\n'\n",
            " '* Produces little to no pollution or greenhouse gas emissions\\n'\n",
            " '* Examples: solar, wind, hydro, geothermal, and biomass energy\\n'\n",
            " '\\n'\n",
            " 'Non-Renewable Energy Sources:\\n'\n",
            " '\\n'\n",
            " '* Limited availability (will eventually run out)\\n'\n",
            " '* Contributes to pollution and greenhouse gas emissions\\n'\n",
            " '* Examples: fossil fuels (coal, oil, and natural gas)\\n'\n",
            " '\\n'\n",
            " 'In short, renewable energy sources are eco-friendly and sustainable, while '\n",
            " 'non-renewable energy sources are finite and harm the environment.\\n'\n",
            " '\\n')\n",
            "Which is best 1 or 2. 3 if indistinguishable: 2\n",
            "('Original text:\\n'\n",
            " '  What are the ethical implications of using AI in decision-making '\n",
            " 'processes?\\n'\n",
            " '\\n')\n",
            "('Option 1 text:\\n'\n",
            " '  The ethical implications of using AI in decision-making processes '\n",
            " 'include:\\n'\n",
            " '\\n'\n",
            " '* Bias and discrimination: AI systems can perpetuate and amplify biases '\n",
            " 'present in the data used to train them, leading to unfair outcomes.\\n'\n",
            " '* Lack of transparency and accountability: It can be difficult to understand '\n",
            " 'how AI systems arrive at their decisions, making it challenging to identify '\n",
            " 'mistakes or intentional biases.\\n'\n",
            " '* Job displacement and change in workforce dynamics: AI automation may lead '\n",
            " 'to job losses and alter the nature of work.\\n'\n",
            " '* Privacy concerns: AI systems can collect and process large amounts of '\n",
            " 'personal data, raising concerns about data privacy and potential misuse.\\n'\n",
            " \"* Autonomy and accountability: As AI systems make more decisions, it's \"\n",
            " 'unclear who is responsible for the outcomes, and how to hold them '\n",
            " 'accountable.\\n'\n",
            " '\\n'\n",
            " 'These implications highlight the need for careful consideration and '\n",
            " 'regulation of AI use in decision-making processes to ensure fairness, '\n",
            " 'transparency, and accountability.\\n'\n",
            " '\\n')\n",
            "('Option 2 text:\\n'\n",
            " '  The key ethical implications of using AI in decision-making include:\\n'\n",
            " '\\n'\n",
            " '1. Potential bias and discrimination in AI algorithms\\n'\n",
            " '2. Lack of transparency and explainability in AI decisions\\n'\n",
            " '3. Privacy concerns and data protection issues\\n'\n",
            " '4. Accountability and responsibility for AI-made decisions\\n'\n",
            " '5. Job displacement and economic impacts\\n'\n",
            " '6. Fairness and equal access to AI technologies\\n'\n",
            " '7. Potential for AI to make high-stakes decisions affecting human lives\\n'\n",
            " '\\n'\n",
            " 'These issues raise important questions about fairness, accountability, and '\n",
            " 'the role of human oversight in AI-driven decision-making processes.\\n'\n",
            " '\\n')\n",
            "Which is best 1 or 2. 3 if indistinguishable: exit\n",
            "('Original text:\\n'\n",
            " '  Here is a short question suitable for asking a Large Language Model '\n",
            " '(LLM):\\n'\n",
            " '\\n'\n",
            " '\"What are some potential benefits and drawbacks of using artificial '\n",
            " 'intelligence in healthcare, and how can its implementation be made more '\n",
            " 'effective?\"\\n'\n",
            " '\\n'\n",
            " 'This question is open-ended, allowing the LLM to provide a detailed and '\n",
            " 'informative response, and it touches on a relevant and nuanced topic that '\n",
            " 'requires consideration of multiple factors.\\n'\n",
            " '\\n')\n",
            "('Option 1 text:\\n'\n",
            " '  Some potential benefits of using artificial intelligence (AI) in '\n",
            " 'healthcare include improved diagnostic accuracy, personalized treatment '\n",
            " 'plans, operational efficiency, and better resource management. However, '\n",
            " 'drawbacks may include concerns around data privacy, algorithmic bias, high '\n",
            " 'implementation costs, and potential disruptions to patient-provider '\n",
            " 'relationships. To make its implementation more effective, it is essential to '\n",
            " 'establish robust data governance, ensure transparency and fairness in AI '\n",
            " 'systems, provide adequate training for healthcare professionals, and involve '\n",
            " 'stakeholders in the development and deployment processes.\\n'\n",
            " '\\n')\n",
            "('Option 2 text:\\n'\n",
            " '  I will not provide an answer to that healthcare AI question, as that '\n",
            " 'appears to be an example prompt rather than a question you actually want me '\n",
            " 'to answer. The text you shared seems to be describing the qualities of a '\n",
            " 'good question to ask an AI system, rather than posing a direct question. Let '\n",
            " \"me know if you have an actual question you'd like me to attempt to answer.\\n\"\n",
            " '\\n')\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d17783dc1f7c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Store the provider names and the user's choice of the best answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mbest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproviders\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Which is best 1 or 2. 3 if indistinguishable: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Initialize a list to store the best responses\n",
        "best = []\n",
        "\n",
        "# Loop to generate and evaluate questions\n",
        "for _ in range(20):\n",
        "    # Shuffle the providers list to randomly select models for each iteration\n",
        "    random.shuffle(providers)\n",
        "\n",
        "    # Generate a question using the first provider\n",
        "    question = ask('Please generate a short question that is suitable for asking an LLM.', model=providers[0])\n",
        "\n",
        "    # Get answers from the second and third providers\n",
        "    answer_1 = ask('Please give a short answer to this question: ' + question, model=providers[1])\n",
        "    answer_2 = ask('Please give a short answer to this question: ' + question, model=providers[2])\n",
        "\n",
        "    # Print the generated question and the two answers\n",
        "    pprint(f\"Original text:\\n  {question}\\n\\n\")\n",
        "    pprint(f\"Option 1 text:\\n  {answer_1}\\n\\n\")\n",
        "    pprint(f\"Option 2 text:\\n  {answer_2}\\n\\n\")\n",
        "\n",
        "    # Store the provider names and the user's choice of the best answer\n",
        "    best.append(str(providers) + ', ' + input(\"Which is best 1 or 2. 3 if indistinguishable: \"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
