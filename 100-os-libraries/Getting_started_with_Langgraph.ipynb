{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Build Fast with AI](https://img.shields.io/badge/BuildFastWithAI-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://www.buildfastwithai.com/genai-course)\n",
        "[![EduChain GitHub](https://img.shields.io/github/stars/satvik314/educhain?style=for-the-badge&logo=github&color=gold)](https://github.com/satvik314/educhain)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1hc_tb7sMdDOa7fTMAe2Fu1wslv1wf9E8#scrollTo=ahguSJ5vp3e7)\n",
        "## Master Generative AI in 6 Weeks\n",
        "**What You'll Learn:**\n",
        "- Build with Latest LLMs\n",
        "- Create Custom AI Apps\n",
        "- Learn from Industry Experts\n",
        "- Join Innovation Community\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "* Empowering the Next Generation of AI Innovators"
      ],
      "metadata": {
        "id": "6LkijOdEpgQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Getting Started With LangGraoh"
      ],
      "metadata": {
        "id": "LhX0AuzY_LBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Introduction ðŸ”–**\n",
        "\n",
        "LangGraph is a library built on top of LangChain that helps create stateful, multi-step AI workflows. Think of it as a way to create flowcharts for AI tasks, where each step can depend on previous results and make decisions about what to do next.\n",
        "\n",
        "### Basic Concepts\n",
        "1. Nodes: Individual steps in your workflow\n",
        "2. Edges: Connections between steps\n",
        "3. State: Information that persists between steps"
      ],
      "metadata": {
        "id": "ahguSJ5vp3e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "To get started, we need to install the last version of langgraph"
      ],
      "metadata": {
        "id": "ONDB3BJiqK6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_openai langchain-community langgraph --quiet"
      ],
      "metadata": {
        "id": "y5B5NidzqGE8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Features :-\n",
        "\n",
        "- Create multi-step AI workflows\n",
        "- Maintain state between steps\n",
        "- Conditional branching\n",
        "- Easy integration with LLMs"
      ],
      "metadata": {
        "id": "WzYtGq8VsNYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Greeting Workflow"
      ],
      "metadata": {
        "id": "UFO9XYyOqsjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This workflow takes a person's name and generates a greeting message."
      ],
      "metadata": {
        "id": "RPo7qpA8rSOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing import TypedDict\n",
        "\n",
        "# Define our state\n",
        "class GreetingState(TypedDict):\n",
        "    name: str\n",
        "    greeting: str\n",
        "\n",
        "# Create nodes\n",
        "def create_greeting(state):\n",
        "    # Create a greeting message using the name\n",
        "    state['greeting'] = f\"Hello, {state['name']}!\"\n",
        "    return state"
      ],
      "metadata": {
        "id": "Y-Bee4rSshO3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create workflow\n",
        "greeting_workflow = StateGraph(GreetingState)\n",
        "\n",
        "# Add nodes\n",
        "greeting_workflow.add_node(\"greet\", create_greeting)\n",
        "\n",
        "# Add edges\n",
        "greeting_workflow.add_edge(START, \"greet\")  # Start the workflow\n",
        "greeting_workflow.add_edge(\"greet\", END)    # End the workflow\n",
        "\n",
        "# Compile\n",
        "chain = greeting_workflow.compile()\n"
      ],
      "metadata": {
        "id": "Fla-y8BmsjF6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Run the Chain"
      ],
      "metadata": {
        "id": "Ec1_63E2AMdX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkGn4ucCpdMI",
        "outputId": "2f233858-927b-4060-a8c6-ee75132b1328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'Alice', 'greeting': 'Hello, Alice!'}\n"
          ]
        }
      ],
      "source": [
        "# Run\n",
        "result = chain.invoke({\n",
        "    \"name\": \"Alice\",\n",
        "    \"greeting\": \"\"\n",
        "})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setting Up API Keys"
      ],
      "metadata": {
        "id": "kJkuiCjGrYTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "kogny3fGraFS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create a Node"
      ],
      "metadata": {
        "id": "H3IoT1nd8Zt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing import TypedDict\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "\n",
        "# Define our state\n",
        "class OpenAIState(TypedDict):\n",
        "    prompt: str\n",
        "    response: str\n",
        "\n",
        "# Create nodes\n",
        "def generate_response(state):\n",
        "    # Use OpenAI to generate a response\n",
        "    llm = ChatOpenAI(temperature=0)\n",
        "    response = llm.invoke(state['prompt'])\n",
        "    state['response'] = response.content\n",
        "    return state"
      ],
      "metadata": {
        "id": "VVclCX7VsprS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create workflow\n",
        "openai_workflow = StateGraph(OpenAIState)\n",
        "\n",
        "# Add nodes\n",
        "openai_workflow.add_node(\"generate\", generate_response)\n",
        "\n",
        "# Add edges\n",
        "openai_workflow.add_edge(START, \"generate\")  # Start the workflow\n",
        "openai_workflow.add_edge(\"generate\", END)    # End the workflow\n",
        "\n",
        "# Compile\n",
        "chain = openai_workflow.compile()"
      ],
      "metadata": {
        "id": "tRwaCqDRsrNc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Run the Chain"
      ],
      "metadata": {
        "id": "12l9ckvH8nuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "result = chain.invoke({\n",
        "    \"prompt\": \"What is the capital of France?\",\n",
        "    \"response\": \"\"\n",
        "})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyqJhhTmqVru",
        "outputId": "ec62f43d-bbc9-4516-c14f-add36c9498a6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': 'What is the capital of France?', 'response': 'The capital of France is Paris.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.invoke({\n",
        "    \"prompt\": \"Highest mountain in India\",\n",
        "    \"response\": \"\"\n",
        "})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdNJa36xrs5z",
        "outputId": "6e82e1f0-dfab-403a-d439-a9ccc6e1d3b0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': 'Highest mountain in India', 'response': 'The highest mountain in India is Kangchenjunga, which stands at an elevation of 8,586 meters (28,169 feet) above sea level. It is located on the border between Nepal and the Indian state of Sikkim in the eastern Himalayas.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common Use Cases ðŸŽ¢\n",
        "- Customer Support Automation\n",
        "- Content Generation\n",
        "- Research Assistance\n",
        "- Data Analysis Pipelines\n",
        "- Decision Support Systems"
      ],
      "metadata": {
        "id": "9GYBUycksUc7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A real life scenario and usecase\n",
        "\n",
        "This code creates an interactive customer support system using LangGraph and OpenAI. It simulates how a support agent might handle a customer's issue by categorizing the problem, determining its priority, and providing a solution.\n",
        "\n"
      ],
      "metadata": {
        "id": "jAQYcpDrtSYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing import TypedDict, List\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "# Set your OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Define our state\n",
        "class TicketState(TypedDict):\n",
        "    customer_name: str\n",
        "    issue: str\n",
        "    category: str\n",
        "    priority: str\n",
        "    solution: str\n",
        "    satisfaction: str\n",
        "    chat_history: List[str]\n"
      ],
      "metadata": {
        "id": "GW6k3tz6t1aV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper node functions"
      ],
      "metadata": {
        "id": "T879Jp-At2Vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Node functions\n",
        "def categorize_issue(state):\n",
        "    llm = ChatOpenAI(temperature=0)\n",
        "    prompt = f\"\"\"\n",
        "    Categorize this customer issue into one of:\n",
        "    - Technical Support\n",
        "    - Billing Query\n",
        "    - Product Information\n",
        "    - General Inquiry\n",
        "\n",
        "    Issue: {state['issue']}\n",
        "\n",
        "    Respond with just the category name.\n",
        "    \"\"\"\n",
        "    state['category'] = llm.invoke(prompt).content.strip()\n",
        "    state['chat_history'].append(f\"System: Categorized as {state['category']}\")\n",
        "    return state\n",
        "\n",
        "def determine_priority(state):\n",
        "    llm = ChatOpenAI(temperature=0)\n",
        "    prompt = f\"\"\"\n",
        "    Based on this customer issue, determine the priority (High/Medium/Low):\n",
        "    Customer: {state['customer_name']}\n",
        "    Issue: {state['issue']}\n",
        "    Category: {state['category']}\n",
        "\n",
        "    Consider:\n",
        "    - Impact on customer\n",
        "    - Urgency of the matter\n",
        "    - Category of issue\n",
        "\n",
        "    Respond with just: High, Medium, or Low\n",
        "    \"\"\"\n",
        "    state['priority'] = llm.invoke(prompt).content.strip()\n",
        "    state['chat_history'].append(f\"System: Priority set to {state['priority']}\")\n",
        "    return state\n",
        "\n",
        "def generate_solution(state):\n",
        "    llm = ChatOpenAI(temperature=0.7)\n",
        "    prompt = f\"\"\"\n",
        "    Generate a helpful and professional solution for this customer:\n",
        "\n",
        "    Customer: {state['customer_name']}\n",
        "    Category: {state['category']}\n",
        "    Priority: {state['priority']}\n",
        "    Issue: {state['issue']}\n",
        "\n",
        "    Provide a clear, step-by-step solution.\n",
        "    \"\"\"\n",
        "    state['solution'] = llm.invoke(prompt).content.strip()\n",
        "    state['chat_history'].append(f\"Agent: {state['solution']}\")\n",
        "    return state\n",
        "\n",
        "def get_satisfaction(state):\n",
        "    while True:\n",
        "        satisfaction = input(\"\\nAre you satisfied with the solution? (yes/no): \").lower()\n",
        "        if satisfaction in ['yes', 'no']:\n",
        "            state['satisfaction'] = satisfaction\n",
        "            state['chat_history'].append(f\"Customer: Satisfaction - {satisfaction}\")\n",
        "            break\n",
        "        print(\"Please enter 'yes' or 'no'\")\n",
        "    return state\n",
        "\n",
        "def handle_dissatisfaction(state):\n",
        "    if state['satisfaction'] == 'no':\n",
        "        llm = ChatOpenAI(temperature=0.7)\n",
        "        prompt = f\"\"\"\n",
        "        The customer is not satisfied with the previous solution.\n",
        "        Generate an alternative solution:\n",
        "\n",
        "        Original Issue: {state['issue']}\n",
        "        Previous Solution: {state['solution']}\n",
        "\n",
        "        Provide a different approach to solve the issue.\n",
        "        \"\"\"\n",
        "        new_solution = llm.invoke(prompt).content.strip()\n",
        "        state['solution'] = new_solution\n",
        "        state['chat_history'].append(f\"Agent: I apologize for the previous solution. Here's an alternative approach:\\n{new_solution}\")\n",
        "    return state"
      ],
      "metadata": {
        "id": "8jdc3G7Ht96W"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Workflow creation\n",
        "Creating a workflow for Customer Support Tickting System"
      ],
      "metadata": {
        "id": "C9D_IZE7t-5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the workflow\n",
        "def create_support_workflow():\n",
        "    workflow = StateGraph(TicketState)\n",
        "\n",
        "    # Add nodes\n",
        "    workflow.add_node(\"categorize\", categorize_issue)\n",
        "    workflow.add_node(\"prioritize\", determine_priority)\n",
        "    workflow.add_node(\"solve\", generate_solution)\n",
        "    workflow.add_node(\"check_satisfaction\", get_satisfaction)\n",
        "    workflow.add_node(\"handle_dissatisfaction\", handle_dissatisfaction)\n",
        "\n",
        "    # Add edges\n",
        "    workflow.add_edge(START, \"categorize\")\n",
        "    workflow.add_edge(\"categorize\", \"prioritize\")\n",
        "    workflow.add_edge(\"prioritize\", \"solve\")\n",
        "    workflow.add_edge(\"solve\", \"check_satisfaction\")\n",
        "    workflow.add_edge(\"check_satisfaction\", \"handle_dissatisfaction\")\n",
        "    workflow.add_edge(\"handle_dissatisfaction\", END)\n",
        "\n",
        "    return workflow.compile()\n",
        "\n",
        "def display_ticket_status(state):\n",
        "    \"\"\"Display the current status of the ticket\"\"\"\n",
        "    clear_output(wait=True)\n",
        "    print(\"\\n=== Customer Support Ticket Status ===\")\n",
        "    print(f\"Customer Name: {state['customer_name']}\")\n",
        "    print(f\"Category: {state['category']}\")\n",
        "    print(f\"Priority: {state['priority']}\")\n",
        "    print(\"\\n=== Chat History ===\")\n",
        "    for message in state['chat_history']:\n",
        "        print(message)\n",
        "        time.sleep(1)  # Add a small delay for better readability\n",
        "\n",
        "def run_support_system():\n",
        "    \"\"\"Main function to run the support system\"\"\"\n",
        "    print(\"Welcome to the Customer Support System!\")\n",
        "\n",
        "    # Get customer information\n",
        "    customer_name = input(\"\\nPlease enter your name: \")\n",
        "    issue = input(\"Please describe your issue: \")\n",
        "\n",
        "    # Initialize the support workflow\n",
        "    chain = create_support_workflow()\n",
        "\n",
        "    # Initial state\n",
        "    initial_state = {\n",
        "        \"customer_name\": customer_name,\n",
        "        \"issue\": issue,\n",
        "        \"category\": \"\",\n",
        "        \"priority\": \"\",\n",
        "        \"solution\": \"\",\n",
        "        \"satisfaction\": \"\",\n",
        "        \"chat_history\": [f\"Customer: {issue}\"]\n",
        "    }\n",
        "\n",
        "    # Run the workflow\n",
        "    print(\"\\nProcessing your ticket...\")\n",
        "    result = chain.invoke(initial_state)\n",
        "\n",
        "    # Display final status\n",
        "    display_ticket_status(result)\n",
        "\n",
        "    # Ask if they want to create another ticket\n",
        "    if input(\"\\nWould you like to create another ticket? (yes/no): \").lower() == 'yes':\n",
        "        run_support_system()\n",
        "    else:\n",
        "        print(\"Thank you for using our support system!\")\n"
      ],
      "metadata": {
        "id": "9bk_NhbJuEmT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the system"
      ],
      "metadata": {
        "id": "YxKV5umFuGHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In this Example We are Using Custumer Support Ticket System\n"
      ],
      "metadata": {
        "id": "m3YDMPaq9Nyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the system\n",
        "if __name__ == \"__main__\":\n",
        "    run_support_system()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w475PAHQr5-v",
        "outputId": "b7e0f146-5be1-466d-c4a2-d22f1538a354"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Customer Support Ticket Status ===\n",
            "Customer Name: shubham\n",
            "Category: Technical Support\n",
            "Priority: High\n",
            "\n",
            "=== Chat History ===\n",
            "Customer: getting error while contacting\n",
            "System: Categorized as Technical Support\n",
            "System: Priority set to High\n",
            "Agent: Step 1: Check your internet connection to ensure it is stable and working properly.\n",
            "\n",
            "Step 2: Make sure you have entered the correct contact information for the person you are trying to reach.\n",
            "\n",
            "Step 3: Try contacting the person using a different method, such as email or a different messaging platform, to see if the issue is specific to one method of communication.\n",
            "\n",
            "Step 4: If you are still experiencing issues, try restarting your device and/or clearing the cache of the app you are using to contact the person.\n",
            "\n",
            "Step 5: If the issue persists, reach out to the customer support team of the app or platform you are using for further assistance. Provide them with details of the error message you are receiving for a quicker resolution.\n",
            "\n",
            "Step 6: If all else fails, consider reaching out to the person through a different device or asking them to contact you instead.\n",
            "Customer: Satisfaction - no\n",
            "Agent: I apologize for the previous solution. Here's an alternative approach:\n",
            "Alternative Solution:\n",
            "Step 1: Check if the person you are trying to contact has blocked you on the messaging platform or app. If they have, try reaching out to them through a different method or ask a mutual contact to help facilitate communication.\n",
            "\n",
            "Step 2: If you are still experiencing issues, consider reaching out to the person through a different platform or social media channel. They may be more responsive on a different platform.\n",
            "\n",
            "Step 3: If the issue persists, try contacting the customer support team of the messaging platform or app you are using. They may be able to provide further assistance in resolving the communication error.\n",
            "\n",
            "Step 4: If all else fails, consider scheduling a face-to-face or phone call meeting with the person to discuss the communication issue in person. This direct approach may help clarify any misunderstandings and resolve the issue effectively.\n",
            "\n",
            "Would you like to create another ticket? (yes/no): no\n",
            "Thank you for using our support system!\n"
          ]
        }
      ]
    }
  ]
}
