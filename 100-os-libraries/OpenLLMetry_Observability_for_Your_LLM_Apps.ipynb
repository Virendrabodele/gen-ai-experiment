{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Build Fast with AI](https://img.shields.io/badge/BuildFastWithAI-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://www.buildfastwithai.com/genai-course)\n",
        "[![EduChain GitHub](https://img.shields.io/github/stars/satvik314/educhain?style=for-the-badge&logo=github&color=gold)](https://github.com/satvik314/educhain)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/17Ue1b-rsj0Y-34xNWLKIQ4c-B12XzfyM?usp=sharing)\n",
        "## Master Generative AI in 8 Weeks\n",
        "**What You'll Learn:**\n",
        "- Build with Latest LLMs\n",
        "- Create Custom AI Apps\n",
        "- Learn from Industry Experts\n",
        "- Join Innovation Community\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "*Empowering the Next Generation of AI Innovators"
      ],
      "metadata": {
        "id": "1Jcj0zWxkYv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ OpenLLMetry: Observability for Your LLM Apps üîç\n",
        "\n",
        "OpenLLMetry (Open Large Language Model Telemetry) brings robust observability to applications powered by Large Language Models (LLMs). It helps you understand, debug, and improve the performance of these complex systems. üöÄ\n",
        "\n",
        "**‚ú® Key Benefits:**\n",
        "\n",
        "*   **üó∫Ô∏è Comprehensive Tracing:** See the flow of data through your LLM app, from user input ‚û°Ô∏è LLM processing ‚û°Ô∏è final output. Find bottlenecks easily!\n",
        "*   **üìù Prompt Observability:** Captures prompt templates, variables, and versions. Analyze prompt impact and potential vulnerabilities.\n",
        "*   **üëç User Feedback Tracking:** Collect feedback (thumbs up/down, comments) on LLM interactions. Track response quality and improve!\n",
        "*   **‚öôÔ∏è Agent & Tool Monitoring:** Insights into agent behavior, tool usage, and system effectiveness for autonomous systems.\n",
        "*   **üîó Framework Integration:** Seamlessly works with Langchain, Haystack, LlamaIndex.\n",
        "*   **üè∑Ô∏è Decorators for Customization:** Use `@workflow`, `@task`, `@agent`, `@tool` for fine-grained control.\n",
        "*   **üêõ Debugging & Optimization:** Debug efficiently, optimize prompts, and boost user experience."
      ],
      "metadata": {
        "id": "GPFkKoMMnLYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üì¶ Dependency Installation**  \n",
        "\n"
      ],
      "metadata": {
        "id": "NAgLlOiKnLeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install traceloop-sdk langchain_community langchain_openai"
      ],
      "metadata": {
        "id": "1I2ONUsruCnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üîë Setup API Keys**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hJr14g_aq9Be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['TRACELOOP_API_KEY'] = userdata.get('TRACELOOP_API_KEY')"
      ],
      "metadata": {
        "id": "dLu9FgnpmfXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üì• Import Required Libraries**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YnQAKewiq96n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from traceloop.sdk import Traceloop\n",
        "from traceloop.sdk.decorators import workflow"
      ],
      "metadata": {
        "id": "HMsu5q_Zp_S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üöÄ Initialize Traceloop**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gup9r9T-q-hP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Traceloop.init(app_name=\"joke_generation_service\")"
      ],
      "metadata": {
        "id": "gRG3MTWS5bl5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fe6a0a7-9bd3-46fa-ce3f-bacee460d2ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mTraceloop exporting traces to https://api.traceloop.com authenticating with bearer token\n",
            "\u001b[39m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<traceloop.sdk.client.client.Client at 0x7eacb3328610>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ü§ñ Define Joke Generation Workflow**"
      ],
      "metadata": {
        "id": "vVcjlOx6q_p3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "@workflow(name=\"joke_creation\")\n",
        "def create_joke():\n",
        "  completion = client.chat.completions.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[{\"role\": \"user\", \"content\": \"Tell me a joke about opentelemetry\"}],\n",
        "  )\n",
        "\n",
        "  return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "__xhlH635dFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_joke()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HEYcN5Fu5TIx",
        "outputId": "19d069b2-bea7-4a6b-d7e9-fb85224bb032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Why did the Opentelemetry developer break up with their partner? Because they couldn't handle the trace of commitment!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üè¥‚Äç‚ò†Ô∏è Define Joke Translation Agent**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QfSVtd_pq7Zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from traceloop.sdk.decorators import agent, tool\n",
        "\n",
        "@agent(name=\"joke_translation\")\n",
        "def translate_joke_to_pirate():\n",
        "    joke=history_jokes_tool()\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": f\"Translate the below joke to pirate-like english:\\n\\n{joke}\"}],\n",
        "    )\n",
        "    return completion.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "7Tcs26_f4yJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üìú Define History Jokes Tool**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CJY8YBn4rBOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@tool(name=\"history_jokes\")\n",
        "def history_jokes_tool():\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": f\"get some history jokes\"}],\n",
        "    )\n",
        "\n",
        "    return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "0joxB-zF5Mzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_joke_to_pirate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "OgebcZ_2-k9w",
        "outputId": "a78c41a2-096a-4300-fce7-c4ff9f367a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"1. Why did the plunderer go broke? Because his plundering be in ruins.\\n2. What did the grog say when it got squashed? Nothing, it just let out a little rum.\\n3. Why did the ancient sailors build the pyramids? Because they couldn't afford a shipwright.\\n4. Why did the Roman Empire ultimately sink? Because they were all in denial about their troubles.\\n5. How did the pirates send secret messages? By Pirate Code.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üìä Track LLM Call with Traceloop**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_BdR41VhplJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from traceloop.sdk.tracing.manual import LLMMessage, track_llm_call\n",
        "\n",
        "with track_llm_call(vendor=\"openai\", type=\"chat\") as span:\n",
        "  span.report_request(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "          LLMMessage(role=\"user\", content=\"Tell me a joke about opentelemetry\")\n",
        "      ],\n",
        "  )\n",
        "\n",
        "  res = client.chat.completions.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "          {\"role\": \"user\", \"content\": \"Tell me a joke about opentelemetry\"}\n",
        "      ],\n",
        "  )\n",
        "\n",
        "  span.report_response(res.model, [text.message.content for text in res.choices])"
      ],
      "metadata": {
        "id": "HhUUyWWm-1Uz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f067fb8-dae8-4888-c10a-70b65290c41f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:opentelemetry.sdk.trace:Calling end() on an ended span.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(res.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsGscc6toKxR",
        "outputId": "2c81cda3-e422-4dd5-86b5-17b74f159227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the opentelemetry project break up with the logging library?\n",
            "\n",
            "Because it couldn't handle its baggage!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üìù Manage Prompt Versions**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W8kHfe_Bwv86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tell_joke(subject):\n",
        "    Traceloop.set_prompt(\"Tell me a joke about {subject}\", {\"subject\": subject}, version=1)\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": f\"Tell me a joke about {subject}\"}],\n",
        "    )\n",
        "    return completion.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "4uSI8HgIsLDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Traceloop.init(app_name=\"joke_app\")\n",
        "joke = tell_joke(\"OpenTelemetry\")\n",
        "print(joke)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvpu6LZtwzoJ",
        "outputId": "90815506-8365-4a4e-d527-6f02769ae145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mTraceloop exporting traces to https://api.traceloop.com authenticating with bearer token\n",
            "\u001b[39m\n",
            "Why did OpenTelemetry break up with Prometheus? Because it couldn't handle all the baggage it was bringing to the relationship!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üé® Decorating Classes in Python**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uBKBBJXE2DDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from traceloop.sdk.decorators import agent\n",
        "\n",
        "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "@agent(name=\"base_joke_generator\", method_name=\"generate_joke\")\n",
        "class JokeAgent:\n",
        "    def generate_joke(self):\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": \"Tell me a joke about Traceloop\"}],\n",
        "        )\n",
        "\n",
        "        return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "gsbSRkSnxoQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joke_agent = JokeAgent()\n",
        "joke = joke_agent.generate_joke()\n",
        "print(joke)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gO9VWpmD1vtC",
        "outputId": "0ecc25f1-d61b-4465-a095-cae570844bac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the Traceloop user bring a ladder to the computer store? \n",
            "\n",
            "To help him reach the cloud!\n"
          ]
        }
      ]
    }
  ]
}