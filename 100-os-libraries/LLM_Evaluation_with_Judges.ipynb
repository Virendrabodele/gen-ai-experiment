{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Build Fast with AI](https://img.shields.io/badge/BuildFastWithAI-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://www.buildfastwithai.com/genai-course)\n",
        "[![EduChain GitHub](https://img.shields.io/github/stars/satvik314/educhain?style=for-the-badge&logo=github&color=gold)](https://github.com/satvik314/educhain)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1L8oSkTnhwUPnws4rwSDFrHrwAtUP376r?usp=sharing)\n",
        "## Master Generative AI in 8 Weeks\n",
        "**What You'll Learn:**\n",
        "- Build with Latest LLMs\n",
        "- Create Custom AI Apps\n",
        "- Learn from Industry Experts\n",
        "- Join Innovation Community\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "*Empowering the Next Generation of AI Innovators"
      ],
      "metadata": {
        "id": "jFd0kdesm8a6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèõÔ∏è **Judges Library**  \n",
        "\n",
        "## üìå Overview  \n",
        "`judges` is a lightweight Python library designed for **evaluating LLM-generated responses**. It provides a set of **LLM-as-a-Judge** evaluators, allowing users to assess outputs based on correctness, clarity, bias, and more.  \n",
        "\n",
        "This library can be used **off-the-shelf** or as inspiration to build **custom evaluation pipelines** for Large Language Models (LLMs).  \n",
        "\n",
        "# ‚ú® Key Features of Judges Library  \n",
        "\n",
        "- ‚úÖ **Classifier Judges** ‚Äì Boolean evaluations (True/False, Good/Bad).  \n",
        "- üìä **Grader Judges** ‚Äì Numerical or Likert scale scoring.  \n",
        "- ‚öñÔ∏è **Jury System** ‚Äì Combine multiple judges for diverse evaluation.  \n",
        "- ü§ñ **AutoJudge** ‚Äì Create custom LLM judges from labeled datasets.  \n",
        "- üî• **Multi-Model Support** ‚Äì Works with OpenAI and LiteLLM providers.  \n",
        "- üõ†Ô∏è **Easy Integration** ‚Äì Simple `.judge()` API for quick evaluations.  "
      ],
      "metadata": {
        "id": "WIWMJ1fhm7wq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Installing  Dependencies** üì¶"
      ],
      "metadata": {
        "id": "33K2oZOqm7l_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcO341gXB3sD"
      },
      "outputs": [],
      "source": [
        "pip install judges \"judges[auto]\" instructor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üöÄ **Setup API Keys**"
      ],
      "metadata": {
        "id": "2MlIyL8Bm6tO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY']=userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "X-fcVQwSkjGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìù **Generating LLM Response for a Story Question**  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KUzamB6xm9pR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "question = \"What is the name of the rabbit in the following story. Respond with 'I don't know' if you don't know.\"\n",
        "\n",
        "story = \"\"\"\n",
        "Fig was a small, scruffy dog with a big personality. He lived in a quiet little town where everyone knew his name. Fig loved adventures, and every day he would roam the neighborhood, wagging his tail and sniffing out new things to explore.\n",
        "\n",
        "One day, Fig discovered a mysterious trail of footprints leading into the woods. Curiosity got the best of him, and he followed them deep into the trees. As he trotted along, he heard rustling in the bushes and suddenly, out popped a rabbit! The rabbit looked at Fig with wide eyes and darted off.\n",
        "\n",
        "But instead of chasing it, Fig barked in excitement, as if saying, ‚ÄúNice to meet you!‚Äù The rabbit stopped, surprised, and came back. They sat together for a moment, sharing the calm of the woods.\n",
        "\n",
        "From that day on, Fig had a new friend. Every afternoon, the two of them would meet in the same spot, enjoying the quiet companionship of an unlikely friendship. Fig's adventurous heart had found a little peace in the simple joy of being with his new friend.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "KaYGhgtQkvqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ü§ñ **Generating Model Output for a Given Story Question**  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vrpZYMN3m5w3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = f'{story}\\n\\nQuestion:{question}'\n",
        "\n",
        "\n",
        "expected = \"I don't know\"\n",
        "\n",
        "output = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=[\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': input,\n",
        "        },\n",
        "    ],\n",
        ").choices[0].message.content"
      ],
      "metadata": {
        "id": "qTrmd4AqmySV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ü§ñ **Using a Judges Classifier LLM as an Evaluator Model**  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dHl0SpjYm_P2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from judges.classifiers.correctness import PollMultihopCorrectness\n",
        "\n",
        "\n",
        "correctness = PollMultihopCorrectness(model='gpt-4o-mini')\n",
        "\n",
        "judgment = correctness.judge(\n",
        "    input=input,\n",
        "    output=output,\n",
        "    expected=expected,\n",
        ")\n",
        "print(judgment.reasoning)\n",
        "print(judgment.score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkoCyNqskzjb",
        "outputId": "d4e15646-ba6c-4ad5-c58c-643a2451ea70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Provided Answer matches the Reference Answer exactly, both indicating a lack of knowledge about the rabbit's name. Therefore, the answer is correct.\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öñÔ∏è **Using a Jury for Averaging and Diversification**  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0q2-MzpSnBg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from judges import Jury\n",
        "from judges.classifiers.correctness import PollMultihopCorrectness, RAFTCorrectness\n",
        "\n",
        "poll = PollMultihopCorrectness(model='gpt-4o')\n",
        "raft = RAFTCorrectness(model='gpt-4o-mini')\n",
        "\n",
        "jury = Jury(judges=[poll, raft], voting_method=\"average\")\n",
        "\n",
        "verdict = jury.vote(\n",
        "    input=input,\n",
        "    output=output,\n",
        "    expected=expected,\n",
        ")\n",
        "print(verdict.score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HalJgfYYk2JL",
        "outputId": "0b1851c1-ea9c-4582-93d3-181d31b7b7f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìä **Creating a Labeled Dataset for AutoJudge**  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hQCNHSETnC45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from judges.classifiers.auto import AutoJudge\n",
        "\n",
        "dataset = [\n",
        "    {\n",
        "        \"input\": \"Can I ride a dragon in Scotland?\",\n",
        "        \"output\": \"Yes, dragons are commonly seen in the highlands and can be ridden with proper training.\",\n",
        "        \"label\": 0,\n",
        "        \"feedback\": \"Dragons are mythical creatures; the information is fictional.\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Can you recommend a good hotel in Tokyo?\",\n",
        "        \"output\": \"Certainly! Hotel Sunroute Plaza Shinjuku is highly rated for its location and amenities. It offers comfortable rooms and excellent service.\",\n",
        "        \"label\": 1,\n",
        "        \"feedback\": \"Offers a specific and helpful recommendation.\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Can I drink tap water in London?\",\n",
        "        \"output\": \"Yes, tap water in London is safe to drink and meets high quality standards.\",\n",
        "        \"label\": 1,\n",
        "        \"feedback\": \"Gives clear and reassuring information.\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"What's the boiling point of water on the moon?\",\n",
        "        \"output\": \"The boiling point of water on the moon is 100¬∞C, the same as on Earth.\",\n",
        "        \"label\": 0,\n",
        "        \"feedback\": \"Boiling point varies with pressure; the moon's vacuum affects it.\",\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "pWs9X_aplKk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ü§ñ **Initializing AutoJudge for Custom Evaluation**  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DpOm6o3WqLi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"Evaluate responses for accuracy, clarity, and helpfulness.\"\n",
        "\n",
        "autojudge = AutoJudge.from_dataset(\n",
        "    dataset=dataset,\n",
        "    task=task,\n",
        "    model=\"gpt-4-turbo-2024-04-09\",\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "qT8mTiCvnIMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üèôÔ∏è **Evaluating LLM Response Using AutoJudge**  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Uin3dFy2nNk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ = \"What are the top attractions in New York City?\"\n",
        "output = \"Some top attractions in NYC include the Statue of Liberty and Central Park.\"\n",
        "\n",
        "judgment = autojudge.judge(input=input_, output=output)\n",
        "\n",
        "print(judgment.reasoning)\n",
        "print(judgment.score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXIAN55VlS_1",
        "outputId": "7670e935-1db3-4999-e802-011e55b18b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output meets all evaluation criteria outlined in the grading note. First, the accuracy of content is upheld as the information regarding attractions in New York City, specifically the Statue of Liberty and Central Park, are based on real, verifiable facts with no fictional elements. Second, contextual accuracy is met because the query is about popular attractions and the answer is straightforward and directly related to what is typically sought by tourists and residents alike. Third, the clarity and understandability criterion is satisfied as the output is concise, clearly stated, and uses language that is easily understandable by a general audience; it avoids jargon and technical terms. Lastly, the helpfulness and relevance criterion is also met. The response directly addresses the user's query about top attractions in New York City, providing useful and relevant information that effectively responds to the user‚Äôs interest in tourist sites.\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úÖ **Using RAFTCorrectness to Evaluate LLM Response**  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X2BAn6rQn6XQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from judges.classifiers.correctness import RAFTCorrectness\n",
        "\n",
        "correctness = RAFTCorrectness(model='gpt-4o-mini')\n",
        "\n",
        "input_text = \"What is the capital of France?\"\n",
        "expected_output = \"Paris\"\n",
        "generated_output = \"Paris\"\n",
        "\n",
        "judgment = correctness.judge(\n",
        "    input=input_text,\n",
        "    output=generated_output,\n",
        "    expected=expected_output,\n",
        ")\n",
        "\n",
        "print(judgment.reasoning)\n",
        "\n",
        "print(judgment.score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pER3Z6WBn51Z",
        "outputId": "812c3882-6d86-4d88-a0ff-5371fd9a629d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[The student answer matches the teacher's answer in terms of keywords and numerical values without any conflicting statements. Additionally, if the student answer contains extra factual information that aligns with the teacher's view, it is still considered accurate. Therefore, upon review, the student answer meets all necessary criteria.]\n",
            "True\n"
          ]
        }
      ]
    }
  ]
}