{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Build Fast with AI](https://img.shields.io/badge/BuildFastWithAI-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://www.buildfastwithai.com/genai-course)\n",
        "[![EduChain GitHub](https://img.shields.io/github/stars/satvik314/educhain?style=for-the-badge&logo=github&color=gold)](https://github.com/satvik314/educhain)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Cr_tq7DbbS7pQC5y6NlJ7NtPqqs0UBDl?usp=sharing)\n",
        "## Master Generative AI in 6 Weeks\n",
        "**What You'll Learn:**\n",
        "- Build with Latest LLMs\n",
        "- Create Custom AI Apps\n",
        "- Learn from Industry Experts\n",
        "- Join Innovation Community\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "*Empowering the Next Generation of AI Innovators"
      ],
      "metadata": {
        "id": "fAMHhp_wnRHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Chonkie-AI: Advanced Text Chunking for RAG**\n",
        "\n",
        "Chonkie-AI is a powerful text-chunking library designed for Retrieval-Augmented Generation (RAG) applications. It provides various chunking methods to efficiently split text into meaningful segments for better retrieval and processing.\n",
        "\n",
        "### **🚀 Supported Chunking Methods**  \n",
        "- **🔢 TokenChunker** – Splits text into fixed-size token chunks.  \n",
        "- **📝 WordChunker** – Chunks text based on words.  \n",
        "- **📖 SentenceChunker** – Chunks text at sentence boundaries.  \n",
        "- **🔄 RecursiveChunker** – Uses hierarchical splitting with customizable rules.  \n",
        "- **🧠 SemanticChunker** – Splits text based on semantic similarity.  \n",
        "- **🔍 SDPMChunker** – Utilizes a Semantic Double-Pass Merge approach.  \n",
        "- **🧪 LateChunker (Experimental)** – Embeds text first, then chunks for improved embeddings."
      ],
      "metadata": {
        "id": "RLUHqgOWnQhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📦 **Dependency Installation**  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KSRv7qV0nQtM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLZ4iMGobpEe"
      },
      "outputs": [],
      "source": [
        "!pip install -q chonkie tiktoken docling model2vec vicinity together rich[jupyter]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🔤 Importing TokenChunker and GPT-2 Tokenizer**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zEg18mo5wNb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from chonkie import TokenChunker\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "UtagcBr9lcoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🛠️ Initializing TokenChunker with GPT-2 Tokenizer**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SAdGDftBwVle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunker = TokenChunker(tokenizer)"
      ],
      "metadata": {
        "id": "x-nlAsIydECo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **📚 Chunking Text with TokenChunker**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DhuEFcc8wcIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = chunker(\"Woah! Chonkie, the chunking library is so cool! I love the tiny hippo hehe.\")"
      ],
      "metadata": {
        "id": "AefblRMijMzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🔍 Iterating Through Chunks and Displaying Details**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hDxU4CWtwcvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in chunks:\n",
        "    print(f\"Chunk: {chunk.text}\")\n",
        "    print(f\"Tokens: {chunk.token_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl6wnNwvlj4S",
        "outputId": "ff130375-d769-4741-8688-96fb79a0d352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk: Woah! Chonkie, the chunking library is so cool! I love the tiny hippo hehe.\n",
            "Tokens: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **📂 Importing Libraries for Document Processing and Embeddings**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0ojZk6Mlwoxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List\n",
        "\n",
        "from docling.document_converter import DocumentConverter\n",
        "from google.colab import userdata\n",
        "from model2vec import StaticModel\n",
        "from rich.console import Console\n",
        "from rich.text import Text\n",
        "from together import Together\n",
        "from transformers import AutoTokenizer\n",
        "from vicinity import Backend, Metric, Vicinity\n",
        "from google.colab import userdata\n",
        "\n",
        "from chonkie import RecursiveChunker, RecursiveLevel, RecursiveRules"
      ],
      "metadata": {
        "id": "GwuduWE8lmyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🖥️ Setting Up Rich Console for Pretty Printing**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cjQev3_fwtRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rich.console import Console\n",
        "\n",
        "console = Console()\n",
        "\n",
        "\n",
        "# A wrapper to pretty print\n",
        "def rprint(text: str, console: Console = console, width: int = 80) -> None:\n",
        "    richtext = Text(text)\n",
        "    console.print(richtext.wrap(console, width=width))"
      ],
      "metadata": {
        "id": "xPY83C6pwyjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🔑 Setting Up API Keys and Loading Models**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V2TZCV1rz_k-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"TOGETHER_API_KEY\"] = userdata.get(\"TOGETHER_API_KEY\")\n",
        "\n",
        "model = StaticModel.from_pretrained(\"minishlab/potion-retrieval-32M\")\n",
        "\n",
        "client = Together()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1\")"
      ],
      "metadata": {
        "id": "8-AtGx3ww00p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **📄 Converting Document to Markdown Format**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9b92xdZ30Dl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter = DocumentConverter()\n",
        "source = \"https://arxiv.org/pdf/1706.03762\"\n",
        "result = converter.convert(source)\n",
        "text = result.document.export_to_markdown()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfas2uayxB8h",
        "outputId": "4279421d-e25f-471c-a9c1-98983f594f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n",
            "WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rprint(text[:2000])"
      ],
      "metadata": {
        "id": "MlZk-7cBxoJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🔢 Calculating Total Token Count in PDF**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9vycB0hQ0IlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_text_tokens = len(tokenizer.encode(text))\n",
        "rprint(f\"This PDF contains: {total_text_tokens} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "xA6fL_Suxp2f",
        "outputId": "a7a9cb39-c3f6-4aad-9bcd-e7e0898ace96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "This PDF contains: 9865 tokens\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">This PDF contains: 9865 tokens\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **📜 Defining Recursive Chunking Rules**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GRSxSaG90MRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rules = RecursiveRules(\n",
        "    levels=[\n",
        "        RecursiveLevel(delimiters=[\"######\", \"#####\", \"####\", \"###\", \"##\", \"#\"], include_delim=\"next\"),\n",
        "        RecursiveLevel(delimiters=[\"\\n\\n\", \"\\n\", \"\\r\\n\", \"\\r\"]),\n",
        "        RecursiveLevel(delimiters=\".?!;:\"),\n",
        "        RecursiveLevel(),\n",
        "    ]\n",
        ")\n",
        "chunker = RecursiveChunker(rules=rules, chunk_size=384)"
      ],
      "metadata": {
        "id": "8jwGVTrtxtFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **📊 Chunking Text and Counting Total Chunks**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4B8G6-sg0Poj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = chunker(text)\n",
        "print(f\"Total number of chunks: {len(chunks)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnRG6WZyxvTm",
        "outputId": "6685df99-b7aa-464c-825c-036b16b41f95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of chunks: 57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🔍 Displaying Sample Chunks from Text**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kVa3v9RU0Tgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in chunks[:4]:\n",
        "    rprint(chunk.text)\n",
        "    print(\"-\" * 80, \"\\n\\n\")"
      ],
      "metadata": {
        "id": "nE8zn11nxxUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **📈 Encoding Chunks into Vector Representations**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xQfJkSva0W2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "items = [chunk.text for chunk in chunks]\n",
        "vectors = model.encode(items)\n",
        "print(vectors.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SV6SHm7qxzKR",
        "outputId": "74beb8e3-4567-46da-ae8e-beab3523e3ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(57, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🧭 Creating a Vicinity Index for Similarity Search**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kG2hF09U0aYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vicinity = Vicinity.from_vectors_and_items(\n",
        "    vectors=vectors, items=items, backend_type=Backend.BASIC, metric=Metric.COSINE\n",
        ")\n"
      ],
      "metadata": {
        "id": "Hqb1uhRhx2qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🔎 Retrieving Similar Chunks Using Embeddings**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nFKUUrQ40eAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_embeddings(query: str):\n",
        "    query_vector = model.encode(query)\n",
        "    results = vicinity.query(query_vector, k=4)\n",
        "    return [x[0] for x in results[0]]"
      ],
      "metadata": {
        "id": "O11Cf-Spx4di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🤖 Retrieving and Displaying Relevant Chunks**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VOBMN0IT0hUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query = \"What is a Multi-Head Self Attention?\"\n",
        "retrieved_chunks = get_embeddings(query)\n",
        "\n",
        "for chunk in retrieved_chunks:\n",
        "    rprint(chunk)\n",
        "    print(\"-\" * 80, \"\\n\\n\")"
      ],
      "metadata": {
        "id": "XxditPhEx58E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🔍 Querying for Multi-Head Self Attention Explanation**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I1L3_Mor0jox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is a Multi-Head Self Attention?\"\n",
        "retrieved_chunks = get_embeddings(query)\n",
        "\n",
        "for chunk in retrieved_chunks:\n",
        "    rprint(chunk)\n",
        "    print(\"-\" * 80, \"\\n\\n\")"
      ],
      "metadata": {
        "id": "9CaI95Bnx8ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **📝 Generating a Prompt for Context-Based Question Answering**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yThGfOho0sd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(chunks: List[str], query: str) -> str:\n",
        "    prompt_template = \"\"\"\n",
        "  Based on the provided contexts, answer the given question to the best of your ability. Remember to also add citations at appropriate points in the format of square brackets like [1][2][3], especially at sentence or paragraph endings.\n",
        "  You will be given 4 passages in the context, marked with a label 'Doc [1]:' to denote the passage number. Use that number for citations. Answer only from the given context, and if there's no appropriate context, reply \"No relevant context found!\".\n",
        "\n",
        "\n",
        "\n",
        "  {context}\n",
        "\n",
        "\n",
        "\n",
        "  {query}\n",
        "\n",
        "  \"\"\"\n",
        "    context = \"\\n\\n\".join([f\"Doc {i+1}: {chunk}\" for i, chunk in enumerate(chunks)])\n",
        "    prompt = prompt_template.format(context=context, query=query)\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "Qz3FQ87LysUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🛠️ Creating a Query-Specific Prompt with Retrieved Context**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BaGsPzNN0xW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is a Multi-Head Self Attention?\"\n",
        "retrieved_chunks = get_embeddings(query)\n",
        "prompt = create_prompt(retrieved_chunks, query)"
      ],
      "metadata": {
        "id": "Q3BS0kLGx-Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🤖 Generating an AI Response Using OpenAI GPT-4o**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T6hNRoOn03IN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "client = openai.OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        ")\n",
        "\n",
        "answer = response.choices[0].message.content\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ-RKD5syAlQ",
        "outputId": "431c321a-c4cf-4c7c-d9d8-de0e244653be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-Head Self Attention is an attention mechanism used in transformer models where instead of performing a single attention function, the model performs multiple attention functions, or heads, in parallel. Each head independently projects the queries, keys, and values into different subspaces by using learned linear projections, allowing the model to capture information from different representation subspaces at different positions. The outputs from these heads are then concatenated and projected again to form the final output. This mechanism allows the model to attend to information jointly from different sources, avoiding the averaging effect that a single attention head would introduce. In the described setup, there are typically 8 parallel attention layers or heads, each with a reduced dimension, making the computational cost similar to that of single-head attention with full dimensionality [4].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **📊 Calculating Token Count for the Prompt**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YxFCkZPq06v3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_tokens = len(tokenizer.encode(prompt))\n",
        "rprint(f\"This prompt contains: {prompt_tokens} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "EdqBoT_qyCSH",
        "outputId": "4b98dbf9-e6c7-405e-b76e-8c74ffdd91b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "This prompt contains: 1050 tokens\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">This prompt contains: 1050 tokens\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}