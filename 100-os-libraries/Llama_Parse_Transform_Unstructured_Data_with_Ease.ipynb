{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Build Fast with AI](https://img.shields.io/badge/BuildFastWithAI-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://www.buildfastwithai.com/genai-course)\n",
        "[![EduChain GitHub](https://img.shields.io/github/stars/satvik314/educhain?style=for-the-badge&logo=github&color=gold)](https://github.com/satvik314/educhain)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1-zxVAH43LjFx-Qmamqx87BvnIAcLkwCt#scrollTo=QtIS_AMUtw56)\n",
        "## Master Generative AI in 6 Weeks\n",
        "**What You'll Learn:**\n",
        "- Build with Latest LLMs\n",
        "- Create Custom AI Apps\n",
        "- Learn from Industry Experts\n",
        "- Join Innovation Community\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "*Empowering the Next Generation of AI Innovators"
      ],
      "metadata": {
        "id": "Ato16ZPupsxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Llama Parse: Transform Unstructured Data with Ease**  \n",
        " Llama Parse is a powerful tool designed to transform unstructured data into structured formats, handling sources like PDFs, HTML, and text files. 📄 It simplifies large-scale data parsing, enabling seamless integration with workflows and making complex tasks more efficient. 💡 Tailored for developers, it offers flexible customization options while connecting parsed data directly to LLMs. 🔗 With its precise, fast, and reliable data extraction capabilities, Llama Parse boosts productivity and empowers AI-driven workflows!\n"
      ],
      "metadata": {
        "id": "SJFxdo74p-PF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Building a RAG Pipeline over Legal Documents**"
      ],
      "metadata": {
        "id": "QRBnQe3jpp5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Setup and Installation**\n",
        "\n"
      ],
      "metadata": {
        "id": "bM6TTjr-qtNL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opCPADfRpd1O"
      },
      "outputs": [],
      "source": [
        "%pip install llama-index llama-parse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['LLAMA_CLOUD_API_KEY']=userdata.get('LLAMA-CLOUD-API')"
      ],
      "metadata": {
        "id": "5IfOc516quz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **📥 Downloading and Extracting Dataset 📂**\n"
      ],
      "metadata": {
        "id": "BGe2fbBUrFWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/user-attachments/files/16447759/data.zip -O data.zip\n",
        "!unzip -o data.zip\n",
        "!rm data.zip"
      ],
      "metadata": {
        "id": "EkdMIdG1qiXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "jysohauAqkIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **📂 Parsing US Legal Documents with LlamaParse ⚖️**"
      ],
      "metadata": {
        "id": "SCWBMHIhrk-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_parse import LlamaParse\n",
        "\n",
        "parser = LlamaParse(\n",
        "    result_type=\"markdown\",\n",
        "    parsing_instruction=\"Provided are a series of US legal documents.\",\n",
        "    use_vendor_multimodal_model=True,\n",
        "    vendor_multimodal_model_name=\"openai-gpt4o\",\n",
        "    show_progress=True,\n",
        ")\n",
        "\n",
        "DATA_DIR = \"data\"\n",
        "\n",
        "\n",
        "def get_data_files(data_dir=DATA_DIR) -> list[str]:\n",
        "    files = []\n",
        "    for f in os.listdir(data_dir):\n",
        "        fname = os.path.join(data_dir, f)\n",
        "        if os.path.isfile(fname):\n",
        "            files.append(fname)\n",
        "    return files\n",
        "\n",
        "\n",
        "files = get_data_files()"
      ],
      "metadata": {
        "id": "XeYL0-71rXNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = parser.load_data(\n",
        "    files,\n",
        "    extra_info={\"name\": \"US legal documents provided by the Library of Congress.\"},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RR08Kd8rmZ8",
        "outputId": "d9be6b3b-3f30-4a13-9166-477a29e9e174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parsing files: 100%|██████████| 8/8 [02:01<00:00, 15.23s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🔍 Setting Up VectorStore Index for Legal Documents 📚**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "49HeZoKLtQLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    StorageContext,\n",
        "    load_index_from_storage,\n",
        "    Settings,\n",
        ")\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
        "llm = OpenAI(\"gpt-4o\")\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "\n",
        "if not os.path.exists(\"storage_legal\"):\n",
        "    index = VectorStoreIndex(documents, embed_model=embed_model)\n",
        "    index.storage_context.persist(persist_dir=\"./storage_legal\")\n",
        "else:\n",
        "    ctx = StorageContext.from_defaults(persist_dir=\"./storage_legal\")\n",
        "    index = load_index_from_storage(ctx)\n",
        "\n",
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "I6rHT2iLrfml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **📝 Querying Legal Document Index for Information 🔍**"
      ],
      "metadata": {
        "id": "uYnpgI4_tU6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "response = query_engine.query(\n",
        "    \"Where did the majority of Barre Savings Bank's loans go?\"\n",
        ")\n",
        "display(Markdown(str(response)))"
      ],
      "metadata": {
        "id": "ghYVvWg2tE_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"Why does Mr. Kubarych believe foreign markets are so important?\"\n",
        ")\n",
        "display(Markdown(str(response)))"
      ],
      "metadata": {
        "id": "RXRvHp1ltIE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"Who is against the proposal of offshore drilling in CA and why?\"\n",
        ")\n",
        "display(Markdown(str(response)))"
      ],
      "metadata": {
        "id": "70FhyDzbtbqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"What is the purpose of the Ocean Science and Technology Subcommittee?\"\n",
        ")\n",
        "display(Markdown(str(response)))"
      ],
      "metadata": {
        "id": "QtIS_AMUtw56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Multimodal Parsing using GPT4o-mini**"
      ],
      "metadata": {
        "id": "e4ErX5zFuB-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **📥 Downloading Llama3.1 Blog PDF 📝**"
      ],
      "metadata": {
        "id": "AofNtMT7vfhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://www.dropbox.com/scl/fi/8iu23epvv3473im5rq19g/llama3.1_blog.pdf?rlkey=5u417tbdox4aip33fdubvni56&st=dzozd11e&dl=1\" -O \"data/llama3.1_blog.pdf\"\n"
      ],
      "metadata": {
        "id": "nNENQZzat0Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Initialize LlamaParse**\n"
      ],
      "metadata": {
        "id": "y2mq0b7_vQLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.schema import TextNode\n",
        "from typing import List\n",
        "import json\n",
        "\n",
        "\n",
        "def get_text_nodes(json_list: List[dict]):\n",
        "    text_nodes = []\n",
        "    for idx, page in enumerate(json_list):\n",
        "        text_node = TextNode(text=page[\"md\"], metadata={\"page\": page[\"page\"]})\n",
        "        text_nodes.append(text_node)\n",
        "    return text_nodes\n",
        "\n",
        "\n",
        "def save_jsonl(data_list, filename):\n",
        "    \"\"\"Save a list of dictionaries as JSON Lines.\"\"\"\n",
        "    with open(filename, \"w\") as file:\n",
        "        for item in data_list:\n",
        "            json.dump(item, file)\n",
        "            file.write(\"\\n\")\n",
        "\n",
        "\n",
        "def load_jsonl(filename):\n",
        "    \"\"\"Load a list of dictionaries from JSON Lines.\"\"\"\n",
        "    data_list = []\n",
        "    with open(filename, \"r\") as file:\n",
        "        for line in file:\n",
        "            data_list.append(json.loads(line))\n",
        "    return data_list\n",
        "from llama_parse import LlamaParse\n",
        "\n",
        "parser = LlamaParse(\n",
        "    result_type=\"markdown\",\n",
        "    use_vendor_multimodal_model=True,\n",
        "    vendor_multimodal_model_name=\"openai-gpt-4o-mini\",\n",
        "    invalidate_cache=True,\n",
        ")\n",
        "json_objs = parser.get_json_result(\"./data/llama3.1_blog.pdf\")\n",
        "json_list = json_objs[0][\"pages\"]\n",
        "docs = get_text_nodes(json_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GK5gWCcSuTV4",
        "outputId": "9b298c4b-9f09-4abc-ef5a-11481f0812bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id c7cd9ead-5a69-4aad-bef0-5b33ff83346e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_jsonl([d.dict() for d in docs], \"docs.jsonl\")\n"
      ],
      "metadata": {
        "id": "LNoGgYyXuZCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Document\n",
        "\n",
        "docs_dicts = load_jsonl(\"docs.jsonl\")\n",
        "docs = [Document.parse_obj(d) for d in docs_dicts]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBdO4uDOuaxq",
        "outputId": "6efe0044-b609-49d1-f9a0-0654dab611ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-ea776a9a360e>:4: PydanticDeprecatedSince20: The `parse_obj` method is deprecated; use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  docs = [Document.parse_obj(d) for d in docs_dicts]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Setup GPT-4o baseline**\n"
      ],
      "metadata": {
        "id": "ryym4LSsvVPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_parse import LlamaParse\n",
        "\n",
        "parser_gpt4o = LlamaParse(\n",
        "    result_type=\"markdown\",\n",
        "    use_vendor_multimodal_model=True,\n",
        "    vendor_multimodal_model=\"openai-gpt4o\",\n",
        "    # invalidate_cache=True\n",
        ")\n",
        "json_objs_gpt4o = parser_gpt4o.get_json_result(\"./data/llama3.1_blog.pdf\")\n",
        "# json_objs_gpt4o = parser.get_json_result(\"./data/llama2-p33.pdf\")\n",
        "json_list_gpt4o = json_objs_gpt4o[0][\"pages\"]\n",
        "docs_gpt4o = get_text_nodes(json_list_gpt4o)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt10NhKSudQB",
        "outputId": "3a8feaab-1fd1-4bf9-d300-2464f2fe6e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id a9760515-355e-4089-b8ff-63642cee140d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_jsonl([d.dict() for d in docs_gpt4o], \"docs_gpt4o.jsonl\")\n"
      ],
      "metadata": {
        "id": "FtZqmeDXuiU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Document\n",
        "\n",
        "docs_gpt4o_dicts = load_jsonl(\"docs_gpt4o.jsonl\")\n",
        "docs_gpt4o = [Document.parse_obj(d) for d in docs_gpt4o_dicts]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjQu4phLufeN",
        "outputId": "4136d606-d764-40e1-b481-9a7d68f6087f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-fae74f861b4c>:4: PydanticDeprecatedSince20: The `parse_obj` method is deprecated; use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  docs_gpt4o = [Document.parse_obj(d) for d in docs_gpt4o_dicts]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**View Results**\n"
      ],
      "metadata": {
        "id": "R6De1hJnvaer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[4].get_content(metadata_mode=\"all\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "savpR4LxuqS5",
        "outputId": "a8f392c6-6127-4f01-b8f9-7d589c61236a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page: 5\n",
            "\n",
            "# Llama 3.1 Model Evaluation\n",
            "\n",
            "## Benchmark Results\n",
            "\n",
            "| Category        | Llama 3.1 8B | Gemma 2 9B IT | Mistral 7B Instruct | Llama 3.1 70B | Mistral 8x22B Instruct | GPT 3.5 Turbo |\n",
            "|-----------------|---------------|----------------|----------------------|----------------|------------------------|----------------|\n",
            "| General         |               |                |                      |                |                        |                |\n",
            "| MMLU            | 73.0          | 72.3           | 60.5                 | 86.0           | 79.9                   | 69.8           |\n",
            "| MMLU PRO        | 48.3          | 36.9           | 36.9                 | 66.4           | 56.3                   | 49.2           |\n",
            "| iEval           | 80.4          | 73.6           | 57.6                 | 87.5           | 69.9                   |                |\n",
            "| Code            |               |                |                      |                |                        |                |\n",
            "| HumanEval       | 72.6          | 54.3           | 40.2                 | 80.5           | 75.6                   |                |\n",
            "| MBPP Eval       | 72.8          | 71.7           | 61.8                 | 82.0           |                        |                |\n",
            "| Mash            | 84.5          | 76.7           | 53.2                 | 95.1           | 88.2                   |                |\n",
            "| MATH            | 51.9          | 44.3           | 13.0                 | 68.0           |                        |                |\n",
            "| Reasoning       |               |                |                      |                |                        |                |\n",
            "| ARC Challenge    | 83.4          | 87.6           | 74.2                 | 28.8           |                        |                |\n",
            "| GPA             | 32.8          | 28.8           | 46.7                 | 33.3           | 33.3                   | 28.5           |\n",
            "| Tool use        |               |                |                      |                |                        |                |\n",
            "| BFCL            | 76.1          | 30.0           | 24.7                 | 56.7           |                        |                |\n",
            "| Nexus           | 38.5          |                |                      |                |                        |                |\n",
            "| Long context    |               |                |                      |                |                        |                |\n",
            "| ZeroSCROLLS/Quality | 81.0     |                |                      | 90.5           |                        |                |\n",
            "| InfiniteBench/En.MC | 65.1     |                |                      |                |                        |                |\n",
            "| NIH/Multi-needle | 98.8         | -              | -                    | 97.5           | -                      | -              |\n",
            "| Multilingual NGSM | 68.9       | 53.2           | 29.9                 | 86.9           | 71.1                   | 51.4           |\n",
            "\n",
            "## Llama 3.1 405B Human Evaluation\n",
            "\n",
            "| Comparison                          | Win Rate | Tie Rate | Loss Rate |\n",
            "|-------------------------------------|----------|----------|-----------|\n",
            "| Llama 3.1 405B vs GPT-4-0125-Preview | 23.3%    | 52.2%    | 24.5%     |\n",
            "| Llama 3.1 405B vs GPT-4o           | 19.1%    | 51.7%    | 29.2%     |\n",
            "| Llama 3.1 405B vs Claude 3.5 Sonnet | 24.9%    | 50.8%    | 24.2%     |\n"
          ]
        }
      ]
    }
  ]
}