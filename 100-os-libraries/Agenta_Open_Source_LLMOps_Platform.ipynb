{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Build Fast with AI](https://img.shields.io/badge/BuildFastWithAI-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://www.buildfastwithai.com/genai-course)\n",
        "[![EduChain GitHub](https://img.shields.io/github/stars/satvik314/educhain?style=for-the-badge&logo=github&color=gold)](https://github.com/satvik314/educhain)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/16UjTf2r6BtlT5B-La40UqJN4Ep17dAPu?usp=sharing)\n",
        "## Master Generative AI in 6 Weeks\n",
        "**What You'll Learn:**\n",
        "- Build with Latest LLMs\n",
        "- Create Custom AI Apps\n",
        "- Learn from Industry Experts\n",
        "- Join Innovation Community\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "*Empowering the Next Generation of AI Innovators"
      ],
      "metadata": {
        "id": "QkLMGU4Rb_qr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Agenta: Open-Source LLMOps Platform\n",
        "\n",
        "[Agenta](https://github.com/Agenta-AI/agenta) simplifies **LLMOps** with tools for **development, deployment, and monitoring** of LLM-powered apps. It supports **RAG, agents,** and frameworks like **LlamaIndex & LangChain**.\n",
        "\n",
        "## üåü Key Features\n",
        "\n",
        "- üé≠ **Prompt Playground** ‚Äì Test & compare 50+ LLMs.  \n",
        "- üîÑ **Custom Workflows** ‚Äì Build RAG & agent-based systems.  \n",
        "- üìä **LLM Evaluation** ‚Äì Use built-in & custom evaluators.  \n",
        "- üßë‚Äç‚öñÔ∏è **Human Evaluation** ‚Äì A/B testing & expert annotations.  \n",
        "- üîç **Prompt Management** ‚Äì Version control & configurations.  \n",
        "- üëÄ **Observability & Tracing** ‚Äì Debug & monitor in real time."
      ],
      "metadata": {
        "id": "VFcyWcEjb_lO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üì¶ **Dependency Installation**  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_Bjn56Mdb_dB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETdNL-ELWHxJ"
      },
      "outputs": [],
      "source": [
        "pip install -U agenta openai opentelemetry-instrumentation-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opentelemetry-instrumentation-langchain langchain_community langchain_openai instructor litellm"
      ],
      "metadata": {
        "id": "3u72XhTJtuyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üîë Setup API Keys**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RNBUMT-yrGCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY']=userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "os.environ[\"AGENTA_API_KEY\"] = userdata.get(\"AGENTA_API_KEY\")\n",
        "os.environ[\"AGENTA_HOST\"] = \"https://cloud.agenta.ai\""
      ],
      "metadata": {
        "id": "1Br6GUnPnxvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ü§ñ Initialize Agenta & OpenAI Client**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EVDheWLqsC5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import agenta as ag\n",
        "from opentelemetry.instrumentation.openai import OpenAIInstrumentor\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "ag.init()\n",
        "\n",
        "OpenAIInstrumentor().instrument()\n",
        "\n",
        "\n",
        "response= client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a short story about AI Engineering.\"},\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "bph-xy8moMME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üõ†Ô∏è Instrumenting a Workflow with a Parent Span**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9s3xaSGLsLfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import agenta as ag\n",
        "from opentelemetry.instrumentation.openai import OpenAIInstrumentor\n",
        "\n",
        "from openai import OpenAI\n",
        "import asyncio\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "ag.init()\n",
        "OpenAIInstrumentor().instrument()\n",
        "\n",
        "\n",
        "@ag.instrument(spankind=\"TASK\")\n",
        "async def generate_story_prompt(topic: str, genre: str):\n",
        "    return f\"Write a {genre} story about {topic}.\"\n",
        "\n",
        "\n",
        "@ag.instrument(spankind=\"WORKFLOW\")\n",
        "async def generate_story(topic: str, genre: str):\n",
        "    prompt = await generate_story_prompt(topic, genre)\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            },\n",
        "        ],\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "async def main():\n",
        "    result = await generate_story(topic=\"the future\", genre=\"sci-fi\")\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        asyncio.run(main())\n",
        "    except RuntimeError as e:\n",
        "        if \"cannot be called from a running event loop\" in str(e):\n",
        "            loop = asyncio.get_event_loop()\n",
        "            if loop.is_running():\n",
        "                print(\"An event loop is already running.  Attempting to use it.\")\n",
        "                loop.run_until_complete(main())\n",
        "            else:\n",
        "                print(\"No running event loop found, even though RuntimeError was raised.\")\n",
        "                raise\n",
        "        else:\n",
        "            raise"
      ],
      "metadata": {
        "id": "q-nvjI6pqc2E",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üîó Instrumenting a Workflow with LangChain**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F3euzZ6SsiEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import agenta as ag\n",
        "from opentelemetry.instrumentation.langchain import LangchainInstrumentor\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain, SequentialChain, TransformChain\n",
        "\n",
        "ag.init()\n",
        "LangchainInstrumentor().instrument()\n",
        "\n",
        "\n",
        "def langchain_app():\n",
        "    chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "    transform = TransformChain(\n",
        "        input_variables=[\"subject\"],\n",
        "        output_variables=[\"prompt\"],\n",
        "        transform=lambda inputs: {\"prompt\": f\"Tell me a joke about {inputs['subject']}.\"},\n",
        "    )\n",
        "\n",
        "    first_prompt_messages = [\n",
        "        SystemMessage(content=\"You are a funny sarcastic nerd.\"),\n",
        "        HumanMessage(content=\"{prompt}\"),\n",
        "    ]\n",
        "    first_prompt_template = ChatPromptTemplate.from_messages(first_prompt_messages)\n",
        "    first_chain = LLMChain(llm=chat, prompt=first_prompt_template, output_key=\"joke\")\n",
        "\n",
        "    second_prompt_messages = [\n",
        "        SystemMessage(content=\"You are an Elf.\"),\n",
        "        HumanMessagePromptTemplate.from_template(\n",
        "            \"Translate the joke below into Sindarin language:\\n{joke}\"\n",
        "        ),\n",
        "    ]\n",
        "    second_prompt_template = ChatPromptTemplate.from_messages(second_prompt_messages)\n",
        "    second_chain = LLMChain(llm=chat, prompt=second_prompt_template)\n",
        "\n",
        "    workflow = SequentialChain(\n",
        "        chains=[transform, first_chain, second_chain],\n",
        "        input_variables=[\"subject\"],\n",
        "    )\n",
        "\n",
        "    result = workflow({\"subject\": \"OpenTelemetry\"})\n",
        "    print(result)\n",
        "\n",
        "langchain_app()"
      ],
      "metadata": {
        "id": "aQ501RektjKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üìö Instrumenting a Workflow with Instructor**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-JkDG5dcstCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import agenta as ag\n",
        "import openai\n",
        "import instructor\n",
        "from pydantic import BaseModel\n",
        "from opentelemetry.instrumentation.openai import OpenAIInstrumentor\n",
        "\n",
        "ag.init()\n",
        "\n",
        "OpenAIInstrumentor().instrument()\n",
        "\n",
        "class UserInfo(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "\n",
        "@ag.instrument(spankind=\"WORKFLOW\")\n",
        "def instructor_workflow():\n",
        "    client = instructor.from_openai(openai.OpenAI())\n",
        "\n",
        "    user_info = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        response_model=UserInfo,\n",
        "        messages=[{\"role\": \"user\", \"content\": \"John Doe is 30 years old.\"}],\n",
        "    )\n",
        "    return user_info\n",
        "\n",
        "user_info = instructor_workflow()\n",
        "print(user_info)"
      ],
      "metadata": {
        "id": "bq7I1aTZuRmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **‚ö° Instrumenting a Workflow with LiteLLM**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1C61ILa2s2g5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import agenta as ag\n",
        "import litellm\n",
        "import asyncio\n",
        "\n",
        "ag.init()\n",
        "\n",
        "@ag.instrument()\n",
        "async def agenerate_completion():\n",
        "    litellm.callbacks = [ag.callbacks.litellm_handler()]\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a short story about AI Engineering.\"},\n",
        "    ]\n",
        "    temperature = 0.2\n",
        "    max_tokens = 100\n",
        "\n",
        "    chat_completion = await litellm.acompletion(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "    print(chat_completion)\n",
        "\n",
        "\n",
        "async def main():\n",
        "    await agenerate_completion()\n",
        "\n",
        "\n",
        "asyncio.get_event_loop().run_until_complete(main())"
      ],
      "metadata": {
        "id": "rOd_xurIv3oA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}