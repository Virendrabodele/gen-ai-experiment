{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Build Fast with AI](https://img.shields.io/badge/BuildFastWithAI-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://www.buildfastwithai.com/genai-course)\n",
        "[![EduChain GitHub](https://img.shields.io/github/stars/satvik314/educhain?style=for-the-badge&logo=github&color=gold)](https://github.com/satvik314/educhain)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/12TOxopyRs1xfFg8nb542U9wTehSLouzL#scrollTo=EDJ18EXZ5BUZ)\n",
        "## Master Generative AI in 6 Weeks\n",
        "**What You'll Learn:**\n",
        "- Build with Latest LLMs\n",
        "- Create Custom AI Apps\n",
        "- Learn from Industry Experts\n",
        "- Join Innovation Community\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "*Empowering the Next Generation of AI Innovators"
      ],
      "metadata": {
        "id": "sZC6IgC78kqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üî•üï∑Ô∏è Crawl4AI: Open-source LLM Friendly Web Crawler & Scrapper\n"
      ],
      "metadata": {
        "id": "EDJ18EXZ5BUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üï∑Ô∏èü§ñ Crawl4AI is an open-source Python library designed to simplify web crawling and extract useful information from web pages. This documentation will guide you through the features, usage, and customization of Crawl4AI.\n",
        "\n"
      ],
      "metadata": {
        "id": "DYstHgDM6_8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Quickstart with Crawl4AI**"
      ],
      "metadata": {
        "id": "hJ-PwEE58e2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Installation**\n",
        "Install Crawl4AI and necessary dependencies:"
      ],
      "metadata": {
        "id": "2ZV4inc45Mf-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jLUo4v5A49lI"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U crawl4ai\n",
        "!pip install nest_asyncio\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setting Up API Keys"
      ],
      "metadata": {
        "id": "3urU4vrW5jT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "eXCeG8-L5kma"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "McV_bRfo5Xjg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  **Basic Setup and Simple Crawl**"
      ],
      "metadata": {
        "id": "-1n2k3r95bN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from crawl4ai import AsyncWebCrawler, CacheMode\n",
        "\n",
        "async def simple_crawl():\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://www.buildfastwithai.com/\",\n",
        "            cach_mode = CacheMode.ENABLED # Default is ENABLED\n",
        "        )\n",
        "        print(result.markdown_v2.raw_markdown[:500].replace(\"\\n\", \" -- \"))  # Print the first 500 characters\n",
        "\n",
        "asyncio.run(simple_crawl())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf_WjXYf5bgz",
        "outputId": "2255c7d8-76fc-4496-b941-265e407dd791"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INIT].... ‚Üí Crawl4AI 0.4.0\n",
            "[COMPLETE] ‚óè Database backup created at: /root/.crawl4ai/crawl4ai.db.backup_20241209_102044\n",
            "[INIT].... ‚Üí Starting database migration...\n",
            "[COMPLETE] ‚óè Migration completed. 0 records processed.\n",
            "[FETCH]... ‚Üì https://www.buildfastwithai.com/... | Status: True | Time: 10.60s\n",
            "[SCRAPE].. ‚óÜ Processed https://www.buildfastwithai.com/... | Time: 824ms\n",
            "[COMPLETE] ‚óè https://www.buildfastwithai.com/... | Status: True | Total: 12.61s\n",
            "[![buildfastwithai](/_next/static/media/light.5e8e48b7.svg)](/) --  -- [GenAI Bootcamp](/genai-course)[Daily GenAI Quiz](/daily-quiz) --  -- [Resources](/#resources) --  -- [App Showcase](https://apps.buildfastwithai.com) --  -- [Events](/#events) --  -- More  --  -- Sign In --  -- [](https://www.linkedin.com/company/build-fast-with-ai/)[](https://x.com/satvikps) --  -- [![buildfastwithai](/_next/static/media/light.5e8e48b7.svg)](/) --  -- Sign In --  -- [](https://www.linkedin.com/company/build-fast-with-ai/)[](https://x.com/satvikps) --  -- ![](/_next/image?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Dynamic Content Handling**"
      ],
      "metadata": {
        "id": "00x8fKlV6G0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def crawl_dynamic_content():\n",
        "\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        js_code = [\n",
        "            \"const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More')); loadMoreButton && loadMoreButton.click();\"\n",
        "        ]\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://www.nbcnews.com/business\",\n",
        "            js_code=js_code,\n",
        "            # wait_for=wait_for,\n",
        "            cach_mode = CacheMode.ENABLED # Default is ENABLED\n",
        "        )\n",
        "        print(result.markdown_v2.raw_markdown[:500].replace(\"\\n\", \" -- \"))  # Print first 500 characters\n",
        "\n",
        "asyncio.run(crawl_dynamic_content())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arjRWVkC6H5m",
        "outputId": "41f0f6bf-5aaa-474b-a43a-7d876ef54587"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INIT].... ‚Üí Crawl4AI 0.4.0\n",
            "[FETCH]... ‚Üì https://www.nbcnews.com/business... | Status: True | Time: 10.76s\n",
            "[SCRAPE].. ‚óÜ Processed https://www.nbcnews.com/business... | Time: 2151ms\n",
            "[COMPLETE] ‚óè https://www.nbcnews.com/business... | Status: True | Total: 13.03s\n",
            "IE 11 is not supported. For an optimal experience visit our site on another browser. --  -- Skip to Content --  -- [NBC News Logo](https://www.nbcnews.com) --  -- Sponsored By --  --   * [Politics](https://www.nbcnews.com/politics) --   * [U.S. News](https://www.nbcnews.com/us-news) --   * Local --   * [New York](https://www.nbcnews.com/new-york) --   * [Los Angeles](https://www.nbcnews.com/los-angeles) --   * [Chicago](https://www.nbcnews.com/chicago) --   * [Dallas-Fort Worth](https://www.nbcnews.com/dallas-fort-worth) --   * [Philadelph\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Content Cleaning and Fit Markdown**"
      ],
      "metadata": {
        "id": "RF2pT42f6U5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from crawl4ai.content_filter_strategy import PruningContentFilter\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "\n",
        "async def clean_content():\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://en.wikipedia.org/wiki/Apple\",\n",
        "            excluded_tags=['nav', 'footer', 'aside'],\n",
        "            remove_overlay_elements=True,\n",
        "            # word_count_threshold=10,\n",
        "            cach_mode = CacheMode.ENABLED,\n",
        "            markdown_generator=DefaultMarkdownGenerator(\n",
        "                content_filter=PruningContentFilter(threshold=0.48, threshold_type=\"fixed\", min_word_threshold=0),\n",
        "                options={\n",
        "                    \"ignore_links\": True\n",
        "                }\n",
        "            ),\n",
        "\n",
        "        )\n",
        "        full_markdown_length = len(result.markdown_v2.raw_markdown)\n",
        "        fit_markdown_length = len(result.markdown_v2.fit_markdown)\n",
        "        print(f\"Full Markdown Length: {full_markdown_length}\")\n",
        "        print(f\"Fit Markdown Length: {fit_markdown_length}\")\n",
        "\n",
        "\n",
        "asyncio.run(clean_content())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yx8W7lk6WOL",
        "outputId": "faac0949-ec97-4e22-c1d6-6af5eacd55ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INIT].... ‚Üí Crawl4AI 0.4.0\n",
            "[FETCH]... ‚Üì https://en.wikipedia.org/wiki/Apple... | Status: True | Time: 6.15s\n",
            "[SCRAPE].. ‚óÜ Processed https://en.wikipedia.org/wiki/Apple... | Time: 3517ms\n",
            "[COMPLETE] ‚óè https://en.wikipedia.org/wiki/Apple... | Status: True | Total: 9.71s\n",
            "Full Markdown Length: 80379\n",
            "Fit Markdown Length: 72208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  **Link Analysis and Smart Filtering**"
      ],
      "metadata": {
        "id": "brKyGU436ioJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def link_analysis():\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://www.nbcnews.com/business\",\n",
        "            cach_mode = CacheMode.ENABLED,\n",
        "            exclude_external_links=True,\n",
        "            exclude_social_media_links=True,\n",
        "            # exclude_domains=[\"facebook.com\", \"twitter.com\"]\n",
        "        )\n",
        "        print(f\"Found {len(result.links['internal'])} internal links\")\n",
        "        print(f\"Found {len(result.links['external'])} external links\")\n",
        "\n",
        "        for link in result.links['internal'][:5]:\n",
        "            print(f\"Href: {link['href']}\\nText: {link['text']}\\n\")\n",
        "\n",
        "\n",
        "asyncio.run(link_analysis())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFlt7Vru6jul",
        "outputId": "ce5a8237-9407-4142-900a-6cd831057a67"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 139 internal links\n",
            "Found 39 external links\n",
            "Href: https://www.nbcnews.com\n",
            "Text: NBC News Logo\n",
            "\n",
            "Href: https://www.nbcnews.com/politics\n",
            "Text: Politics\n",
            "\n",
            "Href: https://www.nbcnews.com/us-news\n",
            "Text: U.S. News\n",
            "\n",
            "Href: https://www.nbcnews.com/new-york\n",
            "Text: New York\n",
            "\n",
            "Href: https://www.nbcnews.com/los-angeles\n",
            "Text: Los Angeles\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  **Media Handling**"
      ],
      "metadata": {
        "id": "PzMEnun06raZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def media_handling():\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://www.nbcnews.com/business\",\n",
        "            cach_mode = CacheMode.ENABLED,\n",
        "            exclude_external_images=False,\n",
        "            # screenshot=True # Set this to True if you want to take a screenshot\n",
        "        )\n",
        "        for img in result.media['images'][:5]:\n",
        "            print(f\"Image URL: {img['src']}, Alt: {img['alt']}, Score: {img['score']}\")\n",
        "\n",
        "asyncio.run(media_handling())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHbdJPOZ6sko",
        "outputId": "29c31997-2008-4ccd-dd55-1c16a0a210b4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image URL: https://media-cldnry.s-nbcnews.com/image/upload/t_focal-762x508,f_auto,q_auto:best/rockcms/2024-12/241203-donald-trump-al-1014-2ee816.jpg, Alt: Donald Trump, Score: 5\n",
            "Image URL: https://media-cldnry.s-nbcnews.com/image/upload/t_focal-762x508,f_auto,q_auto:best/rockcms/2024-12/241208-retail-wm-1127p-b77e92.jpg, Alt: Package Deliveries As Cyber Monday Deals Hit, Score: 5\n",
            "Image URL: https://media-cldnry.s-nbcnews.com/image/upload/t_focal-80x80,f_auto,q_auto:best/rockcms/2024-12/241206-donald-trump-MTP-interview-ac-936p-2e9e2d.jpg, Alt: donald trump mtp exclusive interview politics political politician meet the press, Score: 5\n",
            "Image URL: https://media-cldnry.s-nbcnews.com/image/upload/t_focal-80x80,f_auto,q_auto:best/rockcms/2024-12/241205-bitcoin-se-122p-ae5e82.jpg, Alt: Attendees during the Bitcoin 2024 conference, Score: 5\n",
            "Image URL: https://media-cldnry.s-nbcnews.com/image/upload/t_focal-80x80,f_auto,q_auto:best/rockcms/2024-05/240503-aetna-mn-1605-04ad07.jpg, Alt: Aetna headquarters in Hartford, Conn., on Tuesday, Nov. 22, 2016., Score: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM Extraction\n",
        "\n",
        "This example demonstrates how to use language model-based extraction to retrieve structured data from a pricing page on OpenAI‚Äôs site."
      ],
      "metadata": {
        "id": "nkc3QBDh79mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from crawl4ai.extraction_strategy import LLMExtractionStrategy\n",
        "from pydantic import BaseModel, Field\n",
        "import os, json\n",
        "\n",
        "\n",
        "class OpenAIModelFee(BaseModel):\n",
        "    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n",
        "    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n",
        "    output_fee: str = Field(\n",
        "        ..., description=\"Fee for output token for the OpenAI model.\"\n",
        "    )\n",
        "\n",
        "async def extract_structured_data_using_llm(provider: str, api_token: str = None, extra_headers: dict = None):\n",
        "    print(f\"\\n--- Extracting Structured Data with {provider} ---\")\n",
        "\n",
        "    # Skip if API token is missing (for providers that require it)\n",
        "    if api_token is None and provider != \"ollama\":\n",
        "        print(f\"API token is required for {provider}. Skipping this example.\")\n",
        "        return\n",
        "\n",
        "    extra_args = {\"extra_headers\": extra_headers} if extra_headers else {}\n",
        "\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://openai.com/api/pricing/\",\n",
        "            word_count_threshold=1,\n",
        "            extraction_strategy=LLMExtractionStrategy(\n",
        "                provider=provider,\n",
        "                api_token=api_token,\n",
        "                schema=OpenAIModelFee.schema(),\n",
        "                extraction_type=\"schema\",\n",
        "                instruction=\"\"\"Extract all model names along with fees for input and output tokens.\"\n",
        "                \"{model_name: 'GPT-4', input_fee: 'US$10.00 / 1M tokens', output_fee: 'US$30.00 / 1M tokens'}.\"\"\",\n",
        "                **extra_args\n",
        "            ),\n",
        "            cach_mode = CacheMode.ENABLED\n",
        "        )\n",
        "        print(json.loads(result.extracted_content)[:5])\n",
        "\n",
        "# Usage:\n",
        "await extract_structured_data_using_llm(\"openai/gpt-4o-mini\", os.getenv(\"OPENAI_API_KEY\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pmm3Nfv8BGD",
        "outputId": "6cf6a993-98ec-4e5c-e0bc-e61dd9db781b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Extracting Structured Data with openai/gpt-4o-mini ---\n",
            "[INIT].... ‚Üí Crawl4AI 0.4.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-9cb9ad09a3f6>:30: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  schema=OpenAIModelFee.schema(),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FETCH]... ‚Üì https://openai.com/api/pricing/... | Status: True | Time: 0.04s\n",
            "[SCRAPE].. ‚óÜ Processed https://openai.com/api/pricing/... | Time: 76ms\n",
            "[COMPLETE] ‚óè https://openai.com/api/pricing/... | Status: True | Total: 0.15s\n",
            "[{'model_name': 'GPT-4', 'input_fee': 'US$10.00 / 1M tokens', 'output_fee': 'US$30.00 / 1M tokens', 'error': False}]\n"
          ]
        }
      ]
    }
  ]
}