{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Build Fast with AI](https://img.shields.io/badge/BuildFastWithAI-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://www.buildfastwithai.com/genai-course)\n",
        "[![EduChain GitHub](https://img.shields.io/github/stars/satvik314/educhain?style=for-the-badge&logo=github&color=gold)](https://github.com/satvik314/educhain)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1q31zNNG1toRhcpoSnTpllZtTw0C6HwOJ?usp=sharing)\n",
        "## Master Generative AI in 6 Weeks\n",
        "**What You'll Learn:**\n",
        "- Build with Latest LLMs\n",
        "- Create Custom AI Apps\n",
        "- Learn from Industry Experts\n",
        "- Join Innovation Community\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "*Empowering the Next Generation of AI Innovators"
      ],
      "metadata": {
        "id": "cICHRcO6MZ5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Unsloth: Fine-tune Models 2-5x Faster with 80% Less Memory üöÄ**  \n",
        "Unsloth optimizes the fine-tuning of models like Llama 3.2, Mistral, Phi-3.5, Qwen 2.5, and Gemma, achieving up to 5x faster training.  \n",
        "It reduces memory usage by 80%, enabling efficient training even on hardware with limited resources.\n"
      ],
      "metadata": {
        "id": "bQQ4gU5BMdVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Setup and Installation**"
      ],
      "metadata": {
        "id": "o7CYWGI8Mdqa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN1scVZEMS6U"
      },
      "outputs": [],
      "source": [
        "pip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\" datasets evaluate unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torchvision\n",
        "!pip install torchvision"
      ],
      "metadata": {
        "id": "7Ml3zxfHPcRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Install and Import Necessary Libraries üì¶**\n"
      ],
      "metadata": {
        "id": "CUuUPhtaMrLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from unsloth import is_bfloat16_supported\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from datasets import load_dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjLBGCAWMej8",
        "outputId": "e841447c-fd16-46cc-f239-c24df2fc4139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Set Up Dataset and Max Sequence Length üìä**"
      ],
      "metadata": {
        "id": "_8-JfKgSM4Op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048 # Supports RoPE Scaling internally, so choose any!\n",
        "# Get LAION dataset\n",
        "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
        "dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train\")\n"
      ],
      "metadata": {
        "id": "Bdy8rpuKMZet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Define Available 4-bit Quantized Models ‚ö°**"
      ],
      "metadata": {
        "id": "_rODaZjIM9hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n"
      ],
      "metadata": {
        "id": "3iUYtFnXNCbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load Pretrained Model with 4-bit Quantization üîë**\n",
        "We are Using llama-3-8b Model for Quantization\n"
      ],
      "metadata": {
        "id": "tpPRsn1JNGQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")"
      ],
      "metadata": {
        "id": "I4T6NveDNELe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Apply LoRA Fine-Tuning to the Model üîß**\n"
      ],
      "metadata": {
        "id": "Ryd-ZtagNLjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do model patching and add fast LoRA weights\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    max_seq_length = max_seq_length,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAExrVFvNI8q",
        "outputId": "0dab022c-2030-41c2-8aab-8d7675bcee5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.1.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Set Up the Trainer and Training Arguments üèãÔ∏è**\n"
      ],
      "metadata": {
        "id": "uXIcFjeeNRIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    tokenizer = tokenizer,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 10,\n",
        "        max_steps = 60,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        output_dir = \"outputs\",\n",
        "        optim = \"adamw_8bit\",\n",
        "        seed = 3407,\n",
        "    ),\n",
        ")\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "d9DdeXZUNPGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prepare the Model for Inference and Generate Text ü§ñ**\n"
      ],
      "metadata": {
        "id": "PG37PaKQN_WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure you're using CUDA (GPU) if available, otherwise fallback to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move model to the selected device (CUDA or CPU)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "6GzPTdF1NT2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Query the Model"
      ],
      "metadata": {
        "id": "Z96t58OwDxco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Tell me a story about artificial intelligence and ethics.\"\n",
        "outputs = model.generate(**inputs, max_length=200)\n",
        "\n",
        "# Decode and print the result\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J661OyklRqkT",
        "outputId": "15c347f9-4afb-4a64-ba31-4c1299180991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tell me a story about artificial intelligence and ethics. What's the story?\n",
            "Artificial intelligence (AI) is a rapidly growing field of research and development. It is a branch of computer science that deals with the creation of intelligent machines that are capable of performing tasks that would normally require human intelligence. AI is used in a variety of fields, including robotics, natural language processing, computer vision, and machine learning. AI is also used in a variety of applications, such as autonomous vehicles, virtual assistants, and medical diagnosis. AI is a powerful tool, and it has the potential to revolutionize the way we live and work. However, it is important to consider the ethical implications of AI. AI can be used for both good and bad purposes, and it is important to ensure that AI is used responsibly and ethically. AI should be used to benefit humanity, and not to harm it. AI should be used to improve our lives, and not to replace them. AI should be used to create\n"
          ]
        }
      ]
    }
  ]
}