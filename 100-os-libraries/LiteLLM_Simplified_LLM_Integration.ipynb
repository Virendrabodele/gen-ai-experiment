{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Build Fast with AI](https://img.shields.io/badge/BuildFastWithAI-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://www.buildfastwithai.com/genai-course)\n",
        "[![EduChain GitHub](https://img.shields.io/github/stars/satvik314/educhain?style=for-the-badge&logo=github&color=gold)](https://github.com/satvik314/educhain)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1QAgwSpVHQ_g0cUnlU7v7wMUfK6qPcq5L#scrollTo=EWNH2kNTynj9)\n",
        "## Master Generative AI in 6 Weeks\n",
        "**What You'll Learn:**\n",
        "- Build with Latest LLMs\n",
        "- Create Custom AI Apps\n",
        "- Learn from Industry Experts\n",
        "- Join Innovation Community\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "*Empowering the Next Generation of AI Innovators"
      ],
      "metadata": {
        "id": "X5y2qIBMxj6_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **âš¡ LiteLLM: Simplified LLM Access**\n",
        "\n",
        "**LiteLLM** is a versatile framework that provides seamless access to multiple large language models (LLMs) through a unified API. It supports providers like OpenAI, Google, and Anthropic, with **50+ LLMs supported**, while offering features such as load balancing, cost tracking, and streaming responses for efficient and scalable LLM integration.\n"
      ],
      "metadata": {
        "id": "K6NmZlb1xpSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Setup and Installation**\n",
        "\n"
      ],
      "metadata": {
        "id": "EWNH2kNTynj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install litellm langchain langchain_community"
      ],
      "metadata": {
        "id": "l9G8MV-aykBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Set your API keys**\n"
      ],
      "metadata": {
        "id": "AeE6ocygyuh3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd8yQZDtxWCC"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "OPENAIKEY=os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Call Openai Model using LiteLLM**"
      ],
      "metadata": {
        "id": "xFcJ55rOzo8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import litellm\n",
        "\n",
        "response = litellm.completion(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[{\"content\": \"what's the weather in SF\", \"role\": \"user\"}]\n",
        ")\n",
        "\n",
        "# Print the response\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvTf1YNdy17y",
        "outputId": "f22ecc13-5840-4e23-f1f7-ed01b55f7a73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelResponse(id='chatcmpl-AgQzzRQFeEZ1l9EvsrED2dp76UkKa', created=1734676923, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_9faba9f038', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"I'm unable to provide real-time weather updates. For the most current weather information in San Francisco, I recommend checking a reliable weather website or using a weather app.\", role='assistant', tool_calls=None, function_call=None))], usage=Usage(completion_tokens=33, prompt_tokens=13, total_tokens=46, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Langchain liteLLM Demo**"
      ],
      "metadata": {
        "id": "tYz3EKct0E7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.chat_models import ChatLiteLLM\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n"
      ],
      "metadata": {
        "id": "DrTpgvRuy-Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatLiteLLM(model=\"gpt-4o\")\n",
        "messages = [\n",
        "    HumanMessage(\n",
        "        content=\"what model are you\"\n",
        "    )\n",
        "]\n",
        "chat(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84VHhVTX0K36",
        "outputId": "2a9ce023-0766-4d43-b0ef-d237caa0776d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-f0c4af438933>:7: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  chat(messages)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='I am an AI language model developed by OpenAI, based on the GPT-4 architecture. How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': Usage(completion_tokens=27, prompt_tokens=11, total_tokens=38, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), 'model': 'gpt-4o', 'finish_reason': 'stop'}, id='run-65606b6e-5f50-49f7-adde-79f2f8a7a38e-0')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Function Calling with liteLLM**"
      ],
      "metadata": {
        "id": "UTGo6Fbl00K0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Define Messages, Functions**"
      ],
      "metadata": {
        "id": "EUhTxOGJ1Asi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, litellm\n",
        "from litellm import completion\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is the weather like in Boston?\"}\n",
        "]\n",
        "\n",
        "def get_current_weather(location):\n",
        "  if location == \"Boston, MA\":\n",
        "    return \"The weather is 12F\"\n",
        "\n",
        "functions = [\n",
        "    {\n",
        "      \"name\": \"get_current_weather\",\n",
        "      \"description\": \"Get the current weather in a given location\",\n",
        "      \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"location\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
        "          },\n",
        "          \"unit\": {\n",
        "            \"type\": \"string\",\n",
        "            \"enum\": [\"celsius\", \"fahrenheit\"]\n",
        "          }\n",
        "        },\n",
        "        \"required\": [\"location\"]\n",
        "      }\n",
        "    }\n",
        "  ]"
      ],
      "metadata": {
        "id": "xpuLbSlh0irb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Call gpt-4o Model to Decide what Function to call**"
      ],
      "metadata": {
        "id": "_lGcw-wK1KSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = completion(model=\"gpt-4o\", messages=messages, functions=functions)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBgR5MUB1Gr9",
        "outputId": "979d81e5-aea3-48c3-f5da-0f6120911ff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelResponse(id='chatcmpl-AgR0ScfGLcqXOliuqzZK1vM1WOM4U', created=1734676952, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_5f20662549', choices=[Choices(finish_reason='function_call', index=0, message=Message(content=None, role='assistant', tool_calls=None, function_call=FunctionCall(arguments='{\"location\":\"Boston, MA\"}', name='get_current_weather')))], usage=Usage(completion_tokens=18, prompt_tokens=80, total_tokens=98, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Parse GPT 4o Response**\n"
      ],
      "metadata": {
        "id": "wE8ddF4Q1d7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "function_call_data = response[\"choices\"][0][\"message\"][\"function_call\"]\n",
        "function_call_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-T8eULw-1SZJ",
        "outputId": "83386b1d-d862-453f-dc76-f1d3a97af883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FunctionCall(arguments='{\"location\":\"Boston, MA\"}', name='get_current_weather')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "function_name = function_call_data['name']\n",
        "function_args = function_call_data['arguments']\n",
        "function_args = json.loads(function_args)\n",
        "print(function_name, function_args)"
      ],
      "metadata": {
        "id": "rfWKHorgzwkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Calculating costs for gpt-4o turbo completion()**\n"
      ],
      "metadata": {
        "id": "CLrFAbLc5Qeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from litellm import completion, completion_cost\n",
        "import os\n",
        "\n",
        "\n",
        "messages = [{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
        "response = completion(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=messages,\n",
        ")\n",
        "\n",
        "print(response)\n",
        "\n",
        "cost = completion_cost(completion_response=response)\n",
        "formatted_string = f\"Cost for completion call: ${float(cost):.10f}\"\n",
        "print(formatted_string)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjMCBvq_3usm",
        "outputId": "7d082bd2-0640-413f-a8be-9db6553f36c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelResponse(id='chatcmpl-AgR1VEBicIXheCDu8BXuYQpBRPgwa', created=1734677017, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_d28bcae782', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hello! I'm just a program, so I don't have feelings, but I'm here to help you. How can I assist you today?\", role='assistant', tool_calls=None, function_call=None))], usage=Usage(completion_tokens=28, prompt_tokens=13, total_tokens=41, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier=None)\n",
            "Cost for completion call: $0.0003125000\n"
          ]
        }
      ]
    }
  ]
}