{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
        "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1onZMsObtxZLVroYbmb4i4wsYJvI9C875?usp=sharing)\n",
        "\n",
        "## Master Generative AI in 8 Weeks\n",
        "**What You'll Learn:**\n",
        "- Master cutting-edge AI tools & frameworks\n",
        "- 6 weeks of hands-on, project-based learning\n",
        "- Weekly live mentorship sessions\n",
        "- Join Innovation Community\n",
        "\n",
        "Learn by building. Get expert mentorship and work on real AI projects.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPghOeuekBjo"
      },
      "source": [
        "## Testing OpenAI's o1\n",
        "\n",
        "Notebook by [Build Fast with AI](https://www.buildfastwithai.com/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06OnDc9YVn79"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZ46csSYV5kX"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "from rich import print\n",
        "\n",
        "o1_mini = ChatOpenAI(model = \"openai/o1-mini\",\n",
        "                      openai_api_key = userdata.get(\"OPENROUTER_API_KEY\"),\n",
        "                      openai_api_base = \"https://openrouter.ai/api/v1\"\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "dTfrRmNOebUL",
        "outputId": "143ac6f8-e66e-48b7-f9d1-b301eb447758"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">To determine how long it will take to dry <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span> shirts based on the information provided, let's break down the \n",
              "problem step by step.\n",
              "\n",
              "### Given:\n",
              "- **<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> shirts take <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span> hours to dry.**\n",
              "\n",
              "### Step <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: Determine the Drying Rate\n",
              "First, find out how long it takes to dry **one** shirt.\n",
              "\n",
              "<span style=\"font-weight: bold\">[</span> \\text<span style=\"font-weight: bold\">{</span>Drying Time per Shirt<span style=\"font-weight: bold\">}</span> = \\frac<span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span> \\text<span style=\"font-weight: bold\">{</span> hours<span style=\"font-weight: bold\">}}{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> \\text<span style=\"font-weight: bold\">{</span> shirts<span style=\"font-weight: bold\">}}</span> = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> \\text<span style=\"font-weight: bold\">{</span> hours per shirt<span style=\"font-weight: bold\">}</span> \\<span style=\"font-weight: bold\">]</span>\n",
              "\n",
              "### Step <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: Calculate Time for <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span> Shirts\n",
              "If drying each shirt takes <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> hours **individually**, then drying <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span> shirts would take:\n",
              "\n",
              "<span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span> \\text<span style=\"font-weight: bold\">{</span> shirts<span style=\"font-weight: bold\">}</span> \\times <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> \\text<span style=\"font-weight: bold\">{</span> hours per shirt<span style=\"font-weight: bold\">}</span> = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span> \\text<span style=\"font-weight: bold\">{</span> hours<span style=\"font-weight: bold\">}</span> \\<span style=\"font-weight: bold\">]</span>\n",
              "\n",
              "### **Assumptions:**\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Sequential Drying:** This calculation assumes that you are drying the shirts one after another without any \n",
              "overlap.\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **No Equipment Constraints:** There's no mention of multiple drying machines or racks that could dry several \n",
              "shirts at the same time. If you have the capacity to dry more shirts simultaneously, the total drying time could be\n",
              "reduced proportionally.\n",
              "\n",
              "### **Alternative Scenario: Parallel Drying**\n",
              "If you have the capacity to dry multiple shirts at the same time <span style=\"font-weight: bold\">(</span>for example, using a large drying machine or \n",
              "multiple racks that can handle all <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span> shirts simultaneously<span style=\"font-weight: bold\">)</span>, then the drying time would remain the same as drying\n",
              "the first <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> shirts:\n",
              "\n",
              "<span style=\"font-weight: bold\">[</span> \\text<span style=\"font-weight: bold\">{</span>Total Drying Time<span style=\"font-weight: bold\">}</span> = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span> \\text<span style=\"font-weight: bold\">{</span> hours<span style=\"font-weight: bold\">}</span> \\<span style=\"font-weight: bold\">]</span>\n",
              "\n",
              "However, based on the initial information and without additional details about drying capacity, the most \n",
              "straightforward answer is:\n",
              "\n",
              "**It would take <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> hours to dry <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span> shirts under the given conditions.**\n",
              "</pre>\n"
            ],
            "text/plain": [
              "To determine how long it will take to dry \u001b[1;36m100\u001b[0m shirts based on the information provided, let's break down the \n",
              "problem step by step.\n",
              "\n",
              "### Given:\n",
              "- **\u001b[1;36m5\u001b[0m shirts take \u001b[1;36m50\u001b[0m hours to dry.**\n",
              "\n",
              "### Step \u001b[1;36m1\u001b[0m: Determine the Drying Rate\n",
              "First, find out how long it takes to dry **one** shirt.\n",
              "\n",
              "\u001b[1m[\u001b[0m \\text\u001b[1m{\u001b[0mDrying Time per Shirt\u001b[1m}\u001b[0m = \\frac\u001b[1m{\u001b[0m\u001b[1;36m50\u001b[0m \\text\u001b[1m{\u001b[0m hours\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\u001b[1m{\u001b[0m\u001b[1;36m5\u001b[0m \\text\u001b[1m{\u001b[0m shirts\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m = \u001b[1;36m10\u001b[0m \\text\u001b[1m{\u001b[0m hours per shirt\u001b[1m}\u001b[0m \\\u001b[1m]\u001b[0m\n",
              "\n",
              "### Step \u001b[1;36m2\u001b[0m: Calculate Time for \u001b[1;36m100\u001b[0m Shirts\n",
              "If drying each shirt takes \u001b[1;36m10\u001b[0m hours **individually**, then drying \u001b[1;36m100\u001b[0m shirts would take:\n",
              "\n",
              "\u001b[1m[\u001b[0m \u001b[1;36m100\u001b[0m \\text\u001b[1m{\u001b[0m shirts\u001b[1m}\u001b[0m \\times \u001b[1;36m10\u001b[0m \\text\u001b[1m{\u001b[0m hours per shirt\u001b[1m}\u001b[0m = \u001b[1;36m1000\u001b[0m \\text\u001b[1m{\u001b[0m hours\u001b[1m}\u001b[0m \\\u001b[1m]\u001b[0m\n",
              "\n",
              "### **Assumptions:**\n",
              "\u001b[1;36m1\u001b[0m. **Sequential Drying:** This calculation assumes that you are drying the shirts one after another without any \n",
              "overlap.\n",
              "\u001b[1;36m2\u001b[0m. **No Equipment Constraints:** There's no mention of multiple drying machines or racks that could dry several \n",
              "shirts at the same time. If you have the capacity to dry more shirts simultaneously, the total drying time could be\n",
              "reduced proportionally.\n",
              "\n",
              "### **Alternative Scenario: Parallel Drying**\n",
              "If you have the capacity to dry multiple shirts at the same time \u001b[1m(\u001b[0mfor example, using a large drying machine or \n",
              "multiple racks that can handle all \u001b[1;36m100\u001b[0m shirts simultaneously\u001b[1m)\u001b[0m, then the drying time would remain the same as drying\n",
              "the first \u001b[1;36m5\u001b[0m shirts:\n",
              "\n",
              "\u001b[1m[\u001b[0m \\text\u001b[1m{\u001b[0mTotal Drying Time\u001b[1m}\u001b[0m = \u001b[1;36m50\u001b[0m \\text\u001b[1m{\u001b[0m hours\u001b[1m}\u001b[0m \\\u001b[1m]\u001b[0m\n",
              "\n",
              "However, based on the initial information and without additional details about drying capacity, the most \n",
              "straightforward answer is:\n",
              "\n",
              "**It would take \u001b[1;36m1\u001b[0m,\u001b[1;36m000\u001b[0m hours to dry \u001b[1;36m100\u001b[0m shirts under the given conditions.**\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = o1_mini.invoke(\"\"\"\n",
        "If 5 shirts take 50 hours to dry. How much time will it take to dry 100 shirts?\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "YFuL7tsjOQZ2",
        "outputId": "eb948f67-8c15-495c-d407-c8c9f91b8616"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The word **<span style=\"color: #008000; text-decoration-color: #008000\">\"strawberry\"</span>** contains **two** letter **<span style=\"color: #008000; text-decoration-color: #008000\">'r'</span>s**.\n",
              "\n",
              "Here's the breakdown of the letters in <span style=\"color: #008000; text-decoration-color: #008000\">\"strawberry\"</span>:\n",
              "\n",
              "- **S**\n",
              "- **T**\n",
              "- **R**\n",
              "- **A**\n",
              "- **W**\n",
              "- **B**\n",
              "- **E**\n",
              "- **R**\n",
              "- **R**\n",
              "- **Y**\n",
              "\n",
              "As you can see, the letter **<span style=\"color: #008000; text-decoration-color: #008000\">'R'</span>** appears twice.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "The word **\u001b[32m\"strawberry\"\u001b[0m** contains **two** letter **\u001b[32m'r'\u001b[0ms**.\n",
              "\n",
              "Here's the breakdown of the letters in \u001b[32m\"strawberry\"\u001b[0m:\n",
              "\n",
              "- **S**\n",
              "- **T**\n",
              "- **R**\n",
              "- **A**\n",
              "- **W**\n",
              "- **B**\n",
              "- **E**\n",
              "- **R**\n",
              "- **R**\n",
              "- **Y**\n",
              "\n",
              "As you can see, the letter **\u001b[32m'R'\u001b[0m** appears twice.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = o1_mini.invoke(\"\"\"\n",
        "How many 'r's are there in the word strawberry?\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "xEoKCRZVOy7T",
        "outputId": "b0eb470c-0572-4298-8a2e-b55e6e9d4078"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">To determine which number is greater between **<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.9</span>** and **<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.11</span>**, let's break them down:\n",
              "\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Convert to Decimal Form <span style=\"font-weight: bold\">(</span>if needed<span style=\"font-weight: bold\">)</span>:**\n",
              "   - **<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.9</span>** can be written as **<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.90</span>** for easier comparison.\n",
              "\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Compare the Numbers:**\n",
              "   - **<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.90</span>** versus **<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.11</span>**\n",
              "\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **Step-by-Step Comparison:**\n",
              "   - **Units Place:** Both numbers have **<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>** in the units place.\n",
              "   - **Tenths Place:** \n",
              "     - **<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.90</span>** has **<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>** in the tenths place.\n",
              "     - **<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.11</span>** has **<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>** in the tenths place.\n",
              "   - Since **<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span> <span style=\"font-weight: bold\">(</span>tenths<span style=\"font-weight: bold\">)</span>** is greater than **<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> <span style=\"font-weight: bold\">(</span>tenths<span style=\"font-weight: bold\">)</span>**, **<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.90</span>** is greater than **<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.11</span>**.\n",
              "\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. **Conclusion:**\n",
              "   - **<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.9</span>** is **greater** than **<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.11</span>**.\n",
              "\n",
              "**Therefore, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.9</span> is greater than <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.11</span>.**\n",
              "</pre>\n"
            ],
            "text/plain": [
              "To determine which number is greater between **\u001b[1;36m9.9\u001b[0m** and **\u001b[1;36m9.11\u001b[0m**, let's break them down:\n",
              "\n",
              "\u001b[1;36m1\u001b[0m. **Convert to Decimal Form \u001b[1m(\u001b[0mif needed\u001b[1m)\u001b[0m:**\n",
              "   - **\u001b[1;36m9.9\u001b[0m** can be written as **\u001b[1;36m9.90\u001b[0m** for easier comparison.\n",
              "\n",
              "\u001b[1;36m2\u001b[0m. **Compare the Numbers:**\n",
              "   - **\u001b[1;36m9.90\u001b[0m** versus **\u001b[1;36m9.11\u001b[0m**\n",
              "\n",
              "\u001b[1;36m3\u001b[0m. **Step-by-Step Comparison:**\n",
              "   - **Units Place:** Both numbers have **\u001b[1;36m9\u001b[0m** in the units place.\n",
              "   - **Tenths Place:** \n",
              "     - **\u001b[1;36m9.90\u001b[0m** has **\u001b[1;36m9\u001b[0m** in the tenths place.\n",
              "     - **\u001b[1;36m9.11\u001b[0m** has **\u001b[1;36m1\u001b[0m** in the tenths place.\n",
              "   - Since **\u001b[1;36m9\u001b[0m \u001b[1m(\u001b[0mtenths\u001b[1m)\u001b[0m** is greater than **\u001b[1;36m1\u001b[0m \u001b[1m(\u001b[0mtenths\u001b[1m)\u001b[0m**, **\u001b[1;36m9.90\u001b[0m** is greater than **\u001b[1;36m9.11\u001b[0m**.\n",
              "\n",
              "\u001b[1;36m4\u001b[0m. **Conclusion:**\n",
              "   - **\u001b[1;36m9.9\u001b[0m** is **greater** than **\u001b[1;36m9.11\u001b[0m**.\n",
              "\n",
              "**Therefore, \u001b[1;36m9.9\u001b[0m is greater than \u001b[1;36m9.11\u001b[0m.**\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = o1_mini.invoke(\"\"\"\n",
        "Which one is greater 9.9 or 9.11?\n",
        "\"\"\")\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "JEyc2LWsPYtH",
        "outputId": "0edb60c4-27a2-4639-c0e7-cc5c94a43c5a"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "{'message': 'b\\'{\\\\n  \"error\": {\\\\n    \"message\": \"Invalid prompt: your prompt was flagged as potentially violating our usage policy. Please try again with a different prompt.\",\\\\n    \"type\": \"invalid_request_error\",\\\\n    \"param\": null,\\\\n    \"code\": \"invalid_prompt\"\\\\n  }\\\\n}\\'', 'code': 400}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-36e01642073d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m response = o1_mini.invoke(\"\"\"\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mWhat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfifth\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myour\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;31m?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\")\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         return cast(\n\u001b[1;32m    276\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    775\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    776\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         flattened_outputs = [\n\u001b[1;32m    636\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m                 results.append(\n\u001b[0;32m--> 624\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    625\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    847\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     def _get_request_payload(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_create_chat_result\u001b[0;34m(self, response, generation_info)\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;31m# to mask the true error. Because 'response[\"choices\"]' is None.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0mtoken_usage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"usage\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: {'message': 'b\\'{\\\\n  \"error\": {\\\\n    \"message\": \"Invalid prompt: your prompt was flagged as potentially violating our usage policy. Please try again with a different prompt.\",\\\\n    \"type\": \"invalid_request_error\",\\\\n    \"param\": null,\\\\n    \"code\": \"invalid_prompt\"\\\\n  }\\\\n}\\'', 'code': 400}"
          ]
        }
      ],
      "source": [
        "response = o1_mini.invoke(\"\"\"\n",
        "What is the fifth word in your response to this prompt?\n",
        "\"\"\")\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6UZ9RndJWKu5",
        "outputId": "9ee80c7f-2dc4-4468-d5be-5cc7d5693baf"
      },
      "outputs": [],
      "source": [
        "response = o1_mini.invoke(\"\"\"\n",
        "Create a snake game in Python in pygame\n",
        "\"\"\")\n",
        "\n",
        "print(response.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
