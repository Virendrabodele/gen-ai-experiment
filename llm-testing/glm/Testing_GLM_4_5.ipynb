{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
        "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1tNgdujOz0vbsiuzLK_TYr_EkCEGKOmui?usp=sharing)\n",
        "\n",
        "## Master Generative AI in 8 Weeks\n",
        "**What You'll Learn:**\n",
        "- Master cutting-edge AI tools & frameworks\n",
        "- 6 weeks of hands-on, project-based learning\n",
        "- Weekly live mentorship sessions\n",
        "- No coding experience required\n",
        "- Join Innovation Community\n",
        "\n",
        "Learn by building. Get expert mentorship and work on real AI projects.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_ZKrP-_36l23"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKy3SpsM6jiJ"
      },
      "source": [
        "# Testing GLM 4.5 Model Using OpenRouter\n",
        "\n",
        "This notebook provides a comprehensive guide to using the Qwen model via OpenRouter's API within the LangChain framework. We will cover everything from basic setup to advanced examples.\n",
        "\n",
        "**Note:** You will need an API key from [OpenRouter](https://openrouter.ai/) to run the examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfaRziHC6jiM"
      },
      "source": [
        "###Installation\n",
        "\n",
        "First, let's install the necessary Python libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TjlR0IIK6jiN"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-openai langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###import API Keys"
      ],
      "metadata": {
        "id": "GzofWxWtxHgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENROUTER_API_KEY\"] = userdata.get(\"OPENROUTER_API_KEY\")\n",
        "os.environ[\"tavily_api_key\"] = userdata.get(\"TAVILY_API_KEY\")"
      ],
      "metadata": {
        "id": "fPXW1N1ZxHCI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dTJ4-AV6jiO"
      },
      "source": [
        "###Basic Usage with ChatOpenAI and OpenRouter\n",
        "\n",
        "- Here’s how to set up the `ChatOpenAI` class to connect to the Qwen model through OpenRouter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5A3jXql6jiO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# It's recommended to set your API key as an environment variable\n",
        "api_key = userdata.get(\"OPENROUTER_API_KEY\")\n",
        "\n",
        "# Initialize the ChatOpenAI model for Qwen\n",
        "glm_llm = ChatOpenAI(\n",
        "    model=\"z-ai/glm-4.5\",\n",
        "    openai_api_key=api_key,\n",
        "    openai_api_base=\"https://openrouter.ai/api/v1\"\n",
        ")\n",
        "\n",
        "# Let's test it with a simple prompt\n",
        "response = glm_llm.invoke(\"Explain the difference between LangChain and LlamaIndex in 2 sentences.\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBYsDNig6jiP"
      },
      "source": [
        "### Multilingual Capabilities\n",
        "\n",
        "Qwen models are proficient in multiple languages. Let's test this by sending prompts in Hindi and Spanish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_tC8vos6jiP"
      },
      "outputs": [],
      "source": [
        "# Example in Hindi\n",
        "hindi_prompt = \"कृत्रिम बुद्धिमत्ता क्या है?\" # Translation: What is Artificial Intelligence?\n",
        "hindi_response = glm_llm.invoke(hindi_prompt)\n",
        "print(f\"\"\"\n",
        "Response in Hindi:\n",
        "{hindi_response.content}\"\"\")\n",
        "\n",
        "# Example in Spanish\n",
        "spanish_prompt = \"¿Cuál es la capital de Argentina?\" # Translation: What is the capital of Argentina?\n",
        "spanish_response = glm_llm.invoke(spanish_prompt)\n",
        "print(f\"\"\"\n",
        "Response in Spanish:\n",
        "{spanish_response.content}\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAu9BXF46jiQ"
      },
      "source": [
        "### Advanced Parameter Tuning\n",
        "\n",
        "You can control the model's output by tuning parameters like `temperature` and `top_p`.\n",
        "\n",
        "- **`temperature`**: Controls randomness. Lower values (e.g., 0.1) make the output more deterministic, while higher values (e.g., 0.9) make it more creative.\n",
        "- **`top_p`**: Controls nucleus sampling. It considers only the tokens with the highest probability mass.\n",
        "- **`max_tokens`**: Sets the maximum length of the generated response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgpxAxGy6jiQ"
      },
      "outputs": [],
      "source": [
        "# Creative response with high temperature\n",
        "creative_llm = ChatOpenAI(\n",
        "    model=\"z-ai/glm-4.5\",\n",
        "    openai_api_key=api_key,\n",
        "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "    temperature=0.9,\n",
        "    max_tokens=150\n",
        ")\n",
        "\n",
        "prompt = \"Write a short, futuristic story about a cat who can code.\"\n",
        "creative_response = creative_llm.invoke(prompt)\n",
        "print(f\"\"\"Creative Story:\n",
        "{creative_response.content}\"\"\")\n",
        "\n",
        "# Factual response with low temperature\n",
        "factual_llm = ChatOpenAI(\n",
        "    model=\"z-ai/glm-4.5\",\n",
        "    openai_api_key=api_key,\n",
        "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "prompt = \"Explain the theory of relativity in simple terms.\"\n",
        "factual_response = factual_llm.invoke(prompt)\n",
        "print(f\"\"\"\n",
        "Factual Explanation:\n",
        "{factual_response.content}\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YILCRBuG6jiQ"
      },
      "source": [
        "### Building a Simple Chatbot\n",
        "\n",
        "We can create a simple conversational chatbot by managing the chat history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksog2Dsr6jiR"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "]\n",
        "\n",
        "print(\"Hello! I am ready to chat. Type 'quit' to end the conversation.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    messages.append(HumanMessage(content=user_input))\n",
        "\n",
        "    response = glm_llm.invoke(messages)\n",
        "    print(f\"Assistant: {response.content}\")\n",
        "\n",
        "    # Add the bot's response to the history\n",
        "    messages.append(response)\n",
        "\n",
        "print(\"Goodbye!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tool Calling and Tavily Search Using GLM"
      ],
      "metadata": {
        "id": "HC59dEUCHWoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "\n",
        "# 1. Setup LLM via OpenRouter\n",
        "llm = ChatOpenAI(\n",
        "    model=\"z-ai/glm-4.5\", # Check OpenRouter.ai for valid IDs\n",
        "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "    openai_api_key=userdata.get(\"OPENROUTER_API_KEY\")\n",
        ")\n",
        "\n",
        "# 2. Setup Tavily Search Tool\n",
        "tavily_tool = TavilySearchResults(max_results=2)\n",
        "\n",
        "# 3. Create Agent\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant.\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "])\n",
        "agent = create_tool_calling_agent(llm, [tavily_tool], prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=[tavily_tool], verbose=True)\n",
        "\n",
        "# 4. Run Agent\n",
        "response = agent_executor.invoke({\"input\": \"How do central bank interest rate changes affect the stock market?\"})\n",
        "print(response['output'])"
      ],
      "metadata": {
        "id": "bfug1PTgHU1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy2X3T7U6jiR"
      },
      "source": [
        "###  Code Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZvB_lvr6jiR"
      },
      "outputs": [],
      "source": [
        "prompt = \"Write a Python function that takes a list of numbers and returns the sum of all even numbers in the list.\"\n",
        "code_response = glm_llm.invoke(prompt)\n",
        "print(f\"\"\"Generated Code:\n",
        "{code_response.content}\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t05mJ526jiS"
      },
      "source": [
        "###  Few-Shot Prompting\n",
        "\n",
        "Few-shot prompting provides the model with examples to guide its response format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDUW2gov6jiS"
      },
      "outputs": [],
      "source": [
        "prompt = (\n",
        "    \"\"\"Translate the following English words to French:\n",
        "\n",
        "    \"sea -> mer\"\n",
        "    \"sky -> ciel\"\n",
        "    \"book -> livre\"\n",
        "    \"house ->\"\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "few_shot_response = glm_llm.invoke(prompt)\n",
        "print(f\"Translation of 'house': {few_shot_response.content}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}